{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\interview_qa\\\\research'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\interview_qa\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\i'\n",
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_9596\\804336038.py:1: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  file_path = \"D:\\interview_qa\\Machine - Learning - Tom Mitchell.pdf\"\n",
      "C:\\Users\\himan\\AppData\\Local\\Temp\\ipykernel_9596\\804336038.py:1: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  file_path = \"D:\\interview_qa\\Machine - Learning - Tom Mitchell.pdf\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124minterview_qa\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMachine - Learning - Tom Mitchell.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m loader \u001b[38;5;241m=\u001b[39m PyPDFLoader(file_path)\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:31\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:257\u001b[0m, in \u001b[0;36mPyPDFLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     blob \u001b[38;5;241m=\u001b[39m Blob\u001b[38;5;241m.\u001b[39mfrom_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:127\u001b[0m, in \u001b[0;36mBaseBlobParser.parse\u001b[1;34m(self, blob)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m, blob: Blob) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Eagerly parse the blob into a document or documents.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m    This is a convenience method for interactive development environment.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m        List of documents\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:127\u001b[0m, in \u001b[0;36mPyPDFParser.lazy_parse\u001b[1;34m(self, blob)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mas_bytes_io() \u001b[38;5;28;01mas\u001b[39;00m pdf_file_obj:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     pdf_reader \u001b[38;5;241m=\u001b[39m pypdf\u001b[38;5;241m.\u001b[39mPdfReader(pdf_file_obj, password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword)\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m [\n\u001b[0;32m    126\u001b[0m         Document(\n\u001b[1;32m--> 127\u001b[0m             page_content\u001b[38;5;241m=\u001b[39m\u001b[43m_extract_text_from_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_images_from_page(page),\n\u001b[0;32m    129\u001b[0m             metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: blob\u001b[38;5;241m.\u001b[39msource, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m: page_number},  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    130\u001b[0m         )\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page_number, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pdf_reader\u001b[38;5;241m.\u001b[39mpages)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:117\u001b[0m, in \u001b[0;36mPyPDFParser.lazy_parse.<locals>._extract_text_from_page\u001b[1;34m(page)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m page\u001b[38;5;241m.\u001b[39mextract_text()\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextraction_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextraction_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextraction_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\_page.py:2393\u001b[0m, in \u001b[0;36mPageObject.extract_text\u001b[1;34m(self, orientations, space_width, visitor_operand_before, visitor_operand_after, visitor_text, extraction_mode, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orientations, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m   2391\u001b[0m     orientations \u001b[38;5;241m=\u001b[39m (orientations,)\n\u001b[1;32m-> 2393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2394\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2395\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2396\u001b[0m \u001b[43m    \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCONTENTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_operand_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisitor_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\_page.py:2089\u001b[0m, in \u001b[0;36mPageObject._extract_text\u001b[1;34m(self, obj, pdf, orientations, space_width, content_key, visitor_operand_before, visitor_operand_after, visitor_text)\u001b[0m\n\u001b[0;32m   2086\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m OrientationNotFoundError:\n\u001b[0;32m   2087\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2089\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m operands, operator \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperations\u001b[49m:\n\u001b[0;32m   2090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m visitor_operand_before \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2091\u001b[0m         visitor_operand_before(operator, operands, cm_matrix, tm_matrix)\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:1404\u001b[0m, in \u001b[0;36mContentStream.operations\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moperations\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Any, Any]]:\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operations \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data:\n\u001b[1;32m-> 1404\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_content_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_operations\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:1310\u001b[0m, in \u001b[0;36mContentStream._parse_content_stream\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m   1308\u001b[0m         peek \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1310\u001b[0m     operands\u001b[38;5;241m.\u001b[39mappend(\u001b[43mread_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforced_encoding\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:1443\u001b[0m, in \u001b[0;36mread_object\u001b[1;34m(stream, pdf, forced_encoding)\u001b[0m\n\u001b[0;32m   1441\u001b[0m stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# reset to start\u001b[39;00m\n\u001b[0;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peek \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDictionaryObject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_from_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_hex_string_from_stream(stream, forced_encoding)\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:593\u001b[0m, in \u001b[0;36mDictionaryObject.read_from_stream\u001b[1;34m(stream, pdf, forced_encoding)\u001b[0m\n\u001b[0;32m    591\u001b[0m     tok \u001b[38;5;241m=\u001b[39m read_non_whitespace(stream)\n\u001b[0;32m    592\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 593\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mread_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39mstrict:\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:1447\u001b[0m, in \u001b[0;36mread_object\u001b[1;34m(stream, pdf, forced_encoding)\u001b[0m\n\u001b[0;32m   1445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m read_hex_string_from_stream(stream, forced_encoding)\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tok \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArrayObject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_from_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_encoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tok \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tok \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BooleanObject\u001b[38;5;241m.\u001b[39mread_from_stream(stream)\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:265\u001b[0m, in \u001b[0;36mArrayObject.read_from_stream\u001b[1;34m(stream, pdf, forced_encoding)\u001b[0m\n\u001b[0;32m    263\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;66;03m# read and append obj\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     arr\u001b[38;5;241m.\u001b[39mappend(\u001b[43mread_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_encoding\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:1471\u001b[0m, in \u001b[0;36mread_object\u001b[1;34m(stream, pdf, forced_encoding)\u001b[0m\n\u001b[0;32m   1469\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m IndirectObject\u001b[38;5;241m.\u001b[39mread_from_stream(stream, pdf)\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNumberObject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_from_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1473\u001b[0m     pos \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\generic\\_base.py:569\u001b[0m, in \u001b[0;36mNumberObject.read_from_stream\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    567\u001b[0m num \u001b[38;5;241m=\u001b[39m read_until_regex(stream, NumberObject\u001b[38;5;241m.\u001b[39mNumberPattern)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFloatObject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m NumberObject(num)\n",
      "File \u001b[1;32mc:\\Users\\himan\\Anaconda3\\envs\\interview\\Lib\\site-packages\\pypdf\\generic\\_base.py:466\u001b[0m, in \u001b[0;36mFloatObject.__new__\u001b[1;34m(cls, value, context)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mcls\u001b[39m, value: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, context: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    464\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFloatObject\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value)\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;66;03m# If this isn't a valid decimal (happens in malformed PDFs)\u001b[39;00m\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;66;03m# fallback to 0\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = \"D:\\interview_qa\\Machine - Learning - Tom Mitchell.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 0}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 1}, page_content=' \\n \\nMachine Learning \\n \\nTom M. Mitchell \\n \\n \\nProduct Details  \\n• Hardcover: 432 pages ; Dimensions (in inches): 0.75 x 10.00 x 6.50  \\n• Publisher: McGraw-Hill Science/Engineering/Math; (March 1, 1997)  \\n• ISBN: 0070428077  \\n• Average Customer Review: \\n Based on 16 reviews. \\n• Amazon.com Sales Rank: 42,816  \\n• Popular in: Redmond, WA (#17) , Ithaca, NY (#9)  \\n \\n \\nEditorial Reviews \\nFrom Book News, Inc. An introductory text on primary approaches to machine learning and \\nthe study of computer algorithms that improve  automatically through experience. Introduce \\nbasics concepts from statistics, artificial intellig ence, information theory, and other disciplines as \\nneed arises, with balanced coverage of theory and practice, and presents major algorithms with \\nillustrations of their use. Includes chapter exerci ses. Online data sets and implementations of \\nseveral algorithms are available on a Web site. No  prior background in artificial intelligence or \\nstatistics is assumed. For advanced undergraduat es and graduate students in computer science, \\nengineering, statistics, and social scien ces, as well as software professionals. Book News, Inc.®, \\nPortland, OR  \\nBook Info: Presents the key algorithms and theory that form the core of machine learning. \\nDiscusses such theoretical issues as How does learning performance vary with the number of \\ntraining examples presented? and Which learning algorithms are most a ppropriate for various \\ntypes of learning tasks? DLC: Computer algorithms.  \\nBook Description: This book covers the field of machin e learning, which is the study of \\nalgorithms that allow computer programs to automatically improve through experience. The \\nbook is intended to support upper level undergraduate and introductory level graduate courses in \\nmachine learning '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 2}, page_content=\"PREFACE \\nThe field of machine learning is concerned with the question of how to construct \\ncomputer programs that automatically improve with experience. In recent years \\nmany successful machine learning applications have been developed, ranging from \\ndata-mining programs that learn to detect fraudulent credit card transactions, to \\ninformation-filtering systems that learn users' reading preferences, to autonomous \\nvehicles that learn to drive on public highways. At the same time, there have been \\nimportant advances in the theory and algorithms that form the foundations of this \\nfield. \\nThe goal of this textbook is to present the key algorithms and theory that \\nform the core of machine learning. Machine learning draws on concepts and \\nresults from many fields, including statistics, artificial intelligence, philosophy, \\ninformation theory, biology, cognitive science, computational complexity, and \\ncontrol theory. \\nMy \\nbelief is that the best way to learn about machine learning is \\nto view it from all of these perspectives and to understand the problem settings, \\nalgorithms, and assumptions that underlie each. In the past, this has been difficult \\ndue to the absence of a broad-based single source introduction \\nto \\nthe field. The \\nprimary goal of this book is to provide such an introduction. \\nBecause of the interdisciplinary nature of the material, this book makes \\nfew assumptions about the background of the reader. Instead, it introduces basic \\nconcepts from statistics, artificial intelligence, information theory, and other disci- \\nplines as the need arises, focusing on just those concepts most relevant to machine \\nlearning. The book is intended for both undergraduate and graduate students in \\nfields such as computer science, engineering, statistics, and the social sciences, \\nand as a reference for software professionals and practitioners. Two principles \\nthat guided the writing of the book were that it should be accessible to undergrad- \\nuate students and that it should contain the material I would want my own Ph.D. \\nstudents to learn before beginning their doctoral research in machine learning. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 3}, page_content='xvi PREFACE \\nA third principle that guided the writing of this book was that it should \\npresent a balance of theory and practice. Machine learning theory attempts to an- \\nswer questions such as \"How does learning performance vary with the number of \\ntraining examples presented?\" and \"Which learning algorithms are most appropri- \\nate for various types of learning tasks?\" This book includes discussions of these \\nand other theoretical issues, drawing on theoretical constructs from statistics, com- \\nputational complexity, and Bayesian analysis. The practice of machine learning \\nis covered by presenting the major algorithms in the field, along with illustrative \\ntraces of their operation. Online data sets and implementations of several algo- \\nrithms are available via the World Wide Web at http://www.cs.cmu.edu/-tom1 \\nmlbook.html. These include neural network code and data for face recognition, \\ndecision tree learning, code and data for financial loan analysis, and Bayes clas- \\nsifier code and data for analyzing text documents. I am grateful to a number of \\ncolleagues who have helped to create these online resources, including Jason \\nRen- \\nnie, Paul Hsiung, Jeff Shufelt, Matt Glickman, Scott Davies, Joseph \\nO\\'Sullivan, \\nKen Lang, Andrew \\nMcCallum, \\nand Thorsten Joachims. \\nACKNOWLEDGMENTS \\nIn writing this book, I have been fortunate to be assisted by technical experts \\nin many of the subdisciplines that make up the field of machine learning. This \\nbook could not have been written without their help. I am deeply indebted to \\nthe following scientists who took the time to review chapter drafts and, in many \\ncases, to tutor me and help organize chapters in their individual areas of expertise. \\nAvrim \\nBlum, Jaime Carbonell, William Cohen, Greg Cooper, Mark Craven, \\nKen DeJong, \\nJerry \\nDeJong, Tom Dietterich, Susan Epstein, \\nOren \\nEtzioni, \\nScott Fahlman, Stephanie Forrest, David Haussler, Haym \\nHirsh, \\nRob Holte, \\nLeslie Pack Kaelbling, Dennis \\nKibler, \\nMoshe Koppel, John Koza, Miroslav \\nKubat, John Lafferty, Ramon Lopez de Mantaras, Sridhar Mahadevan, Stan \\nMatwin, \\nAndrew \\nMcCallum, \\nRaymond Mooney, Andrew Moore, Katharina \\nMorik, \\nSteve Muggleton, Michael Pazzani, David Poole, Armand Prieditis, \\nJim Reggia, Stuart Russell, Lorenza \\nSaitta, \\nClaude Sammut, Jeff Schneider, \\nJude \\nShavlik, \\nDevika Subramanian, Michael Swain, Gheorgh Tecuci, \\nSe- \\nbastian \\nThrun, \\nPeter Turney, Paul Utgoff, Manuela Veloso, Alex Waibel, \\nStefan Wrobel, and Yiming Yang. \\nI am also grateful to the many instructors and students at various universi- \\nties who have field tested various drafts of this book and who have contributed \\ntheir suggestions. Although there is no space to thank the hundreds of students, \\ninstructors, and others who tested earlier drafts of this book, I would like to thank \\nthe following for particularly helpful comments and discussions: \\nShumeet \\nBaluja, \\nAndrew \\nBanas, \\nAndy \\nBarto, \\nJim Blackson, Justin \\nBoyan, \\nRich \\nCaruana, \\nPhilip Chan, Jonathan Cheyer, Lonnie Chrisman, Dayne \\nFrei- \\ntag, Geoff Gordon, Warren Greiff, Alexander Harm, Tom Ioerger, Thorsten '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 4}, page_content='PREFACE xvii \\nJoachim, Atsushi Kawamura, Martina Klose, Sven Koenig, Jay Modi, An- \\ndrew Ng, Joseph O\\'Sullivan, Patrawadee Prasangsit, Doina Precup, Bob \\nPrice, Choon Quek, Sean Slattery, Belinda Thom, Astro Teller, Will Tracz \\nI would like to thank Joan Mitchell for creating the index for the book. I \\nalso would like to thank Jean Harpley for help in editing many of the figures. \\nJane \\nLoftus \\nfrom ETP Harrison improved the presentation significantly through \\nher copyediting of the manuscript and generally helped usher the manuscript \\nthrough the intricacies of final production. Eric Munson, my editor at \\nMcGraw \\nHill, provided encouragement and expertise in all phases of this project. \\nAs always, the greatest debt one owes is to one\\'s colleagues, friends, and \\nfamily. In my case, this debt is especially large. I can hardly imagine a more \\nintellectually stimulating environment and supportive set of friends than those I \\nhave at Carnegie Mellon. Among the many here who helped, I would especially \\nlike to thank Sebastian \\nThrun, \\nwho throughout this project was a constant source \\nof encouragement, technical expertise, and support of all kinds. My parents, as \\nalways, encouraged and asked \"Is it done yet?\" at just the right times. Finally, I \\nmust thank my family: Meghan, Shannon, and Joan. They are responsible for this \\nbook in more ways than even they know. This book is dedicated to them. \\nTom M. Mitchell '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 5}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 6}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 7}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 8}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 9}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 10}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 11}, page_content=''),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 12}, page_content='CHAPTER \\nINTRODUCTION \\nEver since computers were invented, we have wondered whether they might be \\nmade to learn. If we could understand how to program them to learn-to improve \\nautomatically with experience-the impact would be dramatic. Imagine comput- \\ners learning from medical records which treatments are most effective for new \\ndiseases, houses learning from experience to optimize energy costs based on the \\nparticular usage patterns of their occupants, or personal software assistants learn- \\ning the evolving interests of their users in order to highlight especially relevant \\nstories from the online morning newspaper. A successful understanding of how to \\nmake computers learn would open up many new uses of computers and new levels \\nof competence and customization. And a detailed understanding of \\ninformation- \\nprocessing algorithms for machine learning might lead to a better understanding \\nof human learning abilities (and disabilities) as well. \\nWe do not yet know how to make computers learn nearly as well as people \\nlearn. However, algorithms have been invented that are effective for certain types \\nof learning tasks, and a theoretical understanding of learning is beginning to \\nemerge. Many practical computer programs have been developed to exhibit use- \\nful types of learning, and significant commercial applications have begun to ap- \\npear. For problems such as speech recognition, algorithms based on machine \\nlearning outperform all other approaches that have been attempted to date. In \\nthe field known as data mining, machine learning algorithms are being used rou- \\ntinely to discover valuable knowledge from large commercial databases containing \\nequipment maintenance records, loan applications, financial transactions, medical \\nrecords, and the like. As our understanding of computers continues to mature, it '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 13}, page_content=\"2 MACHINE LEARNING \\nseems inevitable that machine learning will play an increasingly central role in \\ncomputer science and computer technology. \\nA few specific achievements provide a glimpse of the state of the art: pro- \\ngrams have been developed that successfully learn to recognize spoken words \\n(Waibel 1989; Lee 1989), predict recovery rates of pneumonia patients (Cooper \\net al. \\n1997), \\ndetect fraudulent use of credit cards, drive autonomous vehicles \\non public highways (Pomerleau \\n1989), \\nand play games such as backgammon at \\nlevels approaching the performance of human world champions (Tesauro 1992, \\n1995). Theoretical results have been developed that characterize the fundamental \\nrelationship among the number of training examples observed, the number of hy- \\npotheses under consideration, and the expected error in learned hypotheses. We \\nare beginning to obtain initial models of human and animal learning and to un- \\nderstand their relationship to learning algorithms developed for computers \\n(e.g., \\nLaird et al. 1986; Anderson 1991; Qin et al. 1992; Chi and Bassock 1989; Ahn \\nand Brewer 1993). In applications, algorithms, theory, and studies of biological \\nsystems, the rate of progress has increased significantly over the past decade. Sev- \\neral recent applications of machine learning are summarized in Table 1.1. Langley \\nand Simon (1995) and \\nRumelhart \\net al. (1994) survey additional applications of \\nmachine learning. \\nThis book presents the field of machine learning, describing a variety of \\nlearning paradigms, algorithms, theoretical results, and applications. Machine \\nlearning is inherently a multidisciplinary field. It draws on results from artifi- \\ncial intelligence, probability and statistics, computational complexity theory, con- \\ntrol theory, information \\ntheory, \\nphilosophy, psychology, neurobiology, and other \\nfields. Table 1.2 summarizes key ideas from each of these fields that impact the \\nfield of machine learning. While the material in this book is based on results from \\nmany diverse fields, the reader need not be an expert in any of them. Key ideas \\nare presented from these fields using a nonspecialist's vocabulary, with unfamiliar \\nterms and concepts introduced as the need arises. \\n1.1 WELL-POSED LEARNING PROBLEMS \\nLet us begin our study of machine learning by considering a few learning tasks. For \\nthe purposes of this book we will define learning broadly, to include any .computer \\nprogram that improves its performance at some task through experience. Put more \\nprecisely, \\nDefinition: A computer program is said to learn from experience E with respect \\nto some class of tasks T and performance measure P, if its performance at tasks in \\nT, as measured by P, improves with experience E. \\nFor example, a computer program that learns to play checkers might improve \\nits performance as measured by its abiliry to win at the class of tasks involving \\nplaying checkers games, through experience obtained by playing games against \\nitself. In general, to have a well-defined learning problem, we must identity these \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 14}, page_content=\"CHAPTER 1 INTRODUCITON 3 \\n0 Learning to recognize spoken words. \\nAll of the most successful speech recognition systems employ machine learning in some form. \\nFor example, the SPHINX system \\n(e.g., \\nLee 1989) learns speaker-specific strategies for recognizing \\nthe primitive sounds (phonemes) and words from the observed speech signal. Neural network \\nlearning methods \\n(e.g., \\nWaibel \\net al. 1989) and methods for learning hidden Markov models \\n(e.g., Lee 1989) are effective for automatically customizing to,individual speakers, vocabularies, \\nmicrophone characteristics, background noise, etc. Similar techniques have potential applications \\nin many signal-interpretation problems. \\n0 Learning to drive an autonomous vehicle. \\nMachine learning methods have been used to train computer-controlled vehicles to steer correctly \\nwhen driving on a variety of road types. For example, the ALVINN system (Pomerleau 1989) \\nhas used its learned strategies to drive unassisted at 70 miles per hour for 90 miles on public \\nhighways among other cars. Similar techniques have possible applications in many sensor-based \\ncontrol problems. \\n0 Learning to classify new astronomical structures. \\nMachine learning methods have been applied to a variety of large databases to learn general \\nregularities implicit in the data. For example, decision tree learning algorithms have been used \\nby NASA to learn how to classify celestial objects from the second \\nPalomar \\nObservatory Sky \\nSurvey (Fayyad et al. 1995). This system is now used to automatically classify all objects in the \\nSky Survey, which consists of three terrabytes of image data. \\n0 Learning to play world-class backgammon. \\nThe most successful computer programs for playing games such as backgammon are based on \\nmachiie \\nlearning algorithms. For example, the world's top computer program for backgammon, \\nTD-GAMMON \\n(Tesauro 1992, \\n1995). \\nlearned its strategy by playing over one million practice \\ngames against itself. It now plays at a level competitive with the human world champion. Similar \\ntechniques have applications in many practical problems where very large search spaces must be \\nexamined efficiently. \\nTABLE 1.1 \\nSome successful applications of machiie learning. \\nthree features: the class of tasks, the measure of performance to be improved, and \\nthe source of experience. \\nA checkers learning problem: \\nTask T: playing checkers \\n0 Performance measure P: percent of games won against opponents \\nTraining experience E: playing practice games against itself \\nWe can specify many learning problems in this fashion, such as learning \\nto recognize handwritten words, or learning to drive a robotic automobile au- \\ntonomously. \\nA handwriting recognition learning problem: \\n0 Task T: recognizing and classifying handwritten words within images \\n0 Performance measure P: percent of words correctly classified \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 15}, page_content='4 MACHINE LEARNING \\nArtificial intelligence \\nLearning symbolic representations of concepts. Machine learning as a search problem. Learning \\nas an approach to improving problem solving. Using prior knowledge together with training data \\nto guide learning. \\n0 Bayesian methods \\nBayes\\' theorem as the basis for calculating probabilities of hypotheses. The naive Bayes classifier. \\nAlgorithms for estimating values of unobserved variables. \\n0 Computational complexity theory \\nTheoretical bounds on the inherent complexity of different learning tasks, measured in terms of \\nthe computational effort, number of training examples, number of mistakes, etc. required in order \\nto learn. \\nControl theory \\nProcedures that learn to control processes in order to optimize predefined objectives and that learn \\nto predict the next state of the process they are controlling. \\n0 Information theory \\nMeasures of entropy and information content. Minimum description length approaches to learning. \\nOptimal codes and their relationship to optimal training sequences for encoding a hypothesis. \\nPhilosophy \\nOccam\\'s razor, suggesting that the simplest hypothesis is the best. Analysis of the justification for \\ngeneralizing beyond observed data. \\n0 Psychology and neurobiology \\nThe power law of practice, which states that over a very broad range of learning problems, \\npeople\\'s response time improves with practice according to a power law. Neurobiological studies \\nmotivating artificial neural network models of learning. \\n0 Statistics \\nCharacterization of errors \\n(e.g., \\nbias and variance) that occur when estimating the accuracy of a \\nhypothesis based on a limited sample of data. Confidence intervals, statistical tests. \\nTABLE 1.2 \\nSome disciplines and examples of their influence on machine learning. \\n0 Training experience E: a database of handwritten words with given classi- \\nfications \\nA robot driving learning problem: \\n0 Task T: driving on public four-lane highways using vision sensors \\n0 Performance measure P: average distance traveled before an error (as judged \\nby human overseer) \\n0 Training experience E: a sequence of images and steering commands record- \\ned while observing a human driver \\nOur definition of learning is broad enough to include most tasks that we \\nwould conventionally call \"learning\" tasks, as we use the word in everyday lan- \\nguage. It is also broad enough to encompass computer programs that improve \\nfrom experience in quite straightforward ways. For example, a database system '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 16}, page_content='CHAFTlB 1 INTRODUCTION 5 \\nthat allows users to update data entries would fit our definition of a learning \\nsystem: it improves its performance at answering database queries, based on the \\nexperience gained from database updates. Rather than \\nworry \\nabout whether this \\ntype of activity falls under the usual informal conversational meaning of the word \\n\"learning,\" we will simply adopt our technical definition of the class of programs \\nthat improve through experience. Within this class we will find many types of \\nproblems that require more or less sophisticated solutions. Our concern here is \\nnot to analyze the meaning of the English word \"learning\" as it is used in ev- \\neryday language. Instead, our goal is to define precisely a class of problems that \\nencompasses interesting forms of learning, to explore algorithms that solve such \\nproblems, and to understand the fundamental structure of learning problems and \\nprocesses. \\n1.2 DESIGNING A LEARNING SYSTEM \\nIn order to illustrate some of the basic design issues and approaches to machine \\nlearning, let us consider designing a program to learn to play checkers, with \\nthe goal of entering it in the world checkers tournament. We adopt the obvious \\nperformance measure: the percent of games it wins in this world tournament. \\n1.2.1 Choosing the Training Experience \\nThe first design choice we face is to choose the type of training experience from \\nwhich our system will learn. The type of training experience available can have a \\nsignificant impact on success or failure of the learner. One key attribute is whether \\nthe training experience provides direct or indirect feedback regarding the choices \\nmade by the performance system. For example, in learning to play checkers, the \\nsystem might learn from direct training examples consisting of individual checkers \\nboard states and the correct move for each. Alternatively, it might have available \\nonly indirect information consisting of the move sequences and final outcomes \\nof various games played. In this later case, information about the correctness \\nof specific moves early in the game must be inferred indirectly from the fact \\nthat the game was eventually won or lost. Here the learner faces an additional \\nproblem of credit assignment, or determining the degree to which each move in \\nthe sequence deserves credit or blame for the final outcome. Credit assignment can \\nbe a particularly difficult problem because the game can be lost even when early \\nmoves are optimal, if these are followed later by poor moves. Hence, learning from \\ndirect training feedback is typically easier than learning from indirect feedback. \\nA second important attribute of the training experience is the degree to which \\nthe learner controls the sequence of training examples. For example, the learner \\nmight rely on the teacher to select informative board states and to provide the \\ncorrect move for each. Alternatively, the learner might itself propose board states \\nthat it finds particularly confusing and ask the teacher for the correct move. Or the \\nlearner may have complete control over both the board states and (indirect) training \\nclassifications, as it does when it learns by playing against itself with no teacher '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 17}, page_content=\"present. Notice in this last case the learner may choose between experimenting \\nwith novel board states that it has not yet considered, or honing its skill by playing \\nminor variations of lines of play it currently finds \\nmost \\npromising. Subsequent \\nchapters consider a number of settings for learning, including settings in which \\ntraining experience is provided by a random process outside the learner's control, \\nsettings in which the learner may pose various types of queries to an expert teacher, \\nand settings in which the learner collects training examples by autonomously \\nexploring its environment. \\nA third important attribute of the training experience is how well it repre- \\nsents the distribution of examples over which the final system performance P must \\nbe measured. In general, learning is most reliable when the training examples fol- \\nlow a distribution similar to that of future test examples. In our checkers learning \\nscenario, the performance metric P is the percent of games the system wins in \\nthe world tournament. If its training experience E consists only of games played \\nagainst itself, there is an obvious danger that this training experience might not \\nbe fully representative of the distribution of situations over which it will later be \\ntested. For example, the learner might never encounter certain crucial board states \\nthat are very likely to be played by the human checkers champion. In practice, \\nit is often necessary to learn from a distribution of examples that is somewhat \\ndifferent from those on which the final system will be evaluated \\n(e.g., \\nthe world \\ncheckers champion might not be interested in teaching the program!). Such situ- \\nations are problematic because mastery of one distribution of examples will not \\nnecessary lead to strong performance over some other distribution. We shall see \\nthat most current theory of machine learning rests on the crucial assumption that \\nthe distribution of training examples is identical to the distribution of test ex- \\namples. Despite our need to make this assumption in order to obtain theoretical \\nresults, it is important to keep in mind that this assumption must often be violated \\nin practice. \\nTo proceed with our design, let us decide that our system will train by \\nplaying games against itself. This has the advantage that no external trainer need \\nbe present, and it therefore allows the system to generate as much training data \\nas time permits. We now have a fully specified learning task. \\nA checkers learning problem: \\n0 Task T: playing checkers \\n0 Performance measure P: percent of games won in the world tournament \\n0 Training experience E: games played against itself \\nIn order to complete the design of the learning system, we must now choose \\n1. the exact type of knowledge to be,learned \\n2. a representation for this target knowledge \\n3. a learning mechanism \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 18}, page_content='CHAFTER I INTRODUCTION 7 \\n1.2.2 Choosing the Target Function \\nThe next design choice is to determine exactly what type of knowledge will be \\nlearned \\nand how this will be used by the performance program. Let us begin with \\na \\ncheckers-playing program that can generate the legal moves from any board \\nstate. The program needs only to learn how to choose the best move from among \\nthese \\nlegal moves. This learning task is representative of a large class of tasks for \\nwhich the legal moves that define some large search space are known a \\npriori, \\nbut \\nfor which the best search strategy is not known. Many optimization problems fall \\ninto this class, such as the problems of scheduling and controlling manufacturing \\nprocesses where the available manufacturing steps are well understood, but the \\nbest strategy for sequencing them is not. \\nGiven this setting where we must learn to choose among the legal moves, \\nthe most obvious choice for the type of information to be learned is a program, \\nor function, that chooses the best move for any given board state. Let us call this \\nfunction ChooseMove and use the notation ChooseMove : B -+ M to indicate \\nthat this function accepts as input any board from the set of legal board states B \\nand produces as output some move from the set of legal moves M. Throughout \\nour discussion of machine learning we will find it useful to reduce the problem \\nof improving performance P at task T to the problem of learning some particu- \\nlar targetfunction such as ChooseMove. The choice of the target function will \\ntherefore be a key design choice. \\nAlthough ChooseMove is an obvious choice for the target function in our \\nexample, this function will turn out to be very difficult to learn given the kind of in- \\ndirect training experience available to our system. An alternative target function- \\nand one that will turn out to be easier to learn in this setting-is an evaluation \\nfunction that assigns a numerical score to any given board state. Let us call this \\ntarget function V and again use the notation V : B + 8 to denote that V maps \\nany legal board state from the set B to some real value (we use 8 to denote the set \\nof real numbers). We intend for this target function V to assign higher scores to \\nbetter board states. If the system can successfully learn such a target function V, \\nthen it can easily use it to select the best move from any current board position. \\nThis can be accomplished by generating the successor board state produced by \\nevery legal move, then using V to choose the best successor state and therefore \\nthe best legal move. \\nWhat exactly should be the value of the target function V for any given \\nboard state? Of course any evaluation function that assigns higher scores to better \\nboard states will do. Nevertheless, we will find it useful to define one particular \\ntarget function V among the many that produce optimal play. As we shall see, \\nthis will make it easier to design a training algorithm. Let us therefore define the \\ntarget value V(b) for an arbitrary board state b in B, as follows: \\n1. if b is a final board state that is won, then V(b) = 100 \\n2. if b is a final board state that is lost, then V(b) = -100 \\n3. if b is a final board state that is drawn, then V(b) = 0 '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 19}, page_content=\"4. if b is a not a final state in the game, then V(b) = V(bl), where b' is the best \\nfinal board state that can be achieved starting from b and playing optimally \\nuntil the end of the game (assuming the opponent plays optimally, as well). \\nWhile this recursive definition specifies a value of \\nV(b) \\nfor every board \\nstate b, this definition is not usable by our checkers player because it is not \\nefficiently computable. Except for the trivial cases (cases 1-3) in which the game \\nhas already ended, determining the value of \\nV(b) \\nfor a particular board state \\nrequires (case 4) searching ahead for the optimal line of play, all the way to \\nthe end of the game! Because this definition is not efficiently computable by our \\ncheckers playing program, we say that it is a nonoperational definition. The goal \\nof learning in this case is to discover an operational description of V; that is, a \\ndescription that can be used by the checkers-playing program to evaluate states \\nand select moves within realistic time bounds. \\nThus, we have reduced the learning task in this case to the problem of \\ndiscovering an operational description of the ideal targetfunction V. It may be \\nvery difficult in general to learn such an operational form of V perfectly. In fact, \\nwe often expect learning algorithms to acquire only some approximation to the \\ntarget function, and for this reason the process of learning the target function \\nis often called function approximation. In the current discussion we will use the \\nsymbol ? to refer to the function that is actually learned by our program, to \\ndistinguish it from the ideal target function V. \\n1.23 Choosing a Representation for the Target Function \\nNow that we have specified the ideal target function V, we must choose a repre- \\nsentation that the learning program will use to describe the function c that it will \\nlearn. As with earlier design choices, we again have many options. We could, \\nfor example, allow the program to represent using a large table with a distinct \\nentry specifying the value for each distinct board state. Or we could allow it to \\nrepresent using a collection of rules that match against features of the board \\nstate, or a quadratic polynomial function of predefined board features, or an arti- \\nficial neural network. In general, this choice of representation involves a crucial \\ntradeoff. On one hand, we wish to pick a very expressive representation to allow \\nrepresenting as close an approximation as possible to the ideal target function V. \\nOn the other hand, the more expressive the representation, the more training data \\nthe program will require in order to choose among the alternative hypotheses it \\ncan represent. To keep the discussion brief, let us choose a simple representation: \\nfor any given board state, the function c will be calculated as a linear combination \\nof the following board features: \\n0 xl: the number of black pieces on the board \\nx2: the number of red pieces on the board \\n0 xs: the number of black kings on the board \\n0 x4: the number of red kings on the board \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 20}, page_content=\"CHAPTER I INTRODUCTION 9 \\nx5: the number of black pieces threatened by red (i.e., which can be captured \\non red's next turn) \\nX6: the number of red pieces threatened by black \\nThus, our learning program will represent \\nc(b) \\nas a linear function of the \\nform \\nwhere wo through W6 are numerical coefficients, or weights, to be chosen by the \\nlearning algorithm. Learned values for the weights wl through W6 will determine \\nthe relative importance of the various board features in determining the value of \\nthe board, whereas the weight wo will provide an additive constant to the board \\nvalue. \\nTo summarize our design choices thus far, we have elaborated the original \\nformulation of the learning problem by choosing a type of training experience, \\na target function to be learned, and a representation for this target function. Our \\nelaborated learning task is now \\nPartial design of a checkers learning program: \\nTask T: playing checkers \\nPerformance measure P: percent of games won in the world tournament \\nTraining experience E: games played against itself \\nTargetfunction: V:Board + 8 \\nTargetfunction representation \\nThe first three items above correspond to the specification of the learning task, \\nwhereas the final two items constitute design choices for the implementation of the \\nlearning program. Notice the net effect of this set of design choices is to reduce \\nthe problem of learning a checkers strategy to the problem of learning values for \\nthe coefficients wo through w6 in the target function representation. \\n1.2.4 Choosing a Function Approximation Algorithm \\nIn order to learn the target function f we require a set of training examples, each \\ndescribing a specific board state b and the training value \\nVtrain(b) \\nfor b. In other \\nwords, each training example is an ordered pair of the form (b, \\nV',,,i,(b)). \\nFor \\ninstance, the following training example describes a board state b in which black \\nhas won the game (note x2 = 0 indicates that red has no remaining pieces) and \\nfor which the target function value \\nVZrain(b) \\nis therefore +100. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 21}, page_content=\"10 MACHINE LEARNING \\nBelow we describe a procedure that first derives such training examples from \\nthe indirect training experience available to the learner, then adjusts the weights \\nwi to best fit these training examples. \\n1.2.4.1 ESTIMATING TRAINING VALUES \\nRecall that according to our formulation of the learning problem, the only training \\ninformation available to our learner is whether the game was eventually won or \\nlost. On the other hand, we require training examples that assign specific scores \\nto specific board states. While it is easy to assign a value to board states that \\ncorrespond to the end of the game, it is less obvious how to assign training values \\nto the more numerous intermediate board states that occur before the game's end. \\nOf course the fact that the game was eventually won or lost does not necessarily \\nindicate that every board state along the game path was necessarily good or bad. \\nFor example, even if the program loses the game, it may still be the case that \\nboard states occurring early in the game should be rated very highly and that the \\ncause of the loss was a subsequent poor move. \\nDespite the ambiguity inherent in estimating training values for intermediate \\nboard states, one simple approach has been found to be surprisingly successful. \\nThis approach is to assign the training value of Krain(b) for any intermediate board \\nstate b to be ?(~uccessor(b)), where ? is the learner's current approximation to \\nV and where Successor(b) denotes the next board state following b for which it \\nis again the program's turn to move (i.e., the board state following the program's \\nmove and the opponent's response). This rule for estimating training values can \\nbe summarized as \\n~ulk for estimating training values. \\nV,,,i. (b) c c(~uccessor(b)) \\nWhile it may seem strange to use the current version of f to estimate training \\nvalues that will be used to refine this very same function, notice that we are using \\nestimates of the value of the Successor(b) to estimate the value of board state b. In- \\ntuitively, we can see this will make sense if ? tends to be more accurate for board \\nstates closer to game's end. In fact, under certain conditions (discussed in Chap- \\nter 13) the approach of iteratively estimating training values based on estimates of \\nsuccessor state values can be proven to converge toward perfect estimates of Vtrain. \\n1.2.4.2 ADJUSTING THE WEIGHTS \\nAll that remains is to specify the learning algorithm for choosing the weights wi  to^ \\nbest fit the set of training examples {(b, Vtrain(b))}. As a first step we must define \\nwhat we mean by the bestfit to the training data. One common approach is to \\ndefine the best hypothesis, or set of weights, as that which minimizes the \\nsquarg \\nerror E between the training values and the values predicted by the hypothesis V. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 22}, page_content='Thus, we seek the weights, or equivalently the c, that minimize E for the observed \\ntraining examples. Chapter 6 discusses settings in which minimizing the sum of \\nsquared errors is equivalent to finding the most probable hypothesis given the \\nobserved training data. \\nSeveral algorithms are known for finding weights of a linear function that \\nminimize E defined in this way. In our case, we require an algorithm that will \\nincrementally refine the weights as new training examples become available and \\nthat will be robust to errors in these estimated training values. One such algorithm \\nis called the least mean squares, or LMS training rule. For each observed training \\nexample it adjusts the weights a small amount in the direction that reduces the \\nerror on this training example. As discussed in Chapter 4, this algorithm can be \\nviewed as performing a stochastic gradient-descent search through the space of \\npossible hypotheses (weight values) to minimize the squared \\nenor E. The LMS \\nalgorithm is defined as follows: \\nLMS weight update rule. \\nFor each training example (b, Kmin(b)) \\nUse the current weights to calculate ?(b) \\nFor each weight mi, update it as \\nHere q is a small constant (e.g., 0.1) that moderates the size of the weight update. \\nTo get an intuitive understanding for why this weight update rule works, notice \\nthat when the error \\n(Vtrain(b) - c(b)) is zero, no weights are changed. When \\n(V,,ain(b) - e(b)) is positive (i.e., when f(b) is too low), then each weight is \\nincreased in proportion to the value of its corresponding feature. This will raise \\nthe value of ?(b), reducing the error. Notice that if the value of some feature \\nxi is zero, then its weight is not altered regardless of the error, so that the only \\nweights updated are those whose features actually occur on the training example \\nboard. Surprisingly, in certain settings this simple weight-tuning method can be \\nproven to converge to the least squared error approximation to the \\n&,in \\nvalues \\n(as discussed in Chapter 4). \\n1.2.5 The Final Design \\nThe final design of our checkers learning system can be naturally described by four \\ndistinct program modules that represent the central components in many learning \\nsystems. These four modules, summarized in Figure 1.1, are as follows: \\n0 The Performance System is the module that must solve the given per- \\nformance task, in this case playing checkers, by using the learned target \\nfunction(s). \\nIt takes an instance of a new problem (new game) as input and \\nproduces a trace of its solution (game history) as output. In our case, the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 23}, page_content='12 MACHINE LEARNING \\nExperiment \\nGenerator \\nNew problem \\nHypothesis \\n(initial game board) f VJ \\nPerformance Generalizer \\nSystem \\nSolution tract Training examples \\n(game history) /<bl .Ymtn (blJ >. <bZ. Em(b2) >. ... I \\nCritic \\nFIGURE 1.1 \\nFinal design of the checkers learning program. \\nstrategy used by the Performance System to select its next move at each step \\nis determined by the learned p evaluation function. Therefore, we expect \\nits performance to improve as this evaluation function becomes increasingly \\naccurate. \\ne The Critic takes as input the history or trace of the game and produces as \\noutput a set of training examples of the target function. As shown in the \\ndiagram, each training example in this case corresponds to some game state \\nin the trace, along with an estimate Vtrai, of the target function value for this \\nexample. In our example, the Critic corresponds to the training rule given \\nby Equation (1.1). \\nThe Generalizer takes as input the training examples and produces an output \\nhypothesis that is its estimate of the target function. It generalizes from the \\nspecific training examples, hypothesizing a general function that covers these \\nexamples and other cases beyond the training examples. In our example, the \\nGeneralizer corresponds to the LMS algorithm, and the output hypothesis is \\nthe function f described by the learned weights wo, . . . , W6. \\nThe Experiment Generator takes as input the current hypothesis (currently \\nlearned function) and outputs a new problem \\n(i.e., \\ninitial board state) for the \\nPerformance System to explore. Its role is to pick new practice problems that \\nwill maximize the learning rate of the overall system. In our example, the \\nExperiment Generator follows a very simple strategy: It always proposes the \\nsame initial game board to begin a new game. More sophisticated strategies '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 24}, page_content='could involve creating board positions designed to explore particular regions \\nof the state space. \\nTogether, the design choices we made for our checkers program produce \\nspecific instantiations for the performance system, critic; generalizer, and experi- \\nment generator. Many machine learning systems can-be usefully characterized in \\nterms of these four generic modules. \\nThe sequence of design choices made for the checkers program is summa- \\nrized in Figure 1.2. These design choices have constrained the learning task in a \\nnumber of ways. We have restricted the type of knowledge that can be acquired \\nto a single linear evaluation function. Furthermore, we have constrained this eval- \\nuation function to depend on only the six specific board features provided. If the \\ntrue target function V can indeed be represented by a linear combination of these \\nDetermine Type \\nof Training Experience 1 \\nDetermine \\nTarget Function I \\nI \\nDetermine Representation \\nof Learned Function \\n... \\nLinear function Artificial neural \\nof six features network \\n/ \\\\ \\nI \\nDetermine \\nLearning \\nAlgorithm I \\nFIGURE 1.2 \\nSununary of choices in designing the checkers learning program. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 25}, page_content='particular features, then our program has a good chance to learn it. If not, then the \\nbest we can hope for is that it will learn a good approximation, since a program \\ncan certainly never learn anything that it cannot at least represent. \\nLet us suppose that a good approximation to the true V function can, in fact, \\nbe represented in this form. The question then arises as to whether this learning \\ntechnique is guaranteed to find one. Chapter 13 provides a theoretical analysis \\nshowing that under rather restrictive assumptions, variations on this approach \\ndo indeed converge to the desired evaluation function for certain types of search \\nproblems. Fortunately, practical experience indicates that this approach to learning \\nevaluation functions is often successful, even outside the range of situations for \\nwhich such guarantees can be proven. \\nWould the program we have designed be able to learn well enough to beat \\nthe human checkers world champion? Probably not. In part, this is because the \\nlinear function representation for ? is too simple a representation to capture well \\nthe nuances of the game. However, given a more sophisticated representation for \\nthe target function, this general approach can, in fact, be quite successful. For \\nexample, Tesauro (1992, 1995) reports a similar design for a program that learns \\nto play the game of backgammon, by learning a very similar evaluation function \\nover states of the game. His program represents the learned evaluation function \\nusing an artificial neural network that considers the complete description of the \\nboard state rather than a subset of board features. After training on over one million \\nself-generated training games, his program was able to play very competitively \\nwith top-ranked human backgammon players. \\nOf course we could have designed many alternative algorithms for this \\ncheckers learning task. One might, for example, simply store the given training \\nexamples, then try to find the \"closest\" stored situation to match any new situation \\n(nearest neighbor algorithm, Chapter 8). Or we might generate a large number of \\ncandidate checkers programs and allow them to play against each other, keep- \\ning only the most successful programs and further elaborating or mutating these \\nin a kind of simulated evolution (genetic algorithms, Chapter 9). Humans seem \\nto follow yet a different approach to learning strategies, in which they analyze, \\nor explain to themselves, the reasons underlying specific successes and failures \\nencountered during play (explanation-based learning, Chapter 11). Our design is \\nsimply one of many, presented here to ground our discussion of the decisions that \\nmust go into designing a learning method for a specific class of tasks. \\n1.3 PERSPECTIVES AND ISSUES IN MACHINE LEARNING \\nOne useful perspective on machine learning is that it involves searching a very \\nlarge space of possible hypotheses to determine one that best fits the observed data \\nand any prior knowledge held by the learner. For example, consider the space of \\nhypotheses that could in principle be output by the above checkers learner. This \\nhypothesis space consists of all evaluation functions that can be represented by \\nsome choice of values for the weights wo through w6. The learner\\'s task is thus to \\nsearch through this vast space to locate the hypothesis that is most consistent with '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 26}, page_content=\"the available training examples. The LMS algorithm for fitting weights achieves \\nthis goal by iteratively tuning the weights, adding a correction to each weight \\neach time the hypothesized evaluation function predicts a value that differs from \\nthe training value. This algorithm works well when the hypothesis representation \\nconsidered by the learner defines a continuously parameterized space of potential \\nhypotheses. \\nMany of the chapters in this book present algorithms that search a hypothesis \\nspace defined by some underlying representation \\n(e.g., \\nlinear functions, logical \\ndescriptions, decision trees, artificial neural networks). These different hypothesis \\nrepresentations are appropriate for learning different kinds of target functions. For \\neach of these hypothesis representations, the corresponding learning algorithm \\ntakes advantage of a different underlying structure to organize the search through \\nthe hypothesis space. \\nThroughout this book we will return to this perspective of learning as a \\nsearch problem in order to characterize learning methods by their search strategies \\nand by the underlying structure of the search spaces they explore. We will also \\nfind this viewpoint useful in formally analyzing the relationship between the size \\nof the hypothesis space to be searched, the number of training examples available, \\nand the confidence we can have that a hypothesis consistent with the training data \\nwill correctly generalize to unseen examples. \\n1.3.1 Issues in Machine Learning \\nOur checkers example raises a number of generic questions about machine learn- \\ning. The field of machine learning, and much of this book, is concerned with \\nanswering questions such as the following: \\nWhat algorithms exist for learning general target functions from specific \\ntraining examples? In what settings will particular algorithms converge to the \\ndesired function, given sufficient training data? Which algorithms perform \\nbest for which types of problems and representations? \\nHow much training data is sufficient? What general bounds can be found \\nto relate the confidence in learned hypotheses to the amount of training \\nexperience and the character of the learner's hypothesis space? \\nWhen and how can prior knowledge held by the learner guide the process \\nof generalizing from examples? Can prior knowledge be helpful even when \\nit is only approximately correct? \\nWhat is the best strategy for choosing a useful next training experience, and \\nhow does the choice of this strategy alter the complexity of the learning \\nproblem? \\nWhat is the best way to reduce the learning task to one or more function \\napproximation problems? Put another way, what specific functions should \\nthe system attempt to learn? Can this process itself be automated? \\nHow can the learner automatically alter its representation to improve its \\nability to represent and learn the target function? \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 27}, page_content=\"16 MACHINE LEARNING \\n1.4 HOW TO READ THIS BOOK \\nThis book contains an introduction to the primary algorithms and approaches to \\nmachine learning, theoretical results on the feasibility of various learning tasks \\nand the capabilities of specific algorithms, and examples of practical applications \\nof machine learning to real-world problems. Where possible, the chapters have \\nbeen written to be readable in any sequence. However, some interdependence \\nis unavoidable. If this is being used as a class text, I recommend first covering \\nChapter 1 and Chapter 2. Following these two chapters, the remaining chapters \\ncan be read in nearly any sequence. A one-semester course in machine learning \\nmight cover the first seven chapters, followed by whichever additional chapters \\nare of greatest interest to the class. Below is a brief survey of the chapters. \\nChapter 2 covers concept learning based on symbolic or logical representa- \\ntions. It also discusses the general-to-specific ordering over hypotheses, and \\nthe need for inductive bias in learning. \\n0 Chapter 3 covers decision tree learning and the problem of overfitting the \\ntraining data. It also examines Occam's razor-a principle recommending \\nthe shortest hypothesis among those consistent with the data. \\n0 Chapter 4 covers learning of artificial neural networks, especially the well- \\nstudied \\nBACKPROPAGATION \\nalgorithm, and the general approach of gradient \\ndescent. This includes a detailed example of neural network learning for \\nface recognition, including data and algorithms available over the World \\nWide Web. \\n0 Chapter 5 presents basic concepts from statistics and estimation theory, fo- \\ncusing on evaluating the accuracy of hypotheses using limited samples of \\ndata. This includes the calculation of confidence intervals for estimating \\nhypothesis accuracy and methods for comparing the accuracy of learning \\nmethods. \\n0 Chapter 6 covers the Bayesian perspective on machine learning, including \\nboth the use of Bayesian analysis to characterize non-Bayesian learning al- \\ngorithms and specific Bayesian algorithms that explicitly manipulate proba- \\nbilities. This includes a detailed example applying a naive Bayes classifier to \\nthe task of classifying text documents, including data and software available \\nover the World Wide Web. \\n0 Chapter 7 covers computational learning theory, including the Probably Ap- \\nproximately Correct (PAC) learning model and the Mistake-Bound learning \\nmodel. This includes a discussion of the WEIGHTED MAJORITY algorithm for \\ncombining multiple learning methods. \\n0 Chapter 8 describes instance-based learning methods, including nearest neigh- \\nbor learning, locally weighted regression, and case-based reasoning. \\n0 Chapter 9 discusses learning algorithms modeled after biological evolution, \\nincluding genetic algorithms and genetic programming. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 28}, page_content='0 Chapter 10 covers algorithms for learning sets of rules, including Inductive \\nLogic \\nProgramming \\napproaches to learning first-order Horn clauses. \\n0 Chapter 11 covers explanation-based learning, a learning method that uses \\nprior knowledge to explain observed training examples, then generalizes \\nbased on these explanations. \\n0 Chapter 12 discusses approaches to combining approximate prior knowledge \\nwith available training data in order to improve the accuracy of learned \\nhypotheses. Both symbolic and neural network algorithms are considered. \\n0 Chapter 13 discusses reinforcement learning-an approach to control learn- \\ning that accommodates indirect or delayed feedback as training information. \\nThe checkers learning algorithm described earlier in Chapter 1 is a simple \\nexample of reinforcement learning. \\nThe end of each chapter contains a summary of the main concepts covered, \\nsuggestions for further reading, and exercises. Additional updates to chapters, as \\nwell as data sets and implementations of algorithms, are available on the World \\nWide Web at http://www.cs.cmu.edu/-tom/mlbook.html. \\n1.5 SUMMARY AND FURTHER READING \\nMachine learning addresses the question of how to build computer programs that \\nimprove their performance at some task through experience. Major points of this \\nchapter include: \\nMachine learning algorithms have proven to be of great practical value in a \\nvariety of application domains. They are especially useful in (a) data mining \\nproblems where large databases may contain valuable implicit regularities \\nthat can be discovered automatically \\n(e.g., \\nto analyze outcomes of medical \\ntreatments from patient databases or to learn general rules for credit worthi- \\nness from financial databases); (b) poorly understood domains where humans \\nmight not have the knowledge needed to develop effective algorithms \\n(e.g., \\nhuman face recognition from images); and (c) domains where the program \\nmust dynamically adapt to changing conditions \\n(e.g., \\ncontrolling manufac- \\nturing processes under changing supply stocks or adapting to the changing \\nreading interests of individuals). \\nMachine learning draws on ideas from a diverse set of disciplines, including \\nartificial intelligence, probability and statistics, computational complexity, \\ninformation theory, psychology and neurobiology, control theory, and phi- \\nlosophy. \\n0 A well-defined learning problem requires a well-specified task, performance \\nmetric, and source of training experience. \\n0 Designing a machine learning approach involves a number of design choices, \\nincluding choosing the type of training experience, the target function to \\nbe learned, a representation for this target function, and an algorithm for \\nlearning the target function from training examples. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 29}, page_content='18 MACHINE LEARNING \\n0 Learning involves search: searching through a space of possible hypotheses \\nto find the hypothesis that best fits the available training examples and other \\nprior constraints or knowledge. Much of this book is organized around dif- \\nferent learning methods that search different hypothesis spaces \\n(e.g., \\nspaces \\ncontaining numerical functions, neural networks, decision trees, symbolic \\nrules) and around theoretical results that characterize conditions under which \\nthese search methods converge toward an optimal hypothesis. \\nThere are a number of good sources for reading about the latest research \\nresults in machine learning. Relevant journals include Machine Learning, Neural \\nComputation, Neural Networks, Journal of the American Statistical Association, \\nand the IEEE Transactions on Pattern Analysis and Machine Intelligence. There \\nare also numerous annual conferences that cover different aspects of machine \\nlearning, including the International Conference on Machine Learning, Neural \\nInformation Processing Systems, Conference on Computational Learning The- \\nory, International Conference on Genetic Algorithms, International Conference \\non Knowledge Discovery and Data Mining, European Conference on Machine \\nLearning, and others. \\nEXERCISES \\n1.1. Give three computer applications for which machine learning approaches seem ap- \\npropriate and three for which they seem inappropriate. Pick applications that are not \\nalready mentioned in this chapter, and include a one-sentence justification for each. \\n1.2. Pick some learning task not mentioned in this chapter. Describe it informally in a \\nparagraph in English. Now describe it by stating as precisely as possible the task, \\nperformance measure, and training experience. Finally, propose a target function to \\nbe learned and a target representation. Discuss the main tradeoffs you considered in \\nformulating this learning task. \\n1.3. Prove that the LMS weight update rule described in this chapter performs a gradient \\ndescent to minimize the squared error. In particular, define the squared error E as in \\nthe text. Now calculate the derivative of E with respect to the weight wi, assuming \\nthat ?(b) is a linear function as defined in the text. Gradient descent is achieved by \\nupdating each weight in proportion to -e. Therefore, you must show that the LMS \\ntraining rule alters weights in this proportion for each training example it encounters. \\n1.4. Consider alternative strategies for the Experiment Generator module of Figure 1.2. \\nIn particular, consider strategies in which the Experiment Generator suggests new \\nboard positions by \\nGenerating random legal board positions \\n0 Generating a position by picking a board state from the previous game, then \\napplying one of the moves that was not executed \\nA strategy of your own design \\nDiscuss tradeoffs among these strategies. Which do you feel would work best if the \\nnumber of training examples was held constant, given the performance measure of \\nwinning the most games at the world championships? \\n1.5. Implement an algorithm similar to that discussed for the checkers problem, but use \\nthe simpler game of tic-tac-toe. Represent the learned function V as a linear com- '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 30}, page_content='bination of board features of your choice. To train your program, play it repeatedly \\nagainst a second copy of the program that uses a fixed evaluation function you cre- \\nate by hand. Plot the percent of games won by your system, versus the number of \\ntraining games played. \\nREFERENCES \\nAhn, W., & Brewer, W. F. (1993). Psychological studies of explanation-based learning. In G. DeJong \\n(Ed.), Investigating explanation-based learning. Boston: Kluwer Academic Publishers. \\nAnderson, J. R. (1991). The place of cognitive architecture in rational analysis. In K. VanLehn (Ed.), \\nArchitectures for intelligence @p. 1-24). Hillsdale, NJ: Erlbaum. \\nChi, M. T. H., & Bassock, M. (1989). Learning from examples via self-explanations. In L. Resnick \\n(Ed.), Knowing, learning, and instruction: Essays in honor of Robert Glaser. Hillsdale, NJ: \\nL. Erlbaum Associates. \\nCooper, G., et al. (1997). An evaluation of machine-learning methods for predicting pneumonia \\nmortality. Artificial Intelligence in Medicine, (to appear). \\nFayyad, U. M., \\nUthurusamy, \\nR. (Eds.) (1995). Proceedings of the First International Conference on \\nKnowledge Discovery and Data Mining. Menlo Park, CA: AAAI Press. \\nFayyad, U. M., Smyth, P., Weir, N., Djorgovski, S. (1995). Automated analysis and exploration of \\nimage databases: Results, progress, and challenges. Journal of Intelligent Information Systems, \\n4, 1-19. \\nLaird, J., Rosenbloom, P., & Newell, A. (1986). SOAR: The anatomy of a general learning mecha- \\nnism. Machine Learning, 1(1), 1146. \\nLangley, P., & Simon, H. (1995). Applications of machine learning and rule induction. Communica- \\ntions of the ACM, 38(1 I), 55-64. \\nLee, K. (1989). Automatic speech recognition: The development of the Sphinx system. Boston: Kluwer \\nAcademic Publishers. \\nPomerleau, \\nD. A. (1989). ALVINN: An autonomous land vehicle in a neural network. (Technical \\nReport CMU-CS-89-107). Pittsburgh, PA: Carnegie Mellon University. \\nQin, Y., Mitchell, T., & Simon, H. (1992). Using EBG to simulate human learning from examples \\nand learning by doing. Proceedings of the Florida AI Research Symposium (pp. 235-239). \\nRudnicky, A. I., Hauptmann, A. G., & Lee, K. -F. (1994). Survey of current speech technology in \\nartificial intelligence. Communications of the ACM, 37(3), 52-57. \\nRumelhart, D., \\nWidrow, \\nB., & Lehr, M. (1994). The basic ideas in neural networks. Communications \\nof the ACM, 37(3), 87-92. \\nTesauro, G. (1992). Practical issues in temporal difference learning. Machine Learning, 8, 257. \\nTesauro, G. (1995). Temporal difference learning and TD-gammon. Communications of the ACM, \\n38(3), 5848. \\nWaibel, \\nA,, \\nHanazawa, T., \\nHinton, G., Shikano, K., & Lang, K. (1989). Phoneme recognition using \\ntime-delay neural networks. IEEE Transactions on Acoustics, Speech and Signal Processing, \\n37(3), 328-339. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 31}, page_content='CHAPTER \\nCONCEPT \\nLEARNING \\nAND THE \\nGENERAL-TO-SPECIFIC \\n0,RDERING \\nThe problem of inducing general functions from specific training examples is central \\nto learning. This chapter considers concept learning: acquiring the definition of a \\ngeneral category given a sample of positive and negative training examples of the \\ncategory. Concept learning can be formulated as a problem of searching through a \\npredefined space of potential hypotheses for the hypothesis that best fits the train- \\ning examples. In many cases this search can be efficiently organized by taking \\nadvantage of a naturally occurring structure over the hypothesis space-a \\ngeneral- \\nto-specific ordering of hypotheses. This chapter presents several learning algorithms \\nand considers situations under which they converge to the correct hypothesis. We \\nalso examine the nature of inductive learning and the justification by which any \\nprogram may successfully generalize beyond the observed training data. \\n2.1 INTRODUCTION \\nMuch of learning involves acquiring general concepts from specific training exam- \\nples. People, for example, continually learn general concepts or categories such \\nas \"bird,\" \"car,\" \"situations in which I should study more in order to pass the \\nexam,\" etc. Each such concept can be viewed as describing some subset of ob- \\njects or events defined over a larger set \\n(e.g., \\nthe subset of animals that constitute '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 32}, page_content='CHAFER 2 CONCEm LEARNING AND THE GENERAL-TO-SPECIFIC ORDERWG 21 \\nbirds). Alternatively, each concept can be thought of as a boolean-valued function \\ndefined over this larger set \\n(e.g., \\na function defined over all animals, whose value \\nis true for birds and false for other animals). \\nIn this chapter we consider the problem of automatically inferring the general \\ndefinition of some concept, given examples labeled \\nas+.members \\nor nonmembers \\nof the concept. This task is commonly referred to as concept learning, or approx- \\nimating a boolean-valued function from examples. \\nConcept learning. Inferring a boolean-valued function from training examples of \\nits input and output. \\n2.2 A CONCEPT LEARNING TASK \\nTo ground our discussion of concept learning, consider the example task of learn- \\ning the target concept \"days on which my friend Aldo enjoys his favorite water \\nsport.\" Table 2.1 describes a set of example days, each represented by a set of \\nattributes. The attribute EnjoySport indicates whether or not Aldo enjoys his \\nfavorite water sport on this day. The task is to learn to predict the value of \\nEnjoySport for an arbitrary day, based on the values of its other attributes. \\nWhat hypothesis representation shall we provide to the learner in this case? \\nLet us begin by considering a simple representation in which each hypothesis \\nconsists of a conjunction of constraints on the instance attributes. In particular, \\nlet each hypothesis be a vector of six constraints, specifying the values of the six \\nattributes Sky, AirTemp, Humidity, Wind, Water, and Forecast. For each attribute, \\nthe hypothesis will either \\n0 indicate by a \"?\\' that any value is acceptable for this attribute, \\n0 specify a single required value (e.g., Warm) for the attribute, or \\n0 indicate by a \"0\" that no value is acceptable. \\nIf some instance x satisfies all the constraints of hypothesis h, then h clas- \\nsifies x as a positive example \\n(h(x) = 1). To illustrate, the hypothesis that Aldo \\nenjoys his favorite sport only on cold days with high humidity (independent of \\nthe values of the other attributes) is represented by the expression \\n(?, Cold, High, ?, ?, ?) \\nExample Sky AirTemp Humidity Wind Water Forecast EnjoySport \\n1 Sunny Warm Normal Strong Warm Same Yes \\n2 Sunny Warm High Strong Warm Same Yes \\n3 Rainy Cold High Strong Warm Change No \\n4 Sunny Warm High Strong Cool Change Yes \\nTABLE 2.1 \\nPositive and negative training examples for the target concept EnjoySport. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 33}, page_content='22 MACHINE LEARNING \\nThe most general hypothesis-that every day is a positive example-is repre- \\nsented by \\n(?, ?, ?, ?, ?, ?) \\nand the most specific possible hypothesis-that no day is a positive example-is \\nrepresented by \\n(0,0,0,0,0,0) \\nTo summarize, the EnjoySport concept learning task requires learning the \\nset of days for which EnjoySport = yes, describing this set by a conjunction \\nof constraints over the instance attributes. In general, any concept learning task \\ncan be described by the set of instances over which the target function is defined, \\nthe target function, the set of candidate hypotheses considered by the learner, and \\nthe set of available training examples. The definition of the EnjoySport concept \\nlearning task in this general form is given in Table 2.2. \\n2.2.1 Notation \\nThroughout this book, we employ the following terminology when discussing \\nconcept learning problems. The set of items over which the concept is defined \\nis called the set of instances, which we denote by X. In the current example, X \\nis the set of all possible days, each represented by the attributes Sky, AirTemp, \\nHumidity, Wind, Water, and Forecast. The concept or function to be learned is \\ncalled the target concept, which we denote by c. In general, c can be any boolean- \\nvalued function defined over the instances X; that is, c : X + {O, 1). In the current \\nexample, the target concept corresponds to the value of the attribute EnjoySport \\n(i.e., \\nc(x) = 1 if EnjoySport = Yes, and c(x) = 0 if EnjoySport = No). \\n- \\n0 Given: \\n0 Instances X: Possible days, each described by the attributes \\n0 Sky (with possible values Sunny, Cloudy, and Rainy), \\n0 AirTemp (with values Warm and Cold), \\n0 Humidity (with values Normal and High), \\n0 Wind (with values Strong and Weak), \\n0 Water (with values Warm and Cool), and \\n0 Forecast (with values Same and Change). \\n0 Hypotheses H: Each hypothesis is described by a conjunction of constraints on the at- \\ntributes Sky, AirTemp, Humidity, Wind, Water, and Forecast. The constraints may be \"?\" \\n(any value is acceptable), \"0 (no value is acceptable), or a specific value. \\n0 Target concept c: EnjoySport : X + (0,l) \\n0 Training examples D: Positive and negative examples of the target function (see Table 2.1). \\n0 Determine: \\n0 A hypothesis h in H such that h(x) = c(x) for all x in X. \\nTABLE 2.2 \\nThe EnjoySport concept learning task. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 34}, page_content='When learning the target concept, the learner is presented a set of training \\nexamples, each consisting of an instance x from X, along with its target concept \\nvalue c(x) (e.g., the training examples in Table 2.1). Instances for which c(x) = 1 \\nare called positive examples, or members of the target concept. Instances for which \\nC(X) = 0 are called negative examples, or nonmembers of the target concept. \\nWe will often write the ordered pair (x, c(x)) to describe the training example \\nconsisting of the instance x and its target concept value c(x). We use the symbol \\nD to denote the set of available training examples. \\nGiven a set of training examples of the target concept c, the problem faced \\nby the learner is to hypothesize, or estimate, c. We use the symbol H to denote \\nthe set of all possible hypotheses that the learner may consider regarding the \\nidentity of the target concept. Usually H is determined by the human designer\\'s \\nchoice of hypothesis representation. In general, each hypothesis h in H represents \\na boolean-valued function defined over X; that is, h : X --+ {O, 1). The goal of the \\nlearner is to find a hypothesis h such that h(x) = c(x) for a\" x in X. \\n2.2.2 The Inductive Learning Hypothesis \\nNotice that although the learning task is to determine a hypothesis h identical \\nto the target concept c over the entire set of instances X, the only information \\navailable about c is its value over the training examples. Therefore, inductive \\nlearning algorithms can at best guarantee that the output hypothesis fits the target \\nconcept over the training data. Lacking any further information, our assumption \\nis that the best hypothesis regarding unseen instances is the hypothesis that best \\nfits the observed training data. This is the fundamental assumption of inductive \\nlearning, and we will have much more to say about it throughout this book. We \\nstate it here informally and will revisit and analyze this assumption more formally \\nand more quantitatively in Chapters 5, 6, and 7. \\nThe inductive learning hypothesis. Any hypothesis found to approximate the target \\nfunction well over a sufficiently large set of training examples will also approximate \\nthe target function well over other unobserved examples. \\n2.3 CONCEPT LEARNING AS SEARCH \\nConcept learning can be viewed as the task of searching through a large space of \\nhypotheses implicitly defined by the hypothesis representation. The goal of this \\nsearch is to find the hypothesis that best fits the training examples. It is important \\nto note that by selecting a hypothesis representation, the designer of the learning \\nalgorithm implicitly defines the space of all hypotheses that the program can \\never represent and therefore can ever learn. Consider, for example, the instances \\nX and hypotheses H in the EnjoySport learning task. Given that the attribute \\nSky has three possible values, and that AirTemp, Humidity, Wind, Water, and \\nForecast each have two possible values, the instance space X contains exactly '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 35}, page_content='3 .2 2 .2 2 .2 = 96 distinct instances. A similar calculation shows that there are \\n5.4-4 -4 -4.4 = 5 120 syntactically distinct hypotheses within H. Notice, however, \\nthat every hypothesis containing one or more \"IZI\" symbols represents the empty \\nset of instances; that is, it classifies every instance as negative. Therefore, the \\nnumber of semantically distinct hypotheses is only 1 + (4.3.3.3.3.3) = 973. Our \\nEnjoySport \\nexample is a very simple learning task, with a relatively small, finite \\nhypothesis space. Most practical learning tasks involve much larger, sometimes \\ninfinite, hypothesis spaces. \\nIf we view learning as a search problem, then it is natural that our study \\nof learning algorithms will \\nexa~the \\ndifferent strategies for searching the hypoth- \\nesis space. We will be \\nparticula \\nly \\ninterested in algorithms capable of efficiently \\nsearching very large or infinite hypothesis spaces, to find the hypotheses that best \\nfit the training data. \\n2.3.1 General-to-Specific Ordering of Hypotheses \\nMany algorithms for concept learning organize the search through the hypothesis \\nspace by relying on a very useful structure that exists for any concept learning \\nproblem: a general-to-specific ordering of hypotheses. By taking advantage of this \\nnaturally occurring structure over the hypothesis space, we can design learning \\nalgorithms that exhaustively search even infinite hypothesis spaces without explic- \\nitly enumerating every hypothesis. To illustrate the general-to-specific ordering, \\nconsider the two hypotheses \\nhi = (Sunny, ?, ?, Strong, ?, ?) \\nh2 = (Sunny, ?, ?, ?, ?, ?) \\nNow consider the sets of instances that are classified positive by hl and by h2. \\nBecause \\nh2 \\nimposes fewer constraints on the instance, it classifies more instances \\nas positive. In fact, any instance classified positive by \\nhl \\nwill also be classified \\npositive by \\nh2. \\nTherefore, we say that \\nh2 \\nis more general than \\nhl. \\nThis intuitive \"more general than\" relationship between hypotheses can be \\ndefined more precisely as follows. First, for any instance \\nx \\nin X and hypothesis \\nh in H, we say that x satisjies h if and only if h(x) = 1. We now define the \\nmore-general~han_or.-equal~o relation in terms of the sets of instances that sat- \\nisfy the two hypotheses: Given hypotheses \\nhj \\nand \\nhk, \\nhj \\nis \\nmore-general-thanm-- \\nequaldo \\nhk \\nif and only if any instance that satisfies \\nhk \\nalso satisfies hi. \\nDefinition: Let hj and hk be boolean-valued functions defined over X. Then hj is \\nmoregeneral-than-or-equal-to hk (written hj 2, hk) if and only if \\nWe will also find it useful to consider cases where one hypothesis is strictly more \\ngeneral than the other. Therefore, we will say that \\nhj \\nis (strictly) \\nmore-generaldhan '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 36}, page_content='CHAPTER 2 CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 25 \\nImtances X Hypotheses H \\nI I \\nA \\nSpecific \\nGeneral \\nt \\ni \\nXI= <Sunny, Wan, High, Strong, Cool, Same> hl= <Sunny, ?, ?, Strong, ?, ?> \\nx = <Sunny, Warm, High, Light, Warm, Same> 2 h = <Sunny, ?, ?, ?, ?, ?> 2 \\nh 3 = <Sunny, ?, ?, 7, Cool, ?> \\nFIGURE 2.1 \\nInstances, hypotheses, and the more-general-than relation. The box on the left represents the set X \\nof all instances, the box on the right the set H of all hypotheses. Each hypothesis corresponds to \\nsome subset of X-the subset of instances that it classifies positive. The arrows connecting hypotheses \\nrepresent the more-general-than relation, with the arrow pointing toward the less general hypothesis. \\nNote the subset of instances characterized by h2 subsumes the subset characterized by hl, hence h2 \\nis more-general-than hl. \\nhk (written hj >, hk) if and only if (hj p, hk) A (hk 2, hi). Finally, we will \\nsometimes find the inverse useful and will say that hj is morespecijkthan hk \\nwhen hk is more_general-than hj. \\nTo illustrate these definitions, consider the three hypotheses hl, h2, and \\nh3 from our Enjoysport example, shown in Figure 2.1. How are these three \\nhypotheses related by the p, relation? As noted earlier, hypothesis h2 is more \\ngeneral than hl because every instance that satisfies hl also satisfies h2. Simi- \\nlarly, h2 is more general than h3. Note that neither hl nor h3 is more general \\nthan the other; although the instances satisfied by these two hypotheses intersect, \\nneither set subsumes the other. Notice also that the p, and >, relations are de- \\nfined independent of the target concept. \\nThey \\ndepend only on which instances \\nsatisfy the two hypotheses and not on the classification of those instances accord- \\ning to the target concept. Formally, the p, relation defines a partial order over \\nthe hypothesis space H (the relation is reflexive, antisymmetric, and transitive). \\nInformally, when we say the structure is a partial (as opposed to total) order, we \\nmean there may be pairs of hypotheses such as hl and h3, such that hl 2, h3 and \\nh3 2, hl. \\nThe pg relation is important because it provides a useful structure over the \\nhypothesis space H for any concept learning problem. The following sections \\npresent concept learning algorithms that take advantage of this partial order to \\nefficiently organize the search for hypotheses that fit the training data. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 37}, page_content='1. Initialize h to the most specific hypothesis in H \\n2. For each positive training instance x \\n0 For each attribute constraint a, in h \\nIf the constraint a, is satisfied by x \\nThen do nothing \\nElse replace a, in h by the next more general constraint that is satisfied by x \\n3. Output hypothesis h \\nTABLE 2.3 \\nFIND-S Algorithm. \\n2.4 FIND-S: FINDING A MAXIMALLY SPECIFIC HYPOTHESIS \\nHow can we use the more-general-than partial ordering to organize the search for \\na hypothesis consistent with the observed training examples? One way is to begin \\nwith the most specific possible hypothesis in H, then generalize this hypothesis \\neach time it fails to cover an observed positive training example. (We say that \\na hypothesis \"covers\" a positive example if it correctly classifies the example as \\npositive.) To be more precise about how the partial ordering is used, consider the \\nFIND-S algorithm defined in Table 2.3. \\nTo illustrate this algorithm, assume the learner is given the sequence of \\ntraining examples from Table 2.1 for the EnjoySport task. The first step of FIND- \\nS is to initialize h to the most specific hypothesis in H \\nUpon observing the first training example from Table 2.1, which happens to be a \\npositive example, it becomes clear that our hypothesis is too specific. In particular, \\nnone of the \"0\" constraints in h are satisfied by this example, so each is replaced \\nby the next more general constraint \\n{hat \\nfits the example; namely, the attribute \\nvalues for this training example. \\nh -+ (Sunny, Warm, Normal, Strong, Warm, Same) \\nThis h is still very specific; it asserts that all instances are negative except for \\nthe single positive training example we have observed. Next, the second training \\nexample (also positive in this case) forces the algorithm to further generalize h, \\nthis time substituting a \"?\\' in place of any attribute value in h that is not satisfied \\nby the new example. The refined hypothesis in this case is \\nh -+ (Sunny, Warm, ?, Strong, Warm, Same) \\nUpon encountering the third training example-in this case a negative exam- \\nple-the algorithm makes no change to h. In fact, the FIND-S algorithm simply \\nignores every negative example! While this may at first seem strange, notice that \\nin the current case our hypothesis h is already consistent with the new negative ex- \\nample \\n(i-e., h correctly classifies this example as negative), and hence no revision '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 38}, page_content='is needed. In the general case, as long as we assume that the hypothesis space H \\ncontains a hypothesis that describes the true target concept c and that the training \\ndata contains no errors, then the current hypothesis h can never require a revision \\nin response to a negative example. To see why, recall that the current hypothesis \\nh is the most specific hypothesis in H consistent with the observed positive exam- \\nples. Because the target concept c is also assumed to be in H and to be consistent \\nwith the positive training examples, c must be more.general_than-or-equaldo h. \\nBut the target concept c will never cover a negative example, thus neither will \\nh (by the definition of more-general~han). Therefore, no revision to h will be \\nrequired in response to any negative example. \\nTo complete our trace of FIND-S, the fourth (positive) example leads to a \\nfurther generalization of h \\nh t (Sunny, Warm, ?, Strong, ?, ?) \\nThe FIND-S algorithm illustrates one way in which the more-generaldhan \\npartial ordering can be used to organize the search for an acceptable hypothe- \\nsis. The search moves from hypothesis to hypothesis, searching from the most \\nspecific to progressively more general hypotheses along one chain of the partial \\nordering. Figure 2.2 illustrates this search in terms of the instance and hypoth- \\nesis spaces. At each step, the hypothesis is generalized only as far as neces- \\nsary to cover the new positive example. Therefore, at each stage the hypothesis \\nis the most specific hypothesis consistent with the training examples observed \\nup to this point (hence the name FIND-S). The literature on concept learning is \\nInstances X Hypotheses H \\nspecific \\nGeneral \\n* 1 = <Sunny Warm Normal Strong Warm Same>, + h, = <Sunny Warm Normal Strong Warm Same> \\nx2 = <Sunny Warm High Strong Warm Same>, + h2 = <Sunny Warm ? Strong Warm Same> \\nX3 = <Rainy Cold High Strong Warm Change>, - h = <Sunny Warm ? Strong Warm Same> 3 \\nx - <Sunny Warm High Strong Cool Change>, + h - <Sunny Warm ? Strong ? ? > 4- 4 - \\nFIGURE 2.2 \\n\\'The hypothesis space search performed by FINDS. The search begins (ho) with the most specific \\nhypothesis in H, then considers increasingly general hypotheses (hl through h4) as mandated by the \\ntraining examples. In the instance space diagram, positive training examples are denoted by \"+,\" \\nnegative by \"-,\" and instances that have not been presented as training examples are denoted by a \\nsolid circle. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 39}, page_content='populated by many different algorithms that utilize this same more-general-than \\npartial ordering to organize the search in one fashion or another. A number of \\nsuch algorithms are discussed in this chapter, and several others are presented in \\nChapter 10. \\nThe key property of the FIND-S algorithm is that for hypothesis spaces de- \\nscribed by conjunctions of attribute constraints (such as H for the EnjoySport \\ntask), FIND-S is guaranteed to output the most specific hypothesis within H \\nthat is consistent with the positive training examples. Its final hypothesis will \\nalso be consistent with the negative examples provided the correct target con- \\ncept is contained in H, and provided the training examples are correct. How- \\never, there are several questions still left unanswered by this learning algorithm, \\nsuch as: \\nHas the learner converged to the correct target concept? Although FIND-S \\nwill find a hypothesis consistent with the training data, it has no way to \\ndetermine whether it has found the only hypothesis in H consistent with \\nthe data \\n(i.e., \\nthe correct target concept), or whether there are many other \\nconsistent hypotheses as well. We would prefer a learning algorithm that \\ncould determine whether it had converged and, if not, at least characterize \\nits uncertainty regarding the true identity of the target concept. \\n0 Why prefer the most specific hypothesis? In case there are multiple hypothe- \\nses consistent with the training examples, FIND-S will find the most specific. \\nIt is unclear whether we should prefer this hypothesis over, say, the most \\ngeneral, or some other hypothesis of intermediate generality. \\n0 Are the training examples consistent? In most practical learning problems \\nthere is some chance that the training examples will contain at least some \\nerrors or noise. Such inconsistent sets of training examples can severely \\nmislead FIND-S, given the fact that it ignores negative examples. We would \\nprefer an algorithm that could at least detect when the training data is in- \\nconsistent and, preferably, accommodate such errors. \\n0 What if there are several maximally specific consistent hypotheses? In the \\nhypothesis language H for the EnjoySport task, there is always a unique, \\nmost specific hypothesis consistent with any set of positive examples. How- \\never, for other hypothesis spaces (discussed later) there can be several maxi- \\nmally specific hypotheses consistent with the data. In this case, FIND-S must \\nbe extended to allow it to backtrack on its choices of how to generalize the \\nhypothesis, to accommodate the possibility that the target concept lies along \\na different branch of the partial ordering than the branch it has selected. Fur- \\nthermore, we can define hypothesis spaces for which there is no maximally \\nspecific consistent hypothesis, although this is more of a theoretical issue \\nthan a practical one (see Exercise 2.7). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 40}, page_content='2.5 VERSION SPACES AND THE CANDIDATE-ELIMINATION \\nALGORITHM \\nThis section describes a second approach to concept learning, the CANDIDATE- \\nELIMINATION algorithm, that addresses several of the limitations of FIND-S. Notice \\nthat although FIND-S \\noutputs \\na hypothesis from \\nH,that \\nis consistent with the \\ntraining examples, this is just one of many hypotheses from H that might fit the \\ntraining data equally well. The key idea in the CANDIDATE-ELIMINATION algorithm \\nis to output a description of the set of all hypotheses consistent with the train- \\ning examples. Surprisingly, the CANDIDATE-ELIMINATION algorithm computes the \\ndescription of this set without explicitly enumerating all of its members. This is \\naccomplished by again using the more-general-than partial ordering, this time \\nto maintain a compact representation of the set of consistent hypotheses and to \\nincrementally refine this representation as each new training example is encoun- \\ntered. \\nThe CANDIDATE-ELIMINATION algorithm has been applied to problems such \\nas learning regularities in chemical mass spectroscopy (Mitchell 1979) and learn- \\ning control rules for heuristic search (Mitchell et al. 1983). Nevertheless, prac- \\ntical applications of the CANDIDATE-ELIMINATION and FIND-S algorithms are lim- \\nited by the fact that they both perform poorly when given noisy training data. \\nMore importantly for our purposes here, the CANDIDATE-ELIMINATION algorithm \\nprovides a useful conceptual framework for introducing several fundamental is- \\nsues in machine learning. In the remainder of this chapter we present the algo- \\nrithm and discuss these issues. Beginning with the next chapter, we will ex- \\namine learning algorithms that are used more frequently with noisy training \\ndata. \\n2.5.1 Representation \\nThe CANDIDATE-ELIMINATION algorithm finds all describable hypotheses that are \\nconsistent with the observed training examples. In order to define this algorithm \\nprecisely, we begin with a few basic definitions. First, let us say that a hypothesis \\nis consistent with the training examples if it correctly classifies these examples. \\nDefinition: A hypothesis h is consistent with a set of training examples D if and \\nonly if h(x) = c(x) for each example (x, c(x)) in D. \\nNotice the key difference between this definition of consistent and our earlier \\ndefinition of satisfies. An example x is said to satisfy hypothesis h when h(x) = 1, \\nregardless of whether x is a positive or negative example of the target concept. \\nHowever, whether such an example is consistent with h depends on the target \\nconcept, and in particular, whether h(x) = c(x). \\nThe CANDIDATE-ELIMINATION algorithm represents the set of all hypotheses \\nconsistent with the observed training examples. This subset of all hypotheses is '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 41}, page_content='called the version space with respect to the hypothesis space H and the training \\nexamples D, because it contains all plausible versions of the target concept. \\nDejnition: The version space, denoted VSHVD, with respect to hypothesis space H \\nand training examples D, is the subset of hypotheses from H consistent with the \\ntraining examples in D. \\nVSH,~ = {h E HIConsistent(h, D)] \\n2.5.2 The LIST-THEN-ELIMINATE Algorithm \\nOne obvious way to represent the version space is simply to list all of its members. \\nThis leads to a simple learning algorithm, which we might call the LIST-THEN- \\nELIMINATE algorithm, defined in Table 2.4. \\nThe LIST-THEN-ELIMINATE algorithm first initializes the version space to con- \\ntain all hypotheses in H, then eliminates any hypothesis found inconsistent with \\nany training example. The version space of candidate hypotheses thus shrinks \\nas more examples are observed, until ideally just one hypothesis remains that is \\nconsistent with all the observed examples. This, presumably, is the desired target \\nconcept. If insufficient data is available to narrow the version space to a single \\nhypothesis, then the algorithm can output the entire set of hypotheses consistent \\nwith the observed data. \\nIn principle, the LIST-THEN-ELIMINATE algorithm can be applied whenever \\nthe hypothesis space H is finite. It has many advantages, including the fact that it \\nis guaranteed to output all hypotheses consistent with the training data. Unfortu- \\nnately, it requires exhaustively enumerating all hypotheses in H-an unrealistic \\nrequirement for all but the most trivial hypothesis spaces. \\n2.5.3 A More Compact Representation for Version Spaces \\nThe CANDIDATE-ELIMINATION algorithm works on the same principle as the above \\nLIST-THEN-ELIMINATE algorithm. However, it employs a much more compact rep- \\nresentation of the version space. In particular, the version space is represented \\nby its most general and least general members. These members form general and \\nspecific boundary sets that delimit the version space within the partially ordered \\nhypothesis space. \\nThe LIST-THEN-ELIMINATE Algorithm \\n1. VersionSpace c a list containing every hypothesis in H \\n2. For each training example, (x, c(x)) \\nremove from VersionSpace any hypothesis h for which h(x) # c(x) \\n3. Output the list of hypotheses in VersionSpace \\nTABLE 2.4 \\nThe LIST-THEN-ELIMINATE algorithm. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 42}, page_content=\"{<Sunny, Warm, ?, Strong, 7, ?> 1 \\n<Sunny, ?, 7, Strong, 7, ?> <Sunny, Warm, ?. ?, ?, ?> <?, Warm, ?, strbng, ?, ?> \\nFIGURE 2.3 \\nA version space with its general and specific boundary sets. The version space includes all six \\nhypotheses shown here, but can be represented more simply by S and G. Arrows indicate instances \\nof the more-general-than relation. This is the version space for the Enjoysport concept learning \\nproblem and training examples described in Table 2.1. \\nTo illustrate this representation for version spaces, consider again the En- \\njoysport concept learning problem described in Table 2.2. Recall that given the \\nfour training examples from Table 2.1, \\nFIND-S \\noutputs \\nthe hypothesis \\nh = (Sunny, Warm, ?, Strong, ?, ?) \\nIn fact, this is just one of six different hypotheses from H that are consistent \\nwith these training examples. All six hypotheses are shown in Figure 2.3. They \\nconstitute the version space relative to this set of data and this hypothesis repre- \\nsentation. The arrows among these six hypotheses in Figure 2.3 indicate instances \\nof the more-general~han relation. The CANDIDATE-ELIMINATION algorithm rep- \\nresents the version space by storing only its most general members (labeled G \\nin Figure 2.3) and its most specific (labeled S in the figure). Given only these \\ntwo sets S and G, it is possible to enumerate all members of the version space \\nas needed by generating the hypotheses that lie between these two sets in the \\ngeneral-to-specific partial ordering over hypotheses. \\nIt is intuitively plausible that we can represent the version space in terms of \\nits most specific and most general members. Below we define the boundary sets \\nG and S precisely and prove that these sets do in fact represent the version space. \\nDefinition: The general boundary G, with respect to hypothesis space H and training \\ndata D, is the set of maximally general members of H consistent with D. \\nG = {g E HIConsistent(g, D) A (-3gf E H)[(gf >, g) A Consistent(gt, D)]] \\nDefinition: The specific boundary S, with respect to hypothesis space H and training \\ndata D, is the set of minimally general (i.e., maximally specific) members of H \\nconsistent with D. \\nS rn {s E H(Consistent(s, D) A (-3s' E H)[(s >, sf) A Consistent(st, D)]) \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 43}, page_content='As long as the sets G and S are well defined (see Exercise 2.7), they com- \\npletely specify the version space. In particular, we can show that the version space \\nis precisely the set of hypotheses contained in G, plus those contained in S, plus \\nthose that lie between G and S in the partially ordered hypothesis space. This is \\nstated precisely in Theorem 2.1. \\nTheorem 2.1. Version space representation theorem. Let X be an arbitrary set \\nof instances and let H be a set of boolean-valued hypotheses defined over X. Let \\nc : X + {O, 1) be an arbitrary target concept defined over X, and let D be an \\narbitrary set of training examples {(x, \\nc(x))). \\nFor all X, H, c, and D such that S and \\nG are well defined, \\nProof. To prove the theorem it suffices to show that (1) every h satisfying the right- \\nhand side of the above expression is in VSH,~ and (2) every member of VSH,~ \\nsatisfies the right-hand side of the expression. To show (1) let g be an arbitrary \\nmember of G, s be an arbitrary member of S, and h be an arbitrary member of H, \\nsuch that g 2, h 2, s. Then by the definition of S, s must be satisfied by all positive \\nexamples in D. Because h 2, s, h must also be satisfied by all positive examples in \\nD. Similarly, by the definition of G, g cannot be satisfied by any negative example \\nin D, and because g 2, h, h cannot be satisfied by any negative example in D. \\nBecause h is satisfied by all positive examples in D and by no negative examples \\nin D, h is consistent with D, and therefore h is a member of VSH,~. This proves \\nstep (1). The argument for (2) is a bit more complex. It can be proven by assuming \\nsome h in VSH,~ that does not satisfy the right-hand side of the expression, then \\nshowing that this leads to an inconsistency. (See Exercise 2.6.) 0 \\n2.5.4 CANDIDATE-ELIMINATION Learning Algorithm \\nThe CANDIDATE-ELIMINATION algorithm computes the version space containing \\nall hypotheses from H that are consistent with an observed sequence of training \\nexamples. It begins by initializing the version space to the set of all hypotheses \\nin H; that is, by initializing the G boundary set to contain the most general \\nhypothesis in H \\nGo + {(?, ?, ?, ?, ?, ?)} \\nand initializing the S boundary set to contain the most specific (least general) \\nhypothesis \\nso c- ((@,PI, @,PI, 0,0)1 \\nThese two boundary sets delimit the entire hypothesis space, because every other \\nhypothesis in H is both more general than So and more specific than Go. As \\neach training example is considered, the S and G boundary sets are generalized \\nand specialized, respectively, to eliminate from the version space any hypothe- \\nses found inconsistent with the new training example. After all examples have \\nbeen processed, the computed version space contains all the hypotheses consis- \\ntent with these examples and only these hypotheses. This algorithm is summarized \\nin Table 2.5. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 44}, page_content='CHAPTER 2 CONCEET LEARNJNG AND THE GENERAL-TO-SPECIFIC ORDERING 33 \\nInitialize G to the set of maximally general hypotheses in H \\nInitialize S to the set of maximally specific hypotheses in H \\nFor each training example d, do \\n0 If d is a positive example \\nRemove from G any hypothesis inconsistent with d , \\n0 For each hypothesis s in S that is not consistent with d ,- \\n0 Remove s from S \\n0 Add to S all minimal generalizations h of s such that \\n0 h is consistent with d, and some member of G is more general than h \\n0 Remove from S any hypothesis that is more general than another hypothesis in S \\n0 If d is a negative example \\n0 Remove from S any hypothesis inconsistent with d \\nFor each hypothesis g in G that is not consistent with d \\nRemove g from G \\n0 Add to G all minimal specializations h of g such that \\n0 h is consistent with d, and some member of S is more specific than h \\n0 Remove from G any hypothesis that is less general than another hypothesis in G \\nTABLE 2.5 \\nCANDIDATE-ELIMINATION algorithm using version spaces. Notice the duality in how positive and \\nnegative examples influence S and G. \\nNotice that the algorithm is specified in terms of operations such as comput- \\ning minimal generalizations and specializations of given hypotheses, and identify- \\ning \\nnonrninimal \\nand nonmaximal hypotheses. The detailed implementation of these \\noperations will depend, of course, on the specific representations for instances and \\nhypotheses. However, the algorithm itself can be applied to any concept learn- \\ning task and hypothesis space for which these operations are well-defined. In the \\nfollowing example trace of this algorithm, we see how such operations can be \\nimplemented for the representations used in the EnjoySport example problem. \\n2.5.5 An Illustrative Example \\nFigure 2.4 traces the CANDIDATE-ELIMINATION algorithm applied to the first two \\ntraining examples from Table 2.1. As described above, the boundary sets are first \\ninitialized to \\nGo \\nand \\nSo, \\nthe most general and most specific hypotheses in H, \\nrespectively. \\nWhen the first training example is presented (a positive example in this \\ncase), the CANDIDATE-ELIMINATION algorithm checks the S boundary and finds \\nthat it is overly specific-it fails to cover the positive example. The boundary is \\ntherefore revised by moving it to the least more general hypothesis that covers \\nthis new example. This revised boundary is shown as S1 in Figure 2.4. No up- \\ndate of the G boundary is needed in response to this training example because \\nGo correctly covers this example. When the second training example (also pos- \\nitive) is observed, it has a similar effect of generalizing S further to S2, leaving \\nG again unchanged (i.e., G2 = GI = GO). Notice the processing of these first '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 45}, page_content='34 MACHINE LEARNING \\nS 1 : {<Sunny, Warm, Normal, Strong, Warm, Same> } \\n1 \\nTraining examples: \\nt \\n1. <Sunny, Warm, Normal, Strong, Warm, Same>, Enjoy Sport = Yes \\n2. <Sunny, Warm, High, Strong, Warm, Same>, Enjoy Sport = Yes \\nS2 : \\nFIGURE 2.4 \\nCANDIDATE-ELIMINATION Trace 1. So and Go are the initial boundary sets corresponding to the most \\nspecific and most general hypotheses. Training examples 1 and 2 force the S boundary to become \\nmore general, as in the FIND-S algorithm. They have no effect on the G boundary. \\n{<Sunny, Warm, ?, Strong, Warm, Same>} \\ntwo positive examples is very similar to the processing performed by the FIND-S \\nalgorithm. \\nAs illustrated by these first two steps, positive training examples may force \\nthe S boundary of the version space to become increasingly general. Negative \\ntraining examples play the complimentary role of forcing the G boundary to \\nbecome increasingly specific. Consider the third training example, shown in Fig- \\nure 2.5. This negative example reveals that the G boundary of the version space \\nis overly general; that is, the hypothesis in G incorrectly predicts that this new \\nexample is a positive example. The hypothesis in the G boundary must therefore \\nbe specialized until it correctly classifies this new negative example. As shown in \\nFigure 2.5, there are several alternative minimally more specific hypotheses. All \\nof these become members of the new G3 boundary set. - \\nGiven that there are six attributes that could be specified to specialize G2, \\nwhy are there only three new hypotheses in G3? For example, the hypothesis \\nh = (?, ?, Normal, ?, ?, ?) is a minimal specialization of G2 that correctly la- \\nbels the new example as a negative example, but it is not included in Gg. The \\nreason this hypothesis is excluded is that it is inconsistent with the previously \\nencountered positive examples. The algorithm determines this simply by noting \\nthat h is not more general than the current specific boundary, Sz. In fact, the S \\nboundary of the version space forms a summary of the previously encountered \\npositive examples that can be used to determine whether any given hypothesis '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 46}, page_content='CHmR 2 CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 35 \\ns2 9 s 3 : \\nTraining Example: \\n( <Sunny, Wann, ?. Strong, Warn Same> )] \\nG 3: \\n3. <Rainy, Cold, High, Strong, Warm, Change>, EnjoySporkNo \\n(<Sunny, ?, ?, ?, ?, ?> <?, Wann, ?, ?, ?, ?> <?, ?, ?, ?, ?, Same>} \\nFIGURE 2.5 \\nCANDIDATE-ELMNATION Trace 2. Training example 3 is a negative example that forces the G2 \\nboundary to be specialized to G3. Note several alternative maximally general hypotheses are included \\nin Gj. \\nis consistent with these examples. Any hypothesis more general than S will, by \\ndefinition, cover any example that S covers and thus will cover any past positive \\nexample. In a dual fashion, the G boundary summarizes the information from \\npreviously encountered negative examples. Any hypothesis more specific than G \\nis assured to be consistent with past negative examples. This is true because any \\nsuch hypothesis, by definition, cannot cover examples that G does not cover. \\nThe fourth training example, as shown in Figure 2.6, further generalizes the \\nS boundary of the version space. It also results in removing one member of the G \\nboundary, because this member fails to cover the new positive example. This last \\naction results from the first step under the condition \"If d is a positive example\" \\nin the algorithm shown in Table 2.5. To understand the rationale for this step, it is \\nuseful to consider why the offending hypothesis must be removed from G. Notice \\nit cannot be specialized, because specializing it would not make it cover the new \\nexample. It also cannot be generalized, because by the definition of G, any more \\ngeneral hypothesis will cover at least one negative training example. Therefore, \\nthe hypothesis must be dropped from the G boundary, thereby removing an entire \\nbranch of the partial ordering from the version space of hypotheses remaining \\nunder consideration. \\nAfter processing these four examples, the boundary sets S4 and G4 delimit \\nthe version space of all hypotheses consistent with the set of incrementally ob- \\nserved training examples. \\nThe \\nentire version space, including those hypotheses \\nA \\n\\'32: I<?, ?, ?, ?, ?,?>I '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 47}, page_content='S 3: {<Sunny, Warm, ?, Strong, Warm, Same>) \\nI \\nS 4: I ( <Sunny, Warm ?, Strong, ?, ?> ) I \\nTraining Example: \\n4. <Sunny, Warm, High, Strong, Cool, Change>, EnjoySport = Yes \\nFIGURE 2.6 \\nCANDIDATE-ELIMINATION Trace 3. The positive training example generalizes the S boundary, from \\nS3 to S4. One member of Gg must also be deleted, because it is no longer more general than the S4 \\nboundary. \\nbounded by S4 and G4, is shown in Figure 2.7. This learned version space is \\nindependent of the sequence in which the training examples are presented (be- \\ncause in the end it contains all hypotheses consistent with the set of examples). \\nAs further training data is encountered, the S and G boundaries will move mono- \\ntonically closer to each other, delimiting a smaller and smaller version space of \\ncandidate hypotheses. \\n<Sunny, ?, ?, Strong, ?, ?> <Sunny, Warm, ?, ?, ?, ?> <?, Warm, ?, Strong, ?, ?> \\ns4: \\n{<Sunny, ?, ?, ?, ?, ?>, <?, Warm, ?, ?, ?, ?>) \\n{<Sunny, Warm, ?, Strong, ?, ?>) \\nFIGURE 2.7 \\nThe final version space for the EnjoySport concept learning problem and training examples described \\nearlier. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 48}, page_content='CH.4PTF.R 2 CONCEFT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 37 \\n2.6 REMARKS ON VERSION SPACES AND CANDIDATE-ELIMINATION \\n2.6.1 Will the CANDIDATE-ELIMINATION Algorithm Converge to the \\nCorrect Hypothesis? \\nThe version space learned by the CANDIDATE-ELIMINATION algorithm will con- \\nverge toward the hypothesis that correctly describes the target concept, provided \\n(1) there are no errors in the training examples, and (2) there is some hypothesis \\nin H that correctly describes the target concept. In fact, as new training examples \\nare observed, the version space can be monitored to determine the remaining am- \\nbiguity regarding the true target concept and to determine when sufficient training \\nexamples have been observed to unambiguously identify the target concept. The \\ntarget concept is exactly learned when the S and G boundary sets converge to a \\nsingle, identical, hypothesis. \\nWhat will happen if the training data contains errors? Suppose, for example, \\nthat the second training example above is incorrectly presented as a negative \\nexample instead of a positive example. Unfortunately, in this case the algorithm \\nis certain to remove the correct target concept from the version space! Because, \\nit will remove every hypothesis that is inconsistent with each training example, it \\nwill eliminate the true target concept from the version space as soon as this false \\nnegative example is encountered. Of course, given sufficient additional training \\ndata the learner will eventually detect an inconsistency by noticing that the S and \\nG boundary sets eventually converge to an empty version space. Such an empty \\nversion space indicates that there is no hypothesis in H consistent with all observed \\ntraining examples. A similar symptom will appear when the training examples are \\ncorrect, but the target concept cannot be described in the hypothesis representation \\n(e.g., \\nif the target concept is a disjunction of feature attributes and the hypothesis \\nspace supports only conjunctive descriptions). We will consider such eventualities \\nin greater detail later. For now, we consider only the case in which the training \\nexamples are correct and the true target concept is present in the hypothesis space. \\n2.6.2 What Training Example Should the Learner Request Next? \\nUp to this point we have assumed that training examples are provided to the \\nlearner by some external teacher. Suppose instead that the learner is allowed to \\nconduct experiments in which it chooses the next instance, then obtains the correct \\nclassification for this instance from an external oracle \\n(e.g., \\nnature or a teacher). \\nThis scenario covers situations in which the learner may conduct experiments in \\nnature \\n(e.g., \\nbuild new bridges and allow nature to classify them as stable or \\nunstable), or in which a teacher is available to provide the correct classification \\n(e.g., propose a new bridge and allow the teacher to suggest whether or not it will \\nbe stable). We use the term query to refer to such instances constructed by the \\nlearner, which are then classified by an external oracle. \\nConsider again the version space learned from the four training examples \\nof the Enjoysport concept and illustrated in Figure 2.3. What would be a good \\nquery for the learner to pose at this point? What is a good query strategy in '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 49}, page_content='general? Clearly, the learner should attempt to discriminate among the alternative \\ncompeting hypotheses in its current version space. Therefore, it should choose \\nan instance that would be classified positive by some of these hypotheses, but \\nnegative by others. One such instance is \\n(Sunny, Warm, Normal, Light, Warm, Same) \\nNote that this instance satisfies three of the six hypotheses in the current \\nversion space (Figure 2.3). If the trainer classifies this instance as a positive ex- \\nample, the S boundary of the version space can then be generalized. Alternatively, \\nif the trainer indicates that this is a negative example, the G boundary can then be \\nspecialized. Either way, the learner will succeed in learning more about the true \\nidentity of the target concept, shrinking the version space from six hypotheses to \\nhalf this number. \\nIn general, the optimal query strategy for a concept learner is to generate \\ninstances that satisfy exactly half the hypotheses in the current version space. \\nWhen this is possible, the size of the version space is reduced by half with each \\nnew example, and the correct target concept can therefore be found with only \\nrlog2JVS11 experiments. The situation is analogous to playing the game twenty \\nquestions, in which the goal is to ask yes-no questions to determine the correct \\nhypothesis. The optimal strategy for playing twenty questions is to ask questions \\nthat evenly split the candidate hypotheses into sets that predict yes and no. While \\nwe have seen that it is possible to generate an instance that satisfies precisely \\nhalf the hypotheses in the version space of Figure 2.3, in general it may not be \\npossible to construct an instance that matches precisely half the hypotheses. In \\nsuch cases, a larger number of queries may be required than rlog21VS(1. \\n2.6.3 How Can Partially Learned Concepts Be Used? \\nSuppose that no additional training examples are available beyond the four in \\nour example above, but that the learner is now required to classify new instances \\nthat it has not yet observed. Even though the version space of Figure 2.3 still \\ncontains multiple hypotheses, indicating that the target concept has not yet been \\nfully learned, it is possible to classify certain examples with the same degree of \\nconfidence as if the target concept had been uniquely identified. To illustrate, \\nsuppose the learner is asked to classify the four new instances shown in Ta- \\nble 2.6. \\n9 \\nNote that although instance A was not among the training examples, it is \\nclassified as a positive instance by every hypothesis in the current version space \\n(shown in Figure 2.3). Because the hypotheses in the version space unanimously \\nagree that this is a positive instance, the learner can classify instance A as positive \\nwith the same confidence it would have if it had already converged to the single, \\ncorrect target concept. Regardless of which hypothesis in the version space is \\neventually found to be the correct target concept, it is already clear that it will \\nclassify instance A as a positive example. Notice furthermore that we need not \\nenumerate every hypothesis in the version space in order to test whether each '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 50}, page_content='CHAPTER 2 CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 39 \\nInstance Sky AirTemp Humidity Wind Water Forecast EnjoySport \\n- \\nA Sunny Warm Normal Strong Cool Change ? \\nB Rainy Cold Normal Light Warm Same ? \\nC Sunny Warm Normal Light Warm Same ? \\nD Sunny Cold Normal Strong Warm Same ? \\nTABLE 2.6 \\nNew instances to be classified. \\nclassifies the instance as positive. This condition will be met if and only if the \\ninstance satisfies every member of S (why?). The reason is that every other hy- \\npothesis in the version space is at least as general as some member of S. By our \\ndefinition of more-general~han, if the new instance satisfies all members of S it \\nmust also satisfy each of these more general hypotheses. \\nSimilarly, instance B is classified as a negative instance by every hypothesis \\nin the version space. This instance can therefore be safely classified as negative, \\ngiven the partially learned concept. An efficient test for this condition is that the \\ninstance satisfies none of the members of G (why?). \\nInstance C presents a different situation. Half of the version space hypotheses \\nclassify it as positive and half classify it as negative. Thus, the learner cannot \\nclassify this example with confidence until further training examples are available. \\nNotice that instance C is the same instance presented in the previous section as \\nan optimal experimental query for the learner. This is to be expected, because \\nthose instances whose classification is most ambiguous are precisely the instances \\nwhose true classification would provide the most new information for refining the \\nversion space. \\nFinally, instance D is classified as positive by two of the version space \\nhypotheses and negative by the other four hypotheses. In this case we have less \\nconfidence in the classification than in the unambiguous cases of instances A \\nand B. Still, the vote is in favor of a negative classification, and one approach \\nwe could take would be to output the majority vote, perhaps with a confidence \\nrating indicating how close the vote was. As we will discuss in Chapter 6, if we \\nassume that all hypotheses in H are equally probable a priori, then such a vote \\nprovides the most probable classification of this new instance. Furthermore, the \\nproportion of hypotheses voting positive can be interpreted as the probability that \\nthis instance is positive given the training data. \\n2.7 INDUCTIVE BIAS \\nAs discussed above, the CANDIDATE-ELIMINATION algorithm will converge toward \\nthe true target concept provided it is given accurate training examples and pro- \\nvided its initial hypothesis space contains the target concept. What if the target \\nconcept is not contained in the hypothesis space? Can we avoid this difficulty by \\nusing a hypothesis space that includes every possible hypothesis? How does the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 51}, page_content='size of this hypothesis space influence the ability of the algorithm to generalize \\nto unobserved instances? How does the size of the hypothesis space influence the \\nnumber of training examples that must be observed? These are fundamental ques- \\ntions for inductive inference in general. Here we examine them in the context of \\nthe CANDIDATE-ELIMINATION algorithm. As we shall see, though, the conclusions \\nwe draw from this analysis will apply to any concept learning system that outputs \\nany hypothesis consistent with the training data. \\n2.7.1 A Biased Hypothesis Space \\nSuppose we wish to assure that the hypothesis space contains the unknown tar- \\nget concept. The obvious solution is to enrich the hypothesis space to include \\nevery possible hypothesis. To illustrate, consider again the EnjoySpor t example in \\nwhich we restricted the hypothesis space to include only conjunctions of attribute \\nvalues. Because of this restriction, the hypothesis space is unable to represent \\neven simple disjunctive target concepts such as \"Sky = Sunny or Sky = Cloudy.\" \\nIn fact, given the following three training examples of this disjunctive hypothesis, \\nour algorithm would find that there are zero hypotheses in the version space. \\nExample Sky AirTemp Humidity Wind Water Forecast EnjoySport \\n1 Sunny Warm Normal Strong Cool Change Yes \\n2 Cloudy Warm Normal Strong Cool Change Yes \\n3 Rainy Warm Normal Strong Cool Change No \\nTo see why there are no hypotheses consistent with these three examples, \\nnote that the most specific hypothesis consistent with the first two examples and \\nrepresentable in the given hypothesis space H is \\nS2 : (?, Warm, Normal, Strong, Cool, Change) \\nThis hypothesis, although it is the maximally specific hypothesis from H that is \\nconsistent with the first two examples, is already overly general: it incorrectly \\ncovers the third (negative) training example. The problem is that we have biased \\nthe learner to consider only conjunctive hypotheses. In this case we require a more \\nexpressive hypothesis space. \\n2.7.2 An Unbiased Learner \\nThe obvious solution to the problem of assuring that the target concept is in the \\nhypothesis space H is to provide a hypothesis space capable of representing every \\nteachable concept; that is, it is capable of representing every possible subset of the \\ninstances X. In general, the set of all subsets of a set X is called thepowerset of X. \\nIn the EnjoySport learning task, for example, the size of the instance space \\nX of days described by the six available attributes is 96. How many possible \\nconcepts can be defined over this set of instances? In other words, how large is '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 52}, page_content='the power set of X? In general, the number of distinct subsets that can be defined \\nover a set X containing 1x1 elements (i.e., the size of the power set of X) is 21\\'1. \\nThus, there are 296, or approximately distinct target concepts that could be \\ndefined over this instance space and that our learner might be called upon to learn. \\nRecall from Section 2.3 that our conjunctive hypothesis space is able to represent \\nonly 973 of these-a very biased hypothesis space indeed! \\nLet us \\nreformulate \\nthe Enjoysport learning task in an unbiased way by \\ndefining a new hypothesis space H\\' that can represent every subset of instances; \\nthat is, let H\\' correspond to the power set of X. One way to define such an H\\' is to \\nallow arbitrary disjunctions, conjunctions, and negations of our earlier hypotheses. \\nFor instance, the target concept \"Sky = Sunny or Sky = Cloudy\" could then be \\ndescribed as \\n(Sunny, ?, ?, ?, ?, ?) v (Cloudy, ?, ?, ?, ?, ?) \\nGiven this hypothesis space, we can safely use the CANDIDATE-ELIMINATION \\nalgorithm without worrying that the target concept might not be expressible. How- \\never, while this hypothesis space eliminates any problems of expressibility, it un- \\nfortunately raises a new, equally difficult problem: our concept learning algorithm \\nis now completely unable to generalize beyond the observed examples! To see \\nwhy, suppose we present three positive examples (xl, x2, x3) and two negative ex- \\namples (x4, x5) to the learner. At this point, the S boundary of the version space \\nwill contain the hypothesis which is just the disjunction of the positive examples \\nbecause this is the most specific possible hypothesis that covers these three exam- \\nples. Similarly, the G boundary will consist of the hypothesis that rules out only \\nthe observed negative examples \\nThe problem here is that with this very expressive hypothesis representation, \\nthe S boundary will always be simply the disjunction of the observed positive \\nexamples, while the G boundary will always be the negated disjunction of the \\nobserved negative examples. Therefore, the only examples that will be unambigu- \\nously classified by S and G are the observed training examples themselves. In \\norder to converge to a single, final target concept, we will have to present every \\nsingle instance in X as a training example! \\nIt might at first seem that we could avoid this difficulty by simply using the \\npartially learned version space and by taking a vote among the members of the \\nversion space as discussed in Section 2.6.3. Unfortunately, the only instances that \\nwill produce a unanimous vote are the previously observed training examples. For, \\nall the other instances, taking a vote will be futile: each unobserved instance will \\nbe classified positive by precisely half the hypotheses in the version space and \\nwill be classified negative by the other half (why?). To see the reason, note that \\nwhen H is the power set of X and x is some previously unobserved instance, \\nthen for any hypothesis h in the version space that covers x, there will be anoQer '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 53}, page_content=\"hypothesis h' in the power set that is identical to h except for its classification of \\nx. And of course if h is in the version space, then h' will be as well, because it \\nagrees with h on all the observed training examples. \\n2.7.3 The Futility of Bias-Free Learning \\nThe above discussion illustrates a fundamental property of inductive inference: \\na learner that makes no a priori assumptions regarding the identity of the tar- \\nget concept has no rational basis for classifying any unseen instances. In fact, \\nthe only reason that the CANDIDATE-ELIMINATION algorithm was able to gener- \\nalize beyond the observed training examples in our original formulation of the \\nEnjoySport task is that it was biased by the implicit assumption that the target \\nconcept could be represented by a conjunction of attribute values. In cases where \\nthis assumption is correct (and the training examples are error-free), its classifica- \\ntion of new instances will also be correct. If this assumption is incorrect, however, \\nit is certain that the CANDIDATE-ELIMINATION algorithm will \\nrnisclassify \\nat least \\nsome instances from X. \\nBecause inductive learning requires some form of prior assumptions, or \\ninductive bias, we will find it useful to characterize different learning approaches \\nby the inductive \\nbiast \\nthey employ. Let us define this notion of inductive bias \\nmore precisely. The key idea we wish to capture here is the policy by which the \\nlearner generalizes beyond the observed training data, to infer the classification \\nof new instances. Therefore, consider the general setting in which an arbitrary \\nlearning algorithm L is provided an arbitrary set of training data \\nD, = {(x, c(x))} \\nof some arbitrary target concept c. After training, L is asked to classify a new \\ninstance \\nxi. \\nLet \\nL(xi, \\nD,) \\ndenote the classification \\n(e.g., \\npositive or negative) that \\nL assigns to \\nxi \\nafter learning from the training data \\nD,. \\nWe can describe this \\ninductive inference step performed by L as follows \\nwhere the notation y + z indicates that z is inductively inferred from y. For \\nexample, if we take L to be the CANDIDATE-ELIMINATION algorithm, \\nD, \\nto be \\nthe training data from Table 2.1, and \\nxi \\nto be the \\nfist \\ninstance from Table 2.6, \\nthen the inductive inference performed in this case concludes that \\nL(xi, \\nD,) = \\n(EnjoySport = yes). \\nBecause L is an inductive learning algorithm, the result L(xi, D,) that it in- \\nfers will not in general be provably correct; that is, the classification \\nL(xi, \\nD,) need \\nnot follow deductively from the training data \\nD, \\nand the description of the new \\ninstance xi. However, it is interesting to ask what additional assumptions could be \\nadded to \\nD, \\nr\\\\xi \\nso that \\nL(xi, \\nD,) \\nwould follow deductively. We define the induc- \\ntive bias of L as this set of additional assumptions. More precisely, we define the \\nt~he term inductive bias here is not to be confused with the term estimation bias commonly used in \\nstatistics. Estimation bias will be discussed in Chapter 5. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 54}, page_content='CHAFI%R 2 CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 43 \\ninductive bias of L to be the set of assumptions B such that for all new instances xi \\n(B A D, A xi) F L(xi, D,) \\nwhere the notation y t z indicates that z follows deductively from y (i.e., that z \\nis provable from y). Thus, we define the inductive bias of a learner as the set of \\nadditional assumptions B sufficient to justify its inductive inferences as deductive \\ninferences. To summarize, \\nDefinition: Consider a concept learning algorithm L for the set of instances X. Let \\nc be an arbitrary concept defined over X, and let D, = ((x, c(x))} be an arbitrary \\nset of training examples of c. Let L(xi, D,) denote the classification assigned to \\nthe instance xi by L after training on the data D,. The inductive bias of L is any \\nminimal set of assertions B such that for any target concept c and corresponding \\ntraining examples Dc \\n(Vxi E X)[(B A Dc A xi) k L(xi, D,)] (2.1) \\nWhat, then, is the inductive bias of the CANDIDATE-ELIMINATION algorithm? \\nTo answer this, let us specify L(xi, D,) exactly for this algorithm: given a set \\nof data D,, the CANDIDATE-ELIMINATION algorithm will first compute the version \\nspace VSH,D,, then classify the new instance xi by a vote among hypotheses in this \\nversion space. Here let us assume that it will output a classification for xi only if \\nthis vote among version space hypotheses is unanimously positive or negative and \\nthat it will not output a classification otherwise. Given this definition of L(xi, D,) \\nfor the CANDIDATE-ELIMINATION algorithm, what is its inductive bias? It is simply \\nthe assumption c E H. Given this assumption, each inductive inference performed \\nby the CANDIDATE-ELIMINATION algorithm can be justified deductively. \\nTo see why the classification L(xi, D,) follows deductively from B = {c E \\nH), together with the data D, and description of the instance xi, consider the fol- \\nlowing argument. First, notice that if we assume c E H then it follows deductively \\nthat c E VSH,Dc. This follows from c E H, from the definition of the version space \\nVSH,D, as the set of all hypotheses in H that are consistent with D,, and from our \\ndefinition of D, = {(x, c(x))} as training data consistent with the target concept \\nc. Second, recall that we defined the classification L(xi, D,) to be the unanimous \\nvote of all hypotheses in the version space. Thus, if L outputs the classification \\nL(x,, D,), it must be the case the every hypothesis in VSH,~, also produces this \\nclassification, including the hypothesis c E VSHYDc. Therefore c(xi) = L(xi, D,). \\nTo summarize, the CANDIDATE-ELIMINATION algorithm defined in this fashion can \\nbe characterized by the following bias \\nInductive bias of CANDIDATE-ELIMINATION algorithm. The target concept c is \\ncontained in the given hypothesis space H. \\nFigure 2.8 summarizes the situation schematically. The inductive CANDIDATE- \\nELIMINATION algorithm at the top of the figure takes two inputs: the training exam- \\nples and a new instance to be classified. At the bottom of the figure, a deductive '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 55}, page_content='44 MACHINE LEARNING \\nInductive system \\nClassification of \\nCandidate new instance, or Training examples Elimination \"don\\'t know\" \\nNew instance Using Hypothesis \\nSpace H \\nEquivalent deductive system \\nI I Classification of \\nTraining examples I \\nnew instance, or \\n\"don\\'t know\" \\nTheorem Prover \\nAssertion \" Hcontains \\nthe target concept\" \\n-D \\nP \\nInductive bias \\nmade explicit \\nFIGURE 2.8 \\nModeling inductive systems by equivalent deductive systems. The input-output behavior of the \\nCANDIDATE-ELIMINATION algorithm using a hypothesis space H is identical to that of a deduc- \\ntive theorem prover utilizing the assertion \"H contains the target concept.\" This assertion is therefore \\ncalled the inductive bias of the CANDIDATE-ELIMINATION algorithm. Characterizing inductive systems \\nby their inductive bias allows modeling them by their equivalent deductive systems. This provides a \\nway to compare inductive systems according to their policies for generalizing beyond the observed \\ntraining data. \\ntheorem prover is given these same two inputs plus the assertion \"H contains the \\ntarget concept.\" These two systems will in principle produce identical outputs for \\nevery possible input set of training examples and every possible new instance in \\nX. Of course the inductive bias that is explicitly input to the theorem prover is \\nonly implicit in the code of the CANDIDATE-ELIMINATION algorithm. In a sense, it \\nexists only in the eye of us beholders. Nevertheless, it is a perfectly well-defined \\nset of assertions. \\nOne advantage of viewing inductive inference systems in terms of their \\ninductive bias is that it provides a nonprocedural means of characterizing their \\npolicy for generalizing beyond the observed data. A second advantage is that it \\nallows comparison of different learners according to the strength of the inductive \\nbias they employ. Consider, for example, the following three learning algorithms, \\nwhich are listed from weakest to strongest bias. \\n1. ROTE-LEARNER: Learning corresponds simply to storing each observed train- \\ning example in memory. Subsequent instances are classified by looking them '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 56}, page_content='CHAPTER 2 CONCEPT. LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 45 \\nup in memory. If the instance is found in memory, the stored classification \\nis returned. Otherwise, the system refuses to classify the new instance. \\n2. CANDIDATE-ELIMINATION algorithm: New instances are classified only in the \\ncase where all members of the current version space agree on the classifi- \\ncation. Otherwise, the system refuses to classify the new instance. \\n3. FIND-S: This algorithm, described earlier, finds the most specific hypothesis \\nconsistent with the training examples. It then uses this hypothesis to classify \\nall subsequent instances. \\nThe ROTE-LEARNER has no inductive bias. The classifications it provides \\nfor new instances follow deductively from the observed training examples, with \\nno additional assumptions required. The CANDIDATE-ELIMINATION algorithm has a \\nstronger inductive bias: that the target concept can be represented in its hypothesis \\nspace. Because it has a stronger bias, it will classify some instances that the \\nROTE- \\nLEARNER will not. Of course the correctness of such classifications will depend \\ncompletely on the correctness of this inductive bias. The FIND-S algorithm has \\nan even stronger inductive bias. In addition to the assumption that the target \\nconcept can be described in its hypothesis space, it has an additional inductive \\nbias assumption: that all instances are negative instances unless the opposite is \\nentailed by its other \\nknow1edge.t \\nAs we examine other inductive inference methods, it is useful to keep in \\nmind this means of characterizing them and the strength of their inductive bias. \\nMore strongly biased methods make more inductive leaps, classifying a greater \\nproportion of unseen instances. Some inductive biases correspond to categorical \\nassumptions that completely rule out certain concepts, such as the bias \"the hy- \\npothesis space H includes the target concept.\" Other inductive biases merely rank \\norder the hypotheses by stating preferences such as \"more specific hypotheses are \\npreferred over more general hypotheses.\" Some biases are implicit in the learner \\nand are unchangeable by the learner, such as the ones we have considered here. \\nIn Chapters 11 and 12 we will see other systems whose bias is made explicit as \\na set of assertions represented and manipulated by the learner. \\n2.8 SUMMARY AND FURTHER READING \\nThe main points of this chapter include: \\nConcept learning can be cast as a problem of searching through a large \\npredefined space of potential hypotheses. \\nThe general-to-specific partial ordering of hypotheses, which can be defined \\nfor any concept learning problem, provides a useful structure for organizing \\nthe search through the hypothesis space. \\n+Notice this last inductive bias assumption involves a kind of default, or nonmonotonic reasoning. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 57}, page_content=\"The FINDS algorithm utilizes this general-to-specific ordering, performing \\na specific-to-general search through the hypothesis space along one branch \\nof the partial ordering, to find the most specific hypothesis consistent with \\nthe training examples. \\nThe CANDIDATE-ELIMINATION algorithm utilizes this general-to-specific or- \\ndering to compute the version space (the set of all hypotheses consistent \\nwith the training data) by incrementally computing the sets of maximally \\nspecific (S) and maximally general (G) hypotheses. \\nBecause the S and G sets delimit the entire set of hypotheses consistent with \\nthe data, they provide the learner with a description of its uncertainty regard- \\ning the exact identity of the target concept. This version space of alternative \\nhypotheses can be examined to determine whether the learner has converged \\nto the target concept, to determine when the training data are inconsistent, \\nto generate informative queries to further refine the version space, and to \\ndetermine which unseen instances can be unambiguously classified based on \\nthe partially learned concept. \\nVersion spaces and the CANDIDATE-ELIMINATION algorithm provide a useful \\nconceptual framework for studying concept learning. However, this learning \\nalgorithm is not robust to noisy data or to situations in which the unknown \\ntarget concept is not expressible in the provided hypothesis space. Chap- \\nter 10 describes several concept learning algorithms based on the \\ngeneral- \\nto-specific ordering, which are robust to noisy data. \\n0 Inductive learning algorithms are able to classify unseen examples only be- \\ncause of their implicit inductive bias for selecting one consistent hypothesis \\nover another. The bias associated with the CANDIDATE-ELIMINATION algo- \\nrithm is that the target concept can be found in the provided hypothesis \\nspace (c E H). The output hypotheses and classifications of subsequent in- \\nstances follow deductively from this assumption together with the observed \\ntraining data. \\nIf the hypothesis space is enriched to the point where there is a hypoth- \\nesis corresponding to every possible subset of instances (the power set of \\nthe instances), this will remove any inductive bias from the \\nCANDIDATE- \\nELIMINATION algorithm. Unfortunately, this also removes the ability to clas- \\nsify any instance beyond the observed training examples. An unbiased learner \\ncannot make inductive leaps to classify unseen examples. \\nThe idea of concept learning and using the general-to-specific ordering have \\nbeen studied for quite some time. Bruner et al. (1957) provided an early study \\nof concept learning in humans, and Hunt and Hovland (1963) an early effort \\nto automate it. Winston's (1970) widely known \\nPh.D. \\ndissertation cast concept \\nlearning as a search involving generalization and specialization operators. \\nPlotkin \\n(1970, 1971) provided an early formalization of the more-general-than relation, \\nas well as the related notion of 8-subsumption (discussed in Chapter 10). \\nSimon \\nand Lea (1973) give an early account of learning as search through a hypothesis \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 58}, page_content='CHAFTER 2 CONCEPT LEARNING AND THE GENERALTO-SPECIFIC ORDEIUNG 47 \\nspace. Other early concept learning systems include (Popplestone 1969; Michal- \\nski 1973; Buchanan 1974; Vere 1975; Hayes-Roth 1974). A very large number \\nof algorithms have since been developed for concept learning based on symbolic \\nrepresentations. Chapter 10 describes several more recent algorithms for con- \\ncept learning, including algorithms that learn concepts represented in first-order \\nlogic, algorithms that are robust to noisy training data, and algorithms whose \\nperformance degrades gracefully if the target concept is not representable in the \\nhypothesis space considered by the learner. \\nVersion spaces and the CANDIDATE-ELIMINATION algorithm were introduced \\nby Mitchell (1977, 1982). The application of this algorithm to inferring rules of \\nmass spectroscopy is described in (Mitchell \\n1979), \\nand its application to learning \\nsearch control rules is presented in (Mitchell et al. 1983). Haussler (1988) shows \\nthat the size of the general boundary can grow exponentially in the number of \\ntraining examples, even when the hypothesis space consists of simple conjunctions \\nof features. Smith and Rosenbloom (1990) show a simple change to the repre- \\nsentation of the G set that can improve complexity in certain cases, and Hirsh \\n(1992) shows that learning can be polynomial in the number of examples in some \\ncases when the G set is not stored at all. Subramanian and Feigenbaum (1986) \\ndiscuss a method that can generate efficient queries in certain cases by factoring \\nthe version space. One of the greatest practical limitations of the CANDIDATE- \\nELIMINATION algorithm is that it requires noise-free training data. Mitchell (1979) \\ndescribes an extension that can handle a bounded, predetermined number of mis- \\nclassified examples, and Hirsh (1990, 1994) describes an elegant extension for \\nhandling bounded noise in real-valued attributes that describe the training ex- \\namples. Hirsh (1990) describes an INCREMENTAL VERSION SPACE MERGING algo- \\nrithm that generalizes the CANDIDATE-ELIMINATION algorithm to handle situations \\nin which training information can be different types of constraints represented \\nusing version spaces. The information from each constraint is represented by a \\nversion space and the constraints are then combined by intersecting the version \\nspaces. Sebag (1994, 1996) presents what she calls a disjunctive version space ap- \\nproach to learning disjunctive concepts from noisy data. A separate version space \\nis learned for each positive training example, then new instances are classified \\nby combining the votes of these different version spaces. She reports experiments \\nin several problem domains demonstrating that her approach is competitive with \\nother widely used induction methods such as decision tree learning and k-NEAREST \\nNEIGHBOR. \\nEXERCISES \\n2.1. Explain why the size of the hypothesis space in the EnjoySport learning task is \\n973. How would the number of possible instances and possible hypotheses increase \\nwith the addition of the attribute Watercurrent, which can take on the values \\nLight, Moderate, or Strong? More generally, how does the number of possible \\ninstances and hypotheses grow with the addition of a new attribute A that takes on \\nk possible values? , \\nI '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 59}, page_content='2.2. Give the sequence of S and G boundary sets computed by the CANDIDATE-ELIMINA- \\nTION algorithm if it is given the sequence of training examples from Table 2.1 in \\nreverse order. Although the final version space will be the same regardless of the \\nsequence of examples (why?), the sets S and G computed at intermediate stages \\nwill, of course, depend on this sequence. Can you come up with ideas for ordering \\nthe training examples to minimize the sum of the sizes of these intermediate S and \\nG sets for the H used in the EnjoySport example? \\n2.3. Consider again the EnjoySport learning task and the hypothesis space H described \\nin Section 2.2. Let us define a new hypothesis space H\\' that consists of all painvise \\ndisjunctions of the hypotheses in H. For example, a typical hypothesis in H\\' is \\n(?, Cold, High, ?, ?, ?) v (Sunny, ?, High, ?, ?, Same) \\nTrace the CANDIDATE-ELIMINATION algorithm for the hypothesis space H\\' given the \\nsequence of training examples from Table 2.1 (i.e., show the sequence of S and G \\nboundary sets.) \\n2.4. Consider the instance space consisting of integer points in the x, y plane and the \\nset of hypotheses H consisting of rectangles. More precisely, hypotheses are of the \\nform a 5 x 5 b, c 5 y 5 d, where a, b, c, and d can be any integers. \\n(a) Consider the version space with respect to the set of positive (+) and negative \\n(-) training examples shown below. What is the S boundary of the version space \\nin this case? Write out the hypotheses and draw them in on the diagram. \\n(b) What is the G boundary of this version space? Write out the hypotheses and \\ndraw them in. \\n(c) Suppose the learner may now suggest a new x, y instance and ask the trainer for \\nits classification. Suggest a query guaranteed to reduce the size of the version \\nspace, regardless of how the trainer classifies it. Suggest one that will not. \\n(d) Now assume you are a teacher, attempting to teach a particular target concept \\n(e.g., 3 5 x 5 5,2 ( y 5 9). What is the smallest number of training examples \\nyou can provide so that the CANDIDATE-ELIMINATION algorithm will perfectly \\nlearn the target concept? \\n2.5. Consider the following sequence of positive and negative training examples describ- \\ning the concept \"pairs of people who live in the same house.\" Each training example \\ndescribes an ordered pair of people, with each person described by their sex, hair '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 60}, page_content='CHAPTER 2 CONCEPT LEARNING AND THE GENERAL-TO-SPECIFIC ORDERING 49 \\ncolor (black, brown, or blonde), height (tall, medium, or short), and nationality (US, \\nFrench, German, Irish, Indian, Japanese, or Portuguese). \\n+ ((male brown tall US) (f emale black short US)) \\n+ ((male brown short French)( female black short US)) \\n- ((female brown tall German)( f emale black short Indian)) \\n+ ((male brown tall Irish) (f emale brown short Irish)) \\nConsider a hypothesis space defined over these instances, in which each hy- \\npothesis is represented by a pair of Ctuples, and where each attribute constraint may \\nbe a specific value, \"?,\" or \"0,\" just as in the EnjoySport hypothesis representation. \\nFor example, the hypothesis \\n((male ? tall ?)(female ? ? Japanese)) \\nrepresents the set of all pairs of people where the first is a tall male (of any nationality \\nand hair color), and the second is a Japanese female (of any hair color and height). \\n(a) Provide a hand trace of the CANDIDATE-ELIMINATION algorithm learning from \\nthe above training examples and hypothesis language. In particular, show the \\nspecific and general boundaries of the version space after it has processed the \\nfirst training example, then the second training example, etc. \\n(b) How many distinct hypotheses from the given hypothesis space are consistent \\nwith the following single positive training example? \\n+ ((male black short Portuguese)(f emale blonde tall Indian)) \\n(c) Assume the learner has encountered only the positive example from part (b), \\nand that it is now allowed to query the trainer by generating any instance and \\nasking the trainer to classify it. Give a specific sequence of queries that assures \\nthe learner will converge to the single correct hypothesis, whatever it may be \\n(assuming that the target concept is describable within the given hypothesis \\nlanguage). Give the shortest sequence of queries you can find. How does the \\nlength of this sequence relate to your answer to question (b)? \\n(d) Note that this hypothesis language cannot express all concepts that can be defined \\nover the instances \\n(i.e., \\nwe can define sets of positive and negative examples for \\nwhich there is no corresponding describable hypothesis). If we were to enrich \\nthe language so that it could express all concepts that can be defined over the \\ninstance language, then how would your answer to (c) change? \\n2.6. Complete the proof of the version space representation theorem (Theorem 2.1). \\nConsider a concept learning problem in which each instance is a real number, and \\nin \\nwhich each hypothesis is an interval over the reals. More precisely, each hypothesis \\nin the hypothesis space H is of the form a < x < b, where a and b are any real \\nconstants, and x refers to the instance. For example, the hypothesis 4.5 < x < 6.1 \\nclassifies instances between 4.5 and 6.1 as positive, and others as negative. Explain \\ninformally why there cannot be a maximally specific consistent hypothesis for any \\nset of positive training examples. Suggest a slight modification to the hypothesis \\nrepresentation so that there will be. \\'C '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 61}, page_content='50 MACHINE LEARNING \\n2.8. In this chapter, we commented that given an unbiased hypothesis space (the power \\nset of the instances), the learner would find that each unobserved instance would \\nmatch exactly half the current members of the version space, regardless of which \\ntraining examples had been observed. Prove this. In particular, prove that for any \\ninstance space X, any set of training examples D, and any instance x E X not present \\nin D, that if H is the power set of X, then exactly half the hypotheses in VSH,D will \\nclassify x as positive and half will classify it as negative. \\n2.9. Consider a learning problem where each instance is described by a conjunction of \\nn boolean attributes a1 . . .a,. Thus, a typical instance would be \\n(al = T) A (az = F) A . . . A (a, = T) \\nNow consider a hypothesis space H in which each hypothesis is a disjunction of \\nconstraints over these attributes. For example, a typical hypothesis would be \\nPropose an algorithm that accepts a sequence of training examples and outputs \\na consistent hypothesis if one exists. Your algorithm should run in time that is \\npolynomial in n and in the number of training examples. \\n2.10. Implement the FIND-S algorithm. First verify that it successfully produces the trace in \\nSection 2.4 for the Enjoysport example. Now use this program to study the number \\nof random training examples required to exactly learn the target concept. Implement \\na training example generator that generates random instances, then classifies them \\naccording to the target concept: \\n(Sunny, Warm, ?, ?, ?, ?) \\nConsider training your FIND-S program on randomly generated examples and mea- \\nsuring the number of examples required before the program\\'s hypothesis is identical \\nto the target concept. Can you predict the average number of examples required? \\nRun the experiment at least 20 times and report the mean number of examples re- \\nquired. How do you expect this number to vary with the number of \"?\" in the \\ntarget concept? How would it vary with the number of attributes used to describe \\ninstances and hypotheses? \\nREFERENCES \\nBruner, J. S., Goodnow, J. J., & Austin, G. A. (1957). A study of thinking. New York: John Wiey \\n& Sons. \\nBuchanan, B. G. (1974). Scientific theory \\nformation \\nby computer. In J. C. Simon (Ed.), Computer \\nOriented Learning Processes. Leyden: Noordhoff. \\nGunter, C. A., Ngair, T., Panangaden, P., & Subramanian, D. (1991). The common order-theoretic \\nstructure of version spaces and \\nATMS\\'s. Proceedings of the National Conference on Artijicial \\nIntelligence (pp. 500-505). Anaheim. \\nHaussler, D. (1988). Quantifying inductive bias: A1 learning algorithms and Valiant\\'s learning frame- \\nwork. Artijicial Intelligence, 36, 177-221. \\nHayes-Roth, F. (1974). Schematic classification problems and their solution. Pattern Recognition, 6, \\n105-113. \\nHirsh, \\nH. (1990). Incremental version space merging: A general framework for concept learning. \\nBoston: \\nKluwer. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 62}, page_content=\"Hirsh, H. (1991). Theoretical underpinnings of version spaces. Proceedings of the 12th IJCAI \\n(pp. 665-670). Sydney. \\nHirsh, \\nH. (1994). Generalizing version spaces. Machine Learning, 17(1), 546. \\nHunt, E. G., & Hovland, D. I. (1963). Programming a model of human concept formation. In \\nE. Feigenbaum & J. Feldman (Eds.), Computers and thought (pp. 310-325). New York: Mc- \\nGraw Hill. \\nMichalski, R. S. (1973). AQVALI1: Computer implementation of a variable valued logic system VL1 \\nand examples of its application to pattern recognition. Proceedings of the 1st International Joint \\nConference on Pattern Recognition (pp. 3-17). \\nMitchell, T. M. \\n(1977). \\nVersion spaces: A candidate elimination approach to rule learning. Fijlh \\nInternational Joint Conference on AI @p. 305-310). Cambridge, MA: MIT Press. \\nMitchell, T. M. (1979). Version spaces: An approach to concept learning, (F'h.D. dissertation). Elec- \\ntrical Engineering Dept., Stanford University, Stanford, CA. \\nMitchell, T. M. (1982). Generalization as search. ArtQcial Intelligence, 18(2), 203-226. \\nMitchell, T. M., Utgoff, P. E., & Baneji, R. (1983). Learning by experimentation: Acquiring and \\nmodifying problem-solving heuristics. In \\nMichalski, \\nCarbonell, & Mitchell (Eds.), Machine \\nLearning (Vol. 1, pp. 163-190). Tioga Press. \\nPlotkin, G. D. (1970). A note on inductive generalization. In Meltzer & Michie (Eds.), Machine \\nIntelligence 5 (pp. 153-163). Edinburgh University Press. \\nPlotkin, \\nG. D. (1971). A further note on inductive generalization. In Meltzer & Michie (Eds.), Machine \\nIntelligence 6 (pp. 104-124). Edinburgh University Press. \\nPopplestone, R. J. (1969). An experiment in automatic induction. In Meltzer & Michie (Eds.), Machine \\nIntelligence 5 (pp. 204-215). Edinburgh University Press. \\nSebag, M. (1994). Using constraints to build version spaces. Proceedings of the 1994 European \\nConference on Machine Learning. Springer-Verlag. \\nSebag, M. (1996). Delaying the choice of bias: A disjunctive version space approach. Proceedings of \\nthe 13th International Conference on Machine Learning (pp. 444-452). San Francisco: Morgan \\nKaufmann. \\nSimon, H. A,, & Lea, G. (1973). Problem solving and rule induction: A unified view. In Gregg (Ed.), \\nKnowledge and Cognition (pp. 105-127). New Jersey: Lawrence Erlbaum Associates. \\nSmith, B. D., & Rosenbloom, P. (1990). Incremental non-backtracking focusing: A polynomially \\nbounded generalization algorithm for version spaces. Proceedings of the 1990 National Con- \\nference on ArtQcial Intelligence (pp. 848-853). Boston. \\nSubramanian, D., & Feigenbaum, J. (1986). Factorization in experiment generation. Proceedings of \\nthe \\nI986 \\nNational Conference on ArtQcial Intelligence (pp. 518-522). Morgan Kaufmann. \\nVere, S. A. (1975). Induction of concepts in the predicate calculus. Fourth International Joint Con- \\nference on AI (pp. 281-287). Tbilisi, USSR. \\nWinston, P. H. (1970). Learning structural descriptions from examples, (Ph.D. dissertation). [MIT \\nTechnical Report \\nAI-TR-2311. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 63}, page_content='CHAPTER \\nDECISION TREE \\nLEARNING \\nDecision tree learning is one of the most widely used and practical methods for \\ninductive inference. It is a method for approximating discrete-valued functions that \\nis robust to noisy data and capable of learning disjunctive expressions. This chapter \\ndescribes a family of decision tree learning algorithms that includes widely used \\nalgorithms such as ID3, ASSISTANT, and C4.5. These decision tree learning meth- \\nods search a completely expressive hypothesis space and thus avoid the difficulties \\nof restricted hypothesis spaces. Their inductive bias is a preference for small trees \\nover large trees. \\n3.1 INTRODUCTION \\nDecision tree learning is a method for approximating discrete-valued target func- \\ntions, in which the learned function is represented by a decision tree. Learned trees \\ncan also be re-represented as sets of if-then rules to improve human readability. \\nThese learning methods are among the most popular of inductive inference algo- \\nrithms and have been successfully applied to a broad range of tasks from learning \\nto diagnose medical cases to learning to assess credit risk of loan applicants. \\n3.2 DECISION TREE REPRESENTATION \\nDecision trees classify instances by sorting them down the tree from the root to \\nsome leaf node, which provides the classification of the instance. Each node in the \\ntree specifies a test of some attribute of the instance, and each branch descending '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 64}, page_content='CHAPTER 3 DECISION TREE LEARNING 53 \\nNoma1 Strong Weak \\nNo \\n\\\\ \\nYes \\n/ \\nNo \\n\\\\ \\nYes \\nFIGURE 3.1 \\nA decision tree for the concept PlayTennis. An example is classified by sorting it through the tree \\nto the appropriate leaf node, then returning the classification associated with this leaf (in this case, \\nYes or No). This tree classifies Saturday mornings according to whether or not they are suitable for \\nplaying tennis. \\nfrom that node corresponds to one of the possible values for this attribute. An \\ninstance is classified by starting at the root node of the tree, testing the attribute \\nspecified by this node, then moving down the tree branch corresponding to the \\nvalue of the attribute in the given example. This process is then repeated for the \\nsubtree \\nrooted at the new node. \\nFigure 3.1 illustrates a typical learned decision tree. This decision tree clas- \\nsifies Saturday mornings according to whether they are suitable for playing tennis. \\nFor example, the instance \\n(Outlook = Sunny, Temperature = Hot, Humidity = High, Wind = Strong) \\nwould be sorted down the leftmost branch of this decision tree and would therefore \\nbe classified as a negative instance \\n(i.e., \\nthe tree predicts that PlayTennis = no). \\nThis tree and the example used in Table 3.2 to illustrate the ID3 learning algorithm \\nare adapted from \\n(Quinlan 1986). \\nIn general, decision trees represent a disjunction of conjunctions of con- \\nstraints on the attribute values of instances. Each path from the tree root to a leaf \\ncorresponds to a conjunction of attribute tests, and the tree itself to a disjunc- \\ntion of these conjunctions. For example, the decision tree shown in Figure 3.1 \\ncorresponds to the expression \\n(Outlook = Sunny A Humidity = Normal) \\nV (Outlook = Overcast) \\nv (Outlook = Rain A Wind = Weak) '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 65}, page_content=\"54 MACHINE LEARNWG \\n3.3 APPROPRIATE PROBLEMS FOR DECISION TREE LEARNING \\nAlthough a variety of decision tree learning methods have been developed with \\nsomewhat differing capabilities and requirements, decision tree learning is gener- \\nally best suited to problems with the following characteristics: \\nZnstances are represented by attribute-value pairs. Instances are described by \\na fixed set of attributes \\n(e.g., Temperature) and their values (e.g., Hot). The \\neasiest situation for decision tree learning is when each attribute takes on a \\nsmall number of disjoint possible values \\n(e.g., Hot, Mild, Cold). However, \\nextensions to the basic algorithm (discussed in Section 3.7.2) allow handling \\nreal-valued attributes as well \\n(e.g., \\nrepresenting Temperature numerically). \\nThe targetfunction has discrete output values. The decision tree in Figure 3.1 \\nassigns a boolean classification \\n(e.g., yes or no) to each example. Decision \\ntree methods easily extend to learning functions with more than two possible \\noutput values. A more substantial extension allows learning target functions \\nwith real-valued outputs, though the application of decision trees in this \\nsetting is less common. \\n0 Disjunctive descriptions may be required. As noted above, decision trees \\nnaturally represent disjunctive expressions. \\n0 The training data may contain errors. Decision tree learning methods are \\nrobust to errors, both errors in classifications of the training examples and \\nerrors in the attribute values that describe these examples. \\n0 The training data may contain missing attribute values. Decision tree meth- \\nods can be used even when some training examples have unknown values \\n(e.g., \\nif the Humidity of the day is known for only some of the training \\nexamples). This issue is discussed in Section 3.7.4. \\nMany practical problems have been found to fit these characteristics. De- \\ncision tree learning has therefore been applied to problems such as learning to \\nclassify medical patients by their disease, equipment malfunctions by their cause, \\nand loan applicants by their likelihood of defaulting on payments. Such problems, \\nin which the task is to classify examples into one of a discrete set of possible \\ncategories, are often referred to as classijication problems. \\nThe remainder of this chapter is organized as follows. Section 3.4 presents \\nthe basic ID3 algorithm for learning decision trees and illustrates its operation \\nin detail. Section 3.5 examines the hypothesis space search performed by this \\nlearning algorithm, contrasting it with algorithms from Chapter 2. Section 3.6 \\ncharacterizes the inductive bias of this decision tree learning algorithm and ex- \\nplores more generally an inductive bias called Occam's razor, which corresponds \\nto a preference for the most simple hypothesis. Section 3.7 discusses the issue of \\noverfitting the training data, as well as strategies such as rule post-pruning to deal \\nwith this problem. This section also discusses a number of more advanced topics \\nsuch as extending the algorithm to accommodate real-valued attributes, training \\ndata with unobserved attributes, and attributes with differing costs. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 66}, page_content='CHAPTER 3 DECISION TREE LEARMNG 55 \\n3.4 THE BASIC DECISION TREE LEARNING ALGORITHM \\nMost algorithms that have been developed for learning decision trees are vari- \\nations on a core algorithm that employs a top-down, greedy search through the \\nspace of possible decision trees. This approach is exemplified by the ID3 algorithm \\n(Quinlan 1986) and its successor C4.5 (Quinlan 1993), which form the primary \\nfocus of our discussion here. In this section we present the basic algorithm for \\ndecision tree learning, corresponding approximately to the ID3 algorithm. In Sec- \\ntion 3.7 we consider a number of extensions to this basic algorithm, including \\nextensions incorporated into C4.5 and other more recent algorithms for decision \\ntree learning. \\nOur basic algorithm, ID3, learns decision trees by constructing them \\ntop- \\ndown, beginning with the question \"which attribute should be tested at the root \\nof the \\ntree?\\'To \\nanswer this question, each instance attribute is evaluated using \\na statistical test to determine how well it alone classifies the training examples. \\nThe best attribute is selected and used as the test at the root node of the tree. \\nA descendant of the root node is then created for each possible value of this \\nattribute, and the training examples are sorted to the appropriate descendant node \\n(i.e., \\ndown the branch corresponding to the example\\'s value for this attribute). \\nThe entire process is then repeated using the training examples associated with \\neach descendant node to select the best attribute to test at that point in the tree. \\nThis forms a greedy search for an acceptable decision tree, in which the algorithm \\nnever backtracks to reconsider earlier choices. A simplified version of the algo- \\nrithm, specialized to learning boolean-valued functions \\n(i.e., \\nconcept learning), is \\ndescribed in Table 3.1. \\n3.4.1 Which Attribute Is the Best Classifier? \\nThe central choice in the ID3 algorithm is selecting which attribute to test at \\neach node in the tree. We would like to select the attribute that is most useful \\nfor classifying examples. What is a good quantitative measure of the worth of \\nan attribute? We will define a statistical property, called informution gain, that \\nmeasures how well a given attribute separates the training examples according to \\ntheir target classification. ID3 uses this information gain measure to select among \\nthe candidate attributes at each step while growing the tree. \\n3.4.1.1 ENTROPY MEASURES HOMOGENEITY OF EXAMPLES \\nIn order to define information gain precisely, we begin by defining a measure com- \\nmonly used in information theory, called entropy, that characterizes the (im)purity \\nof an arbitrary collection of examples. Given a collection S, containing positive \\nand negative examples of some target concept, the entropy of S relative to this \\nboolean classification is '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 67}, page_content='ID3(Examples, Targetattribute, Attributes) \\nExamples are the training examples. \\nTargetattribute \\nis the attribute whose value is to be \\npredicted by the tree. Attributes is a list of other attributes that may be tested by the learned \\ndecision tree. Returns a decision tree that correctly \\nclassiJies \\nthe given Examples. \\nCreate a Root node for the tree \\nIf all Examples are positive, Return the single-node tree Root, with label = + \\nIf all Examples are negative, Return the single-node tree Root, with label = - \\nIf Attributes is empty, Return the single-node tree Root, with label = most common value of \\nTargetattribute in Examples \\nOtherwise Begin \\nA t the attribute from Attributes that best* classifies Examples \\n0 The decision attribute for Root c A \\nFor each possible value, vi, of A, \\nAdd a new tree branch below Root, corresponding to the test A = vi \\n0 Let Examples,, be the subset of Examples that have value vi for A \\nIf Examples,, is empty \\nThen below this new branch add a leaf node with label = most common \\nvalue of Target attribute in Examples \\nElse below this new branch add the subtree \\nID3(Examples,,, Targetattribute, Attributes - (A))) \\nEnd \\nReturn Root \\n* The best attribute is the one with highest information gain, as defined in Equation (3.4). \\nTABLE 3.1 \\nSummary of the ID3 algorithm specialized to learning boolean-valued functions. ID3 is a greedy \\nalgorithm that grows the tree top-down, at each node selecting the attribute that best classifies the \\nlocal training examples. This process continues until the tree perfectly classifies the training examples, \\nor until all attributes have been used. \\nwhere p, is the proportion of positive examples in S and p, is the proportion of \\nnegative examples in S. In all calculations involving entropy we define 0 log 0 to \\nbe 0. \\nTo illustrate, suppose S is a collection of 14 examples of some boolean \\nconcept, including 9 positive and 5 negative examples (we adopt the notation \\n[9+, 5-1 to summarize such a sample of data). Then the entropy of S relative to \\nthis boolean classification is \\nNotice that the entropy is 0 if all members of S belong to the same class. For \\nexample, if all members are positive \\n(pe = I), then p, is 0, and Entropy(S) = \\n-1 . log2(1) - 0 . log2 0 = -1 . 0 - 0 . log2 0 = 0. Note the entropy is 1 when \\nthe collection contains an equal number of positive and negative examples. If \\nthe collection contains unequal numbers of positive and negative examples, the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 68}, page_content='CHAPTER 3 DECISION TREE LEARNING 57 \\nFIGURE 3.2 \\nThe entropy function relative to a boolean classification, \\n0.0 0.5 LO as the proportion, pe, of positive examples varies \\npe between 0 and 1. \\nentropy is between 0 and 1. Figure 3.2 shows the form of the entropy function \\nrelative to a boolean classification, as p, varies between 0 and 1. \\nOne interpretation of entropy from information theory is that it specifies the \\nminimum number of bits of information needed to encode the classification of \\nan arbitrary member of S (i.e., a member of S drawn at random with uniform \\nprobability). For example, if p, is 1, the receiver knows the drawn example will \\nbe positive, so no message need be sent, and the entropy is zero. On the other hand, \\nif pe is 0.5, one bit is required to indicate whether the drawn example is positive \\nor negative. If pe is 0.8, then a collection of messages can be encoded using on \\naverage less than 1 bit per message by assigning shorter codes to collections of \\npositive examples and longer codes to less likely negative examples. \\nThus far we have discussed entropy in the special case where the target \\nclassification is boolean. More generally, if the target attribute can take on \\nc \\ndifferent values, then the entropy of S relative to this c-wise classification is \\ndefined as \\nC \\nEntropy(S) - -pi log, pi \\ni=l \\nwhere pi is the proportion of S belonging to class i. Note the logarithm is still \\nbase 2 because entropy is a measure of the expected encoding length measured \\nin bits. Note also that if the target attribute can take on c possible values, the \\nentropy can be as large as log, c. \\n3.4.1.2 INFORMATION GAIN MEASURES THE EXPECTED REDUCTION \\nIN ENTROPY \\nGiven entropy as a measure of the impurity in a collection of training examples, \\nwe can now define a measure of the effectiveness of an attribute in classifying \\nthe \\ntraining data. The measure we will use, called information gain, is simply the \\nexpected reduction in entropy caused by partitioning the examples according to \\nthis attribute. More precisely, the information gain, Gain(S, A) of an attribute A, '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 69}, page_content='relative to a collection of examples S, is defined as \\nISVl Gain(S, A) I Entropy(S) - -Entropy (S,) IS1 (3.4) \\nveValues(A) \\nwhere Values(A) is the set of all possible values for attribute A, and S, is the \\nsubset of S for which attribute A has value v (i.e., S, = {s E SIA(s) = v)). Note \\nthe first term in Equation (3.4) is just the entropy of the original collection S, \\nand the second term is the expected value of the entropy after S is partitioned \\nusing attribute A. The expected entropy described by this second term is simply \\nthe sum of the entropies of each subset S,, weighted by the fraction of examples \\nthat belong to S,. Gain(S, A) is therefore the expected reduction in entropy \\ncaused by knowing the value of attribute A. Put another way, Gain(S, A) is the \\ninformation provided about the target &action value, given the value of some \\nother attribute A. The value of Gain(S, A) is the number of bits saved when \\nencoding the target value of an arbitrary member of S, by knowing the value of \\nattribute A. \\nFor example, suppose S is a collection of training-example days described by \\nattributes including Wind, which can have the values Weak or Strong. As before, \\nassume S is a collection containing 14 examples, [9+, 5-1. Of these 14 examples, \\nsuppose 6 of the positive and 2 of the negative examples have Wind = Weak, and \\nthe remainder have Wind = Strong. The information gain due to sorting the \\noriginal 14 examples by the attribute Wind may then be calculated as \\nValues(Wind) = Weak, Strong \\nIS, l Gain(S, Wind) = Entropy(S) - -Entropy(S,) \\nv~(Weak,Strong] Is1 \\nInformation gain is precisely the measure used by ID3 to select the best attribute at \\neach step in growing the tree. The use of information gain to evaluate the relevance \\nof attributes is summarized in Figure 3.3. In this figure the information gain of two \\ndifferent attributes, Humidity and Wind, is computed in order to determine which \\nis the better attribute for classifying the training examples shown in Table 3.2. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 70}, page_content='CHAPTER 3 DECISION TREE LEARNING 59 \\nWhich attribute is the best classifier? \\nS: [9+,5-I \\nE \\n=0.940 \\nHumidity \\nHigh \\n[3+,4-I \\n[6t,l-l \\nE \\nS.985 \\nE \\nS.592 \\nGain \\n(S, \\nHurnidiry ) \\nS: [9+,5-I \\nE \\nS.940 \\nwx Strong \\n[6+,2-I [3+,3-I \\nES.811 \\nE =1.00 \\nGain (S, Wind) \\n= ,940 - (8/14).811 - (6114)l.O \\n= ,048 \\nFIGURE 3.3 \\nHumidity provides greater information gain than Wind, relative to the target classification. Here, E \\nstands for entropy and S for the original collection of examples. Given an initial collection S of 9 \\npositive and 5 negative examples, [9+, 5-1, sorting these by their Humidity produces collections of \\n[3+, 4-1 (Humidity = High) and [6+, 1-1 (Humidity = Normal). The information gained by this \\npartitioning is \\n.151, \\ncompared to a gain of only .048 for the attribute Wind. \\n3.4.2 An Illustrative Example \\nTo illustrate the operation of ID3, consider the learning task represented by the \\ntraining examples of Table 3.2. Here the target attribute PlayTennis, which can \\nhave values yes or no for different Saturday mornings, is to be predicted based \\non other attributes of the morning in question. Consider the first step through \\nDay Outlook Temperature Humidity Wind PlayTennis \\nDl Sunny Hot High Weak No \\nD2 \\nSunny Hot High Strong No \\nD3 \\nOvercast Hot High Weak Yes \\nD4 Rain Mild High Weak Yes \\nD5 \\nRain Cool Normal Weak Yes \\nD6 \\nRain Cool Normal Strong No \\nD7 Overcast Cool Normal Strong Yes \\nD8 \\nSunny Mild High Weak No \\nD9 \\nSunny \\nCool \\nNormal Weak Yes \\nDl0 \\nRain Mild Normal Weak Yes \\nDl1 \\nSunny Mild Normal Strong Yes \\nDl2 \\nOvercast Mild High Strong Yes \\nDl3 \\nOvercast Hot Normal Weak Yes \\nDl4 \\nRain Mild High Strong No \\nTABLE 3.2 \\nTraining examples for the target concept PlayTennis. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 71}, page_content='the algorithm, in which the topmost node of the decision tree is created. Which \\nattribute should be tested first in the tree? ID3 determines the information gain for \\neach candidate attribute \\n(i.e., Outlook, Temperature, Humidity, and Wind), then \\nselects the one with highest information gain. The computation of information \\ngain for two of these attributes is shown in Figure 3.3. The information gain \\nvalues for all four attributes are \\nGain(S, Outlook) = 0.246 \\nGain(S, Humidity) = 0.151 \\nGain(S, Wind) = 0.048 \\nGain(S, Temperature) = 0.029 \\nwhere S denotes the collection of training examples from Table 3.2. \\nAccording to the information gain measure, the Outlook attribute provides \\nthe best prediction of the target attribute, PlayTennis, over the training exam- \\nples. Therefore, Outlook is selected as the decision attribute for the root node, \\nand branches are created below the root for each of its possible values \\n(i.e., \\nSunny, Overcast, and Rain). The resulting partial decision tree is shown in Fig- \\nure 3.4, along with the training examples sorted to each new descendant node. \\nNote that every example for which Outlook = Overcast is also a positive ex- \\nample of PlayTennis. Therefore, this node of the tree becomes a leaf node with \\nthe classification PlayTennis = Yes. In contrast, the descendants corresponding to \\nOutlook = Sunny and Outlook = Rain still have nonzero entropy, and the decision \\ntree will be further elaborated below these nodes. \\nThe process of selecting a new attribute and partitioning the training exam- \\nples is now repeated for each \\nnontenninal \\ndescendant node, this time using only \\nthe training examples associated with that node. Attributes that have been incor- \\nporated higher in the tree are excluded, so that any given attribute can appear at \\nmost once along any path through the tree. This process continues for each new \\nleaf node until either of two conditions is met: (1) every attribute has already been \\nincluded along this path through the tree, or (2) the training examples associated \\nwith this leaf node all have the same target attribute value \\n(i.e., \\ntheir entropy \\nis zero). Figure 3.4 illustrates the computations of information gain for the next \\nstep in growing the decision tree. The final decision tree learned by ID3 from the \\n14 training examples of Table 3.2 is shown in Figure 3.1. \\n3.5 HYPOTHESIS SPACE SEARCH IN DECISION TREE \\nLEARNING \\nAs with other inductive learning methods, ID3 can be characterized as searching a \\nspace of hypotheses for one that fits the training examples. The hypothesis space \\nsearched by ID3 is the set of possible decision trees. ID3 performs a \\nsimple-to- \\ncomplex, hill-climbing search through this hypothesis space, beginning with the \\nempty tree, then considering progressively more elaborate hypotheses in search of \\na decision tree that correctly classifies the training data. The evaluation function '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 72}, page_content=\"{Dl, D2, ..., Dl41 \\nP+S-I \\nWhich attribute should be tested here? \\nGain (Ssunnyj Temperaare) = ,970 - (215) 0.0 - (Y5) 1.0 - (115) 0.0 = ,570 \\nGain (Sss,,,, Wind) = 970 - (215) 1.0 - (315) ,918 = ,019 \\nFIGURE 3.4 \\nThe partially learned decision tree resulting from the first step of ID3. The training examples are \\nsorted to the corresponding descendant nodes. The Overcast descendant has only positive examples \\nand therefore becomes a leaf node with classification Yes. The other two nodes will be further \\nexpanded, by selecting the attribute with highest information gain relative to the new subsets of \\nexamples. \\nthat guides this hill-climbing search is the information gain measure. This search \\nis depicted in Figure 3.5. \\nBy viewing  ID^ in terms of its search space and search strategy, we can get \\nsome insight into its capabilities and limitations. \\n1~3's \\nhypothesis space of all decision trees is a complete space of finite \\ndiscrete-valued functions, relative to the available attributes. Because every \\nfinite discrete-valued function can be represented by some decision tree, ID3 \\navoids one of the major risks of methods that search incomplete hypothesis \\nspaces (such as methods that consider only conjunctive hypotheses): that the \\nhypothesis space might not contain the target function. \\nID3 maintains only a single current hypothesis as it searches through the \\nspace of decision trees. This contrasts, for example, with the earlier ver- \\nsion space \\ncandidate-~lirninat-od, \\nwhich maintains the set of all \\nhypotheses consistent with the available training examples. By determin- \\ning only a single hypothesis,  ID^ loses the capabilities that follow from \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 73}, page_content='F: + - + FIGURE 3.5 \\nHypothesis space search by ID3. \\nID3 searches throuah the mace of - \\npossible decision trees from simplest \\nto increasingly complex, guided by the \\n... ... information gain heuristic. \\nexplicitly representing all consistent hypotheses. For example, it does not \\nhave the ability to determine how many alternative decision trees are con- \\nsistent with the available training data, or to pose new instance queries that \\noptimally resolve among these competing hypotheses. \\n0 ID3 in its pure form performs no backtracking in its search. Once it,se- \\nlects \\nan attribute to test at a particular level in the tree, it never backtracks \\nto reconsider this choice. Therefore, it is susceptible to the usual risks of \\nhill-climbing search without \\nbacktracking: \\nconverging to locally optimal so- \\nlutions that are not globally optimal. In the case of ID3, a locally optimal \\nsolution corresponds to the decision tree it selects along the single search \\npath it explores. However, this locally optimal solution may be less desir- \\nable than trees that would have been encountered along a different branch of \\nthe search. Below we discuss an extension that adds a form of backtracking \\n(post-pruning the decision tree). \\n0 ID3 uses all training examples at each step in the search to make statistically \\nbased decisions regarding how to refine its current hypothesis. This contrasts \\nwith methods that make decisions incrementally, based on individual train- \\ning examples \\n(e.g., \\nFIND-S or CANDIDATE-ELIMINATION). One advantage of \\nusing statistical properties of all the examples \\n(e.g., \\ninformation gain) is that \\nthe resulting search is much less sensitive to errors in individual training \\nexamples. ID3 can be easily extended to handle noisy training data by mod- \\nifying its termination criterion to accept hypotheses that imperfectly fit the \\ntraining data. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 74}, page_content='3.6 INDUCTIVE BIAS IN DECISION TREE LEARNING \\nWhat is the policy by which ID3 generalizes from observed training examples \\nto classify unseen instances? In other words, what is its inductive bias? Recall \\nfrom Chapter 2 that inductive bias is the set of assumptions that, together with \\nthe \\ntraining data, deductively justify the classifications assigned by the learner to \\nfuture instances. \\nGiven a collection of training examples, there are typically many decision \\ntrees consistent with these examples. Describing the inductive bias of ID3 there- \\nfore consists of describing the basis by which it chooses one of these consis- \\ntent hypotheses over the others. Which of these decision trees does ID3 choose? \\nIt chooses the first acceptable tree it encounters in its simple-to-complex, \\nhill- \\nclimbing search through the space of possible trees. Roughly speaking, then, the \\nID3 search strategy (a) selects in favor of shorter trees over longer ones, and \\n(b) selects trees that place the attributes with highest information gain closest to \\nthe root. Because of the subtle interaction between the attribute selection heuris- \\ntic used by ID3 and the particular training examples it encounters, it is difficult \\nto characterize precisely the inductive bias exhibited by ID3. However, we can \\napproximately characterize its bias as a preference for short decision trees over \\ncomplex trees. \\nApproximate inductive bias of ID3: Shorter trees are preferred over larger trees. \\nIn fact, one could imagine an algorithm similar to ID3 that exhibits precisely \\nthis inductive bias. Consider an algorithm that begins with the empty tree and \\nsearches breadth Jirst through progressively more complex trees, first considering \\nall trees of depth 1, then all trees of depth 2, etc. Once it finds a decision tree \\nconsistent with the training data, it returns the smallest consistent tree at that \\nsearch depth \\n(e.g., \\nthe tree with the fewest nodes). Let us call this breadth-first \\nsearch algorithm BFS-ID3. BFS-ID3 finds a shortest decision tree and thus exhibits \\nprecisely the bias \"shorter trees are preferred over longer trees.\" ID3 can be \\nviewed as an efficient approximation to BFS-ID3, using a greedy heuristic search \\nto attempt to find the shortest tree without conducting the entire breadth-first \\nsearch through the hypothesis space. \\nBecause ID3 uses the information gain heuristic and a hill climbing strategy, \\nit \\nexhibits a more complex bias than BFS-ID3. In particular, it does not always \\nfind the shortest consistent tree, and it is biased to favor trees that place attributes \\nwith high information gain closest to the root. \\nA closer approximation to the inductive bias of ID3: Shorter trees are preferred \\nover longer trees. Trees that place high information gain attributes close to the root \\nare preferred over those that do not. \\n3.6.1 Restriction Biases and Preference Biases \\nThere is an interesting difference between the types of inductive bias exhibited \\nby ID3 and by the CANDIDATE-ELIMINATION algorithm discussed in Chapter 2. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 75}, page_content='Consider the difference between the hypothesis space search in these two ap- \\nproaches: \\nID3 searches a complete hypothesis space (i.e., one capable of expressing \\nany finite discrete-valued function). It searches incompletely through this \\nspace, from simple to complex hypotheses, until its termination condition is \\nmet \\n(e.g., \\nuntil it finds a hypothesis consistent with the data). Its inductive \\nbias is solely a consequence of the ordering of hypotheses by its search \\nstrategy. Its hypothesis space introduces no additional bias. \\n0 The version space CANDIDATE-ELIMINATION algorithm searches an incom- \\nplete hypothesis space \\n(i.e., \\none that can express only a subset of the poten- \\ntially teachable concepts). It searches this space completely, finding every \\nhypothesis consistent with the training data. Its inductive bias is solely a \\nconsequence of the expressive power of its hypothesis representation. Its \\nsearch strategy introduces no additional bias. \\nIn brief, the inductive bias of ID3 follows from its search strategy, whereas \\nthe inductive bias of the CANDIDATE-ELIMINATION algorithm follows from the def- \\ninition of its search space. \\nThe inductive bias of ID3 is thus a preference for certain hypotheses over \\nothers \\n(e.g., \\nfor shorter hypotheses), with no hard restriction on the hypotheses that \\ncan be eventually enumerated. This form of bias is typically called a preference \\nbias (or, alternatively, a search bias). In contrast, the bias of the \\nCANDIDATE- \\nELIMINATION algorithm is in the form of a categorical restriction on the set of \\nhypotheses considered. This form of bias is typically called a restriction bias (or, \\nalternatively, a language bias). \\nGiven that some form of inductive bias is required in order to generalize \\nbeyond the training data (see Chapter 2), which type of inductive bias shall we \\nprefer; a preference bias or restriction bias? \\nTypically, a preference bias is more desirable than a restriction bias, be- \\ncause it allows the learner to work within a complete hypothesis space that is \\nassured to contain the unknown target function. In contrast, a restriction bias that \\nstrictly limits the set of potential hypotheses is generally less desirable, because \\nit introduces the possibility of excluding the unknown target function altogether. \\nWhereas ID3 exhibits a purely preference bias and CANDIDATE-ELIMINATION \\na purely restriction bias, some learning systems combine both. Consider, for ex- \\nample, the program described in Chapter 1 for learning a numerical evaluation \\nfunction for game playing. In this case, the learned evaluation function is repre- \\nsented by a linear combination of a fixed set of board features, and the learning \\nalgorithm adjusts the parameters of this linear combination to best fit the available \\ntraining data. In this case, the decision to use a linear function to represent the eval- \\nuation function introduces a restriction bias (nonlinear evaluation functions cannot \\nbe represented in this form). At the same time, the choice of a particular parameter \\ntuning method (the LMS algorithm in this case) introduces a preference bias stem- \\nming from the ordered search through the space of all possible parameter values. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 76}, page_content=\"3.6.2 Why Prefer Short Hypotheses? \\nIs ID3's inductive bias favoring shorter decision trees a sound basis for generaliz- \\ning beyond the training data? Philosophers and others have debated this question \\nfor centuries, and the debate remains unresolved to this day. William of Occam \\nwas one of the first to \\ndiscusst \\nthe question, around the year 1320, so this bias \\noften goes by the name of Occam's razor. \\nOccam's razor: Prefer the simplest hypothesis that fits the data. \\nOf course giving an inductive bias a name does not justify it. Why should one \\nprefer simpler hypotheses? Notice that scientists sometimes appear to follow this \\ninductive bias. Physicists, for example, prefer simple explanations for the motions \\nof the planets, over more complex explanations. Why? One argument is that \\nbecause there are fewer short hypotheses than long ones (based on straightforward \\ncombinatorial arguments), it is less likely that one will find a short hypothesis that \\ncoincidentally fits the training data. In contrast there are often many very complex \\nhypotheses that fit the current training data but fail to generalize correctly to \\nsubsequent data. Consider decision tree hypotheses, for example. There are many \\nmore 500-node decision trees than 5-node decision trees. Given a small set of \\n20 training examples, we might expect to be able to find many 500-node deci- \\nsion trees consistent with these, whereas we would be more surprised if a 5-node \\ndecision tree could perfectly fit this data. We might therefore believe the 5-node \\ntree is less likely to be a statistical coincidence and prefer this hypothesis over \\nthe 500-node hypothesis. \\nUpon closer examination, it turns out there is a major difficulty with the \\nabove argument. By the same reasoning we could have argued that one should \\nprefer decision trees containing exactly 17 leaf nodes with 11 nonleaf nodes, that \\nuse the decision attribute A1 at the root, and test attributes A2 through All, in \\nnumerical order. There are relatively few such trees, and we might argue (by the \\nsame reasoning as above) that our a priori chance of finding one consistent with \\nan arbitrary set of data is therefore small. The difficulty here is that there are very \\nmany small sets of hypotheses that one can define-most of them rather arcane. \\nWhy should we believe that the small set of hypotheses consisting of decision \\ntrees with short descriptions should be any more relevant than the multitude of \\nother small sets of hypotheses that we might define? \\nA second problem with the above argument for Occam's razor is that the size \\nof \\na hypothesis is determined by the particular representation used internally by \\nthe \\nlearner. Two learners using different internal representations could therefore \\nanive \\nat different hypotheses, both justifying their contradictory conclusions by \\nOccam's \\nrazor! For example, the function represented by the learned decision \\ntree \\nin Figure 3.1 could be represented as a tree with just one decision node, by a \\nlearner that uses the boolean attribute XYZ, where we define the attribute XYZ to \\n~~prentl~ while shaving. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 77}, page_content=\"be true for instances that are classified positive by the decision tree in Figure 3.1 \\nand false otherwise. Thus, two learners, both applying Occam's razor, would \\ngeneralize in different ways if one used the XYZ attribute to describe its examples \\nand the other used only the attributes Outlook, Temperature, Humidity, and Wind. \\nThis last argument shows that Occam's razor will produce two different \\nhypotheses from the same training examples when it is applied by two learners \\nthat perceive these examples in terms of different internal representations. On this \\nbasis we might be tempted to reject Occam's razor altogether. However, consider \\nthe following scenario that examines the question of which internal representa- \\ntions might arise from a process of evolution and natural selection. Imagine a \\npopulation of artificial learning agents created by a simulated evolutionary pro- \\ncess involving reproduction, mutation, and natural selection of these agents. Let \\nus assume that this evolutionary process can alter the perceptual systems of these \\nagents from generation to generation, thereby changing the internal attributes by \\nwhich they perceive their world. For the sake of argument, let us also assume that \\nthe learning agents employ a fixed learning algorithm (say ID3) that cannot be \\naltered by evolution. It is reasonable to assume that over time evolution will pro- \\nduce internal representation that make these agents increasingly successful within \\ntheir environment. Assuming that the success of an agent depends highly on its \\nability to generalize accurately, we would therefore expect evolution to develop \\ninternal representations that work well with whatever learning algorithm and in- \\nductive bias is present. If the species of agents employs a learning algorithm whose \\ninductive bias is Occam's razor, then we expect evolution to produce internal rep- \\nresentations for which Occam's razor is a successful strategy. The essence of the \\nargument here is that evolution will create internal representations that make the \\nlearning algorithm's inductive bias a self-fulfilling prophecy, simply because it \\ncan alter the representation easier than it can alter the learning algorithm. \\nFor now, we leave the debate regarding Occam's razor. We will revisit it in \\nChapter 6, where we discuss the Minimum Description Length principle, a version \\nof Occam's razor that can be interpreted within a Bayesian framework. \\n3.7 ISSUES IN DECISION TREE LEARNING \\nPractical issues in learning decision trees include determining how deeply to grow \\nthe decision tree, handling continuous attributes, choosing an appropriate attribute \\nselection measure, \\nandling \\ntraining data with missing attribute values, handling \\nattributes with differing costs, and improving computational efficiency. Below \\nwe discuss each of these issues and extensions to the basic ID3 algorithm that \\naddress them. ID3 has itself been extended to address most of these issues, with \\nthe resulting system renamed C4.5 (Quinlan 1993). \\n3.7.1 Avoiding Overfitting the Data \\nThe algorithm described in Table 3.1 grows each branch of the tree just deeply \\nenough to perfectly classify the training examples. While this is sometimes a \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 78}, page_content=\"reasonable strategy, in fact it can lead to difficulties when there is noise in the data, \\nor \\nwhen the number of training examples is too small to produce a representative \\nsample of the true target function. In either of these cases, this simple algorithm \\ncan \\nproduce trees that overjt the training examples. \\nWe will say that a hypothesis \\noverfits \\nthe training examples if some other \\nhypothesis that fits the training examples less well actually performs better over the \\nentire distribution of instances \\n(i.e., \\nincluding instances beyond the training set). \\nDefinition: Given a hypothesis space H, a hypothesis h E H is said to overlit the \\ntraining data if there exists some alternative hypothesis h' E H, such that h has \\nsmaller error than h' over the training examples, but h' has a smaller error than h \\nover the entire distribution of instances. \\nFigure 3.6 illustrates the impact of overfitting in a typical application of deci- \\nsion tree learning. In this case, the ID3 algorithm is applied to the task of learning \\nwhich medical patients have a form of diabetes. The horizontal axis of this plot \\nindicates the total number of nodes in the decision tree, as the tree is being con- \\nstructed. The vertical axis indicates the accuracy of predictions made by the tree. \\nThe solid line shows the accuracy of the decision tree over the training examples, \\nwhereas the broken line shows accuracy measured over an independent set of test \\nexamples (not included in the training set). Predictably, the accuracy of the tree \\nover the training examples increases monotonically as the tree is grown. How- \\never, the accuracy measured over the independent test examples first increases, \\nthen decreases. As can be seen, once the tree size exceeds approximately 25 nodes, \\nOn training data - \\nOn test data ---- i \\nSize of tree (number of nodes) \\nFIGURE 3.6 \\nOverfitting in decision tree learning. As ID3 adds new nodes to grow the decision tree, the accuracy of \\nthe \\ntree measured over the training examples increases monotonically. However, when measured over \\na set of test examples independent of the training examples, accuracy first increases, then decreases. \\nSoftware and data for experimenting with variations on this plot are available on the World Wide \\nWeb at http://www.cs.cmu.edu/-torn/mlbook.html. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 79}, page_content=\"further elaboration of the tree decreases its accuracy over the test examples despite \\nincreasing its accuracy on the training examples. \\nHow can it be possible for tree h to fit the training examples better than h', \\nbut for it to perform more poorly over subsequent examples? One way this can \\noccur is when the training examples contain random errors or noise. To illustrate, \\nconsider the effect of adding the following positive training example, incorrectly \\nlabeled as negative, to the (otherwise correct) examples in Table 3.2. \\n(Outlook = Sunny, Temperature = Hot, Humidity = Normal, \\nWind = Strong, PlayTennis = No) \\nGiven the original error-free data, ID3 produces the decision tree shown in Fig- \\nure 3.1. However, the addition of this incorrect example will now cause ID3 to \\nconstruct a more complex tree. In particular, the new example will be sorted into \\nthe second leaf node from the left in the learned tree of Figure 3.1, along with the \\nprevious positive examples D9 and \\nDl \\n1. Because the new example is labeled as \\na negative example, ID3 will search for further refinements to the tree below this \\nnode. Of course as long as the new erroneous example differs in some arbitrary \\nway from the other examples affiliated with this node, ID3 will succeed in finding \\na new decision attribute to separate out this new example from the two previous \\npositive examples at this tree node. The result is that ID3 will output a decision \\ntree (h) that is more complex than the original tree from Figure 3.1 (h'). Of course \\nh will fit the collection of training examples perfectly, whereas the simpler h' will \\nnot. However, given that the new decision node is simply a consequence of fitting \\nthe noisy training example, we expect h to outperform h' over subsequent data \\ndrawn from the same instance distribution. \\nThe above example illustrates how random noise in the training examples \\ncan lead to overfitting. In fact, overfitting is possible even when the training data \\nare noise-free, especially when small numbers of examples are associated with leaf \\nnodes. In this case, it is quite possible for coincidental regularities to occur, in \\nwhich some attribute happens to partition the examples very well, despite being \\nunrelated to the actual target function. Whenever such coincidental regularities \\nexist, there is a risk of overfitting. \\nOverfitting is a significant practical difficulty for decision tree learning and \\nmany other learning methods. For example, in one experimental study of ID3 \\ninvolving five different learning tasks with noisy, nondeterministic data (Mingers \\n1989b), \\noverfitting was found to decrease the accuracy of learned decision trees \\nby 10-25% on most problems. \\nThere are several approaches to avoiding overfitting in decision tree learning. \\nThese can be grouped into two classes: \\napproaches that stop growing the tree earlier, before it reaches the point \\nwhere it perfectly classifies the training data, \\n0 approaches that allow the tree to overfit the data, and then post-prune the \\ntree. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 80}, page_content='Although the first of these approaches might seem.more direct, the second \\napproach of post-pruning \\noverfit \\ntrees has been found to be more successful in \\npractice. This is due to the difficulty in the first approach of estimating precisely \\nwhen to stop growing the tree. \\nRegardless of whether the correct tree size is found by stopping early or \\nby post-pruning, a key question is what criterion is to be used to determine the \\ncorrect final tree size. Approaches include: \\n0 Use a separate set of examples, distinct from the training examples, to eval- \\nuate the utility of post-pruning nodes from the tree. \\n0 Use all the available data for training, but apply a statistical test to estimate \\nwhether expanding (or pruning) a particular node is likely to produce an \\nimprovement beyond the training set. For example, Quinlan (1986) uses a \\nchi-square test to estimate whether further expanding a node is likely to \\nimprove performance over the entire instance distribution, or only on the \\ncurrent sample of training data. \\n0 Use an explicit measure of the complexity for encoding the training exam- \\nples and the decision tree, halting growth of the tree when this encoding \\nsize is minimized. This approach, based on a heuristic called the Minimum \\nDescription Length principle, is discussed further in Chapter 6, as well as \\nin Quinlan and \\nRivest \\n(1989) and Mehta et al. (199.5). \\nThe first of the above approaches is the most common and is often referred \\nto as a training and validation set approach. We discuss the two main variants of \\nthis approach below. In this approach, the available data are separated into two \\nsets of examples: a training set, which is used to form the learned hypothesis, and \\na separate validation set, which is used to evaluate the accuracy of this hypothesis \\nover subsequent data and, in particular, to evaluate the impact of pruning this \\nhypothesis. The motivation is this: Even though the learner may be misled by \\nrandom errors and coincidental regularities within the training set, the validation \\nset is unlikely to exhibit the same random fluctuations. Therefore, the validation \\nset can be expected to provide a safety check against overfitting the spurious \\ncharacteristics of the training set. Of course, it is important that the validation set \\nbe large enough to itself provide a statistically significant sample of the instances. \\nOne common heuristic is to withhold one-third of the available examples for the \\nvalidation set, using the other two-thirds for training. \\n3.7.1.1 REDUCED ERROR PRUNING \\nHow exactly might we use a validation set to prevent overfitting? One approach, \\ncalled reduced-error pruning (Quinlan \\n1987), \\nis to consider each of the decision \\nnodes in \\nthe.tree \\nto be candidates for pruning. Pruning a decision node consists of \\nremoving the \\nsubtree \\nrooted at that node, making it a leaf node, and assigning it \\nthe most common classification of the training examples affiliated with that node. \\nNodes are removed only if the resulting pruned tree performs no worse than-the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 81}, page_content='original over the validation set. This has the effect that any leaf node added due \\nto coincidental regularities in the training set is likely to be pruned because these \\nsame coincidences are unlikely to occur in the validation set. Nodes are pruned \\niteratively, always choosing the node whose removal most increases the decision \\ntree accuracy over the validation set. Pruning of nodes continues until further \\npruning is harmful \\n(i.e., \\ndecreases accuracy of the tree over the validation set). \\nThe impact of reduced-error pruning on the accuracy of the decision tree \\nis illustrated in Figure 3.7. As in Figure 3.6, the accuracy of the tree is shown \\nmeasured over both training examples and test examples. The additional line in \\nFigure 3.7 shows accuracy over the test examples as the tree is pruned. When \\npruning begins, the tree is at its maximum size and lowest accuracy over the test \\nset. As pruning proceeds, the number of nodes is reduced and accuracy over the \\ntest set increases. Here, the available data has been split into three subsets: the \\ntraining \\nexamples, the validation examples used for pruning the tree, and a set of \\ntest examples used to provide an unbiased estimate of accuracy over future unseen \\nexamples. The plot shows accuracy over the training and test sets. Accuracy over \\nthe validation set used for pruning is not shown. \\nUsing a separate set of data to guide pruning is an effective approach pro- \\nvided a large amount of data is available. The major drawback of this approach \\nis that when data is limited, withholding part of it for the validation set reduces \\neven further the number of examples available for training. The following section \\npresents an alternative approach to pruning that has been found useful in many \\npractical situations where data is limited. Many additional techniques have been \\nproposed as well, involving partitioning the available data several different times in \\n7 \\n---..--..._.._.._~~ \\n\" .------.------- 2--... -, .--.. -..... -... .-... \\n\\\\_____.. \\n--... -.... --... -.___._..___...-_-------- \\nOn training data - \\nOn test data ---- \\nOn test data (during pruning) - - - - - \\n0 10 20 30 40 50 60 70 80 90 100 \\nSize of tree (number of nodes) \\nFIGURE 3.7 \\nEffect of reduced-error pruning in decision tree learning. This plot shows the same curves of training \\nand test set accuracy as in Figure 3.6. In addition, it shows the impact of reduced error pruning of \\nthe tree produced by ID3. Notice the increase in accuracy over the test set as nodes are pruned from \\nthe tree. Here, the validation set used for pruning is distinct from both the training and test sets. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 82}, page_content='multiple ways, then averaging the results. Empirical evaluations of alternative tree \\npruning methods are reported by Mingers \\n(1989b) \\nand by Malerba et al. (1995). \\n3.7.1.2 RULE POST-PRUNING \\nIn practice, one quite successful method for finding high accuracy hypotheses is \\na technique we shall call rule post-pruning. A variant of this pruning method is \\nused by C4.5 (Quinlan \\n1993), \\nwhich is an outgrowth of the original ID3 algorithm. \\nRule post-pruning involves the following steps: \\n1. Infer the decision tree from the training set, growing the tree until the training \\ndata is fit as well as possible and allowing overfitting to occur. \\n2. Convert the learned tree into an equivalent set of rules by creating one rule \\nfor each path from the root node to a leaf node. \\n3. Prune (generalize) each rule by removing any preconditions that result in \\nimproving its estimated accuracy. \\n4. Sort the pruned rules by their estimated accuracy, and consider them in this \\nsequence when classifying subsequent instances. \\nTo illustrate, consider again the decision tree in Figure 3.1. In rule \\npost- \\npruning, one rule is generated for each leaf node in the tree. Each attribute test \\nalong the path from the root to the leaf becomes a rule antecedent (precondition) \\nand the classification at the leaf node becomes the rule consequent (postcondition). \\nFor example, the \\nleftmost \\npath of the tree in Figure 3.1 is translated into the rule \\nIF (Outlook = Sunny) A (Humidity = High) \\nTHEN \\nPlayTennis = No \\nNext, each such rule is pruned by removing any antecedent, or precondi- \\ntion, whose removal does not worsen its estimated accuracy. Given the above \\nrule, for example, rule post-pruning would consider removing the preconditions \\n(Outlook = Sunny) and (Humidity = High). It would select whichever of these \\npruning steps produced the greatest improvement in estimated rule accuracy, then \\nconsider pruning the second precondition as a further pruning step. No pruning \\nstep is performed if it reduces the estimated rule accuracy. \\nAs noted above, one method to estimate rule accuracy is to use a validation \\nset of examples disjoint from the training set. Another method, used by C4.5, \\nis \\nto evaluate performance based on the training set itself, using a pessimistic \\nestimate to make up for the fact that the training data gives an estimate biased \\nin \\nfavor of the rules. More precisely, C4.5 calculates its pessimistic estimate by \\ncalculating the rule accuracy over the training examples to which it applies, then \\ncalculating the standard deviation in this estimated accuracy assuming a binomial \\ndistribution. For a given confidence level, the lower-bound estimate is then taken \\nas the measure of rule performance \\n(e.g., \\nfor a 95% confidence interval, rule \\naccuracy is pessimistically estimated by the observed accuracy over the training '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 83}, page_content='set, minus 1.96 times the estimated standard deviation). The net effect is that for \\nlarge data sets, the pessimistic estimate is very close to the observed accuracy \\n(e.g., \\nthe standard deviation is very small), whereas it grows further from the \\nobserved accuracy as the size of the data set decreases. Although this heuristic \\nmethod is not statistically valid, it has nevertheless been found useful in practice. \\nSee Chapter 5 for a discussion of statistically valid approaches to estimating means \\nand confidence intervals. \\nWhy convert the decision tree to rules before pruning? There are three main \\nadvantages. \\nConverting to rules allows distinguishing among the different contexts in \\nwhich a decision node is used. Because each distinct path through the deci- \\nsion tree node produces a distinct rule, the pruning decision regarding that \\nattribute test can be made differently for each path. In contrast, if the tree \\nitself were pruned, the only two choices would be to remove the decision \\nnode completely, or to retain it in its original form. \\nConverting to rules removes the distinction between attribute tests that occur \\nnear the root of the tree and those that occur near the leaves. Thus, we avoid \\nmessy bookkeeping issues such as how to reorganize the tree if the root node \\nis pruned while retaining part of the \\nsubtree \\nbelow this test. \\nConverting to rules improves readability. Rules are often easier for \\nto understand. \\n3.7.2 Incorporating Continuous-Valued Attributes \\nOur initial definition of ID3 is restricted to attributes that take on a discrete set \\nof values. First, the target attribute whose value is predicted by the learned tree \\nmust be discrete valued. Second, the attributes tested in the decision nodes of \\nthe tree must also be discrete valued. This second restriction can easily be re- \\nmoved so that continuous-valued decision attributes can be incorporated into the \\nlearned tree. This can be accomplished by dynamically defining new \\ndiscrete- \\nvalued attributes that partition the continuous attribute value into a discrete set \\nof intervals. In particular, for an attribute A that is continuous-valued, the algo- \\nrithm can dynamically create a new boolean attribute A, that is true if A < c \\nand false otherwise. The only question is how to select the best value for the \\nthreshold c. \\nAs an example, suppose we wish to include the continuous-valued attribute \\nTemperature in describing the training example days in the learning task of Ta- \\nble 3.2. Suppose further that the training examples associated with a particular \\nnode in the decision tree have the following values for Temperature and the target \\nattribute PlayTennis. \\nTemperature: 40 48 60 72 80 90 \\nPlayTennis: No No Yes Yes Yes NO '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 84}, page_content='CHAPTER 3 DECISION TREE LEARNING 73 \\nWhat threshold-based boolean attribute should be defined based on Temper- \\nature? Clearly, we would like to pick a threshold, c, that produces the greatest \\ninformation gain. By sorting the examples according to the continuous attribute \\nA, then identifying adjacent examples that differ in their target classification, we \\ncan generate a set of candidate thresholds midway between the corresponding \\nvalues of A. It can be shown that the value of c that maximizes information gain \\nmust always lie at such a boundary (Fayyad 1991). These candidate thresholds \\ncan then be evaluated by computing the information gain associated with each. \\nIn \\nthe current example, there are two candidate thresholds, corresponding to the \\nvalues of Temperature at which the value of \\nPlayTennis \\nchanges: (48 + 60)/2, \\nand (80 + 90)/2. The information gain can then be computed for each of the \\ncandidate attributes, \\nTemperat~re,~~ \\nand \\nTemperat~re,~~, \\nand the best can be \\nselected \\n(Temperat~re,~~). \\nThis dynamically created boolean attribute can then \\ncompete with the other discrete-valued candidate attributes available for growing \\nthe decision tree. Fayyad and \\nIrani \\n(1993) discuss an extension to this approach \\nthat splits the continuous attribute into multiple intervals rather than just two in- \\ntervals based on a single threshold. Utgoff and Brodley (1991) and \\nMurthy \\net al. \\n( 1994) discuss approaches that define features by thresholding linear combinations \\nof several continuous-valued attributes. \\n3.7.3 Alternative Measures for Selecting Attributes \\nThere is a natural bias in the information gain measure that favors attributes with \\nmany values over those with few values. As an extreme example, consider the \\nattribute Date, which has a very large number of possible values \\n(e.g., \\nMarch 4, \\n1979). If we were to add this attribute to the data in Table 3.2, it would have \\nthe highest information gain of any of the attributes. This is because Date alone \\nperfectly predicts the target attribute over the training data. Thus, it would be \\nselected as the decision attribute for the root node of the tree and lead to a (quite \\nbroad) tree of depth one, which perfectly classifies the training data. Of course, \\nthis decision tree would fare poorly on subsequent examples, because it is not a \\nuseful predictor despite the fact that it perfectly separates the training data. \\nWhat is wrong with the attribute Date? Simply put, it has so many possible \\nvalues that it is bound to separate the training examples into very small subsets. \\nBecause of this, it will have a very high information gain relative to the training \\nexamples, despite being a very poor predictor of the target function over unseen \\ninstances. \\nOne way to avoid this difficulty is to select decision attributes based on some \\nmeasure other than information gain. One alternative measure that has been used \\nsuccessfully is the gain ratio (Quinlan 1986). The gain ratio measure penalizes \\nattributes such as Date by incorporating a term, called split informution, that is \\nsensitive to how broadly and uniformly the attribute splits the data: '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 85}, page_content=\"74 MACHINE LEARNING \\nwhere S1 through S, are the c subsets of examples resulting from partitioning S \\nby the c-valued attribute A. Note that \\nSplitlnfomzation \\nis actually the entropy of \\nS with respect to the values of attribute A. This is in contrast to our previous \\nuses of entropy, in which we considered only the entropy of S with respect to the \\ntarget attribute whose value is to be predicted by the learned tree. \\nThe Gain Ratio measure is defined in terms of the earlier Gain measure, as \\nwell as this \\nSplitlnfomzation, as follows \\nGain (S, A) \\nGainRatio(S, \\nA) r Split Inf ormation(S, A) \\nNotice that the Splitlnfomzation term discourages the selection of attributes with \\nmany uniformly distributed values. For example, consider a collection of n ex- \\namples that are completely separated by attribute A (e.g., Date). In this case, the \\nSplitlnfomzation value will be log, n. In contrast, a boolean attribute B that splits \\nthe same n examples exactly in half will have Splitlnfomzation of 1. If attributes \\nA and B produce the same information gain, then clearly B will score higher \\naccording to the Gain Ratio measure. \\nOne practical issue that arises in using GainRatio in place of Gain to \\nselect attributes is that the denominator can be zero or very small when \\nISi 1 x IS1 \\nfor one of the \\nSi. \\nThis either makes the GainRatio undefined or very large for \\nattributes that happen to have the same value for nearly all members of S. To \\navoid selecting attributes purely on this basis, we can adopt some heuristic such \\nas first calculating the Gain of each attribute, then applying the GainRatio test \\nonly considering those attributes with above average Gain (Quinlan 1986). \\nAn alternative to the GainRatio, designed to directly address the above \\ndifficulty, is a distance-based measure introduced by Lopez de \\nMantaras \\n(1991). \\nThis measure is based on defining a distance metric between partitions \\nof'the \\ndata. Each attribute is evaluated based on the distance between the data partition \\nit creates and the perfect partition \\n(i.e., \\nthe partition that perfectly classifies the \\ntraining data). The attribute whose partition is closest to the perfect partition is \\nchosen. Lopez de \\nMantaras \\n(1991) defines this distance measure, proves that it \\nis not biased toward attributes with large numbers of values, and reports experi- \\nmental studies indicating that the predictive accuracy of the induced trees is not \\nsignificantly different from that obtained with the Gain and Gain Ratio measures. \\nHowever, this distance measure avoids the practical difficulties associated with the \\nGainRatio measure, and in his experiments it produces significantly smaller trees \\nin the case of data sets whose attributes have very different numbers of values. \\nA variety of other selection measures have been proposed as well \\n(e.g., \\nsee Breiman et al. 1984; Mingers \\n1989a; \\nKearns \\nand \\nMansour \\n1996; Dietterich \\net al. 1996). Mingers \\n(1989a) \\nprovides an experimental analysis of the relative \\neffectiveness of several selection measures over a variety of problems. He reports \\nsignificant differences in the sizes of the unpruned trees produced by the different \\nselection measures. However, in his experimental domains the choice of attribute \\nselection measure appears to have a smaller impact on final accuracy than does \\nthe extent and method of post-pruning. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 86}, page_content='CHAPTER 3 DECISION TREE LEARNING 75 \\n3.7.4 Handling Training Examples with Missing Attribute Values \\nIn certain cases, the available data may be missing values for some attributes. \\nFor example, in a medical domain in which we wish to predict patient outcome \\nbased on various laboratory tests, it may be that the lab test Blood-Test-Result is \\navailable only for a subset of the patients. In such cases, it is common to estimate \\nthe \\nmissing attribute value based on other examples for which this attribute has a \\nknown value. \\nConsider the situation in which Gain(S, A) is to be calculated at node n in \\nthe decision tree to evaluate whether the attribute A is the best attribute to test \\nat \\nthis decision node. Suppose that (x, c(x)) is one of the training examples in S \\nand that the value A(x) is unknown. \\nOne strategy for dealing with the missing attribute value is to assign it the \\nvalue that is most common among training examples at node n. Alternatively, we \\nmight assign it the most common value among examples at node n that have the \\nclassification c(x). The elaborated training example using this estimated value for \\nA(x) can then be used directly by the existing decision tree learning algorithm. \\nThis strategy is examined by Mingers \\n(1989a). \\nA second, more complex procedure is to assign a probability to each of the \\npossible values of A rather than simply assigning the most common value to A(x). \\nThese probabilities can be estimated again based on the observed frequencies of \\nthe various values for A among the examples at node n. For example, given a \\nboolean attribute A, if node n contains six known examples with A = 1 and four \\nwith A = 0, then we would say the probability that A(x) = 1 is 0.6, and the \\nprobability that A(x) = 0 is 0.4. A fractional 0.6 of instance x is now distributed \\ndown the branch for A = 1, and a fractional 0.4 of x down the other tree branch. \\nThese fractional examples are used for the purpose of computing information \\nGain and can be further subdivided at subsequent branches of the tree if a second \\nmissing attribute value must be tested. This same fractioning of examples can \\nalso be applied after learning, to classify new instances whose attribute values \\nare unknown. In this case, the classification of the new instance is simply the \\nmost probable classification, computed by summing the weights of the instance \\nfragments classified in different ways at the leaf nodes of the tree. This method \\nfor \\nhandling missing attribute values is used in C4.5 (Quinlan 1993). \\n3.7.5 Handling Attributes with Differing Costs \\nIn some learning tasks the instance attributes may have associated costs. For \\nexample, in learning to classify medical diseases we might describe patients in \\nterms of attributes such as Temperature, BiopsyResult, Pulse, BloodTestResults, \\netc. \\nThese attributes vary significantly in their costs, both in terms of monetary \\ncost and cost to patient comfort. In such tasks, we would prefer decision trees that \\nuse \\nlow-cost attributes where possible, relying on high-cost attributes only when \\nneeded to produce reliable classifications. \\nID3 can be modified to take into account attribute costs by introducing a cost \\nterm into the attribute selection measure. For example, we might divide the Gpin '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 87}, page_content=\"by the cost of the attribute, so that lower-cost attributes would be preferred. While \\nsuch cost-sensitive measures do not guarantee finding an optimal cost-sensitive \\ndecision tree, they do bias the search in favor of low-cost attributes. \\nTan and Schlimmer (1990) and Tan (1993) describe one such approach and \\napply it to a robot perception task in which the robot must learn to classify dif- \\nferent objects according to how they can be grasped by the robot's manipulator. \\nIn this case the attributes correspond to different sensor readings obtained by a \\nmovable sonar on the robot. Attribute cost is measured by the number of seconds \\nrequired to obtain the attribute value by positioning and operating the sonar. They \\ndemonstrate that more efficient recognition strategies are learned, without sacri- \\nficing classification accuracy, by replacing the information gain attribute selection \\nmeasure by the following measure \\nCost (A) \\nNunez (1988) describes a related approach and its application to learning \\nmedical diagnosis rules. Here the attributes are different symptoms and laboratory \\ntests with differing costs. His system uses a somewhat different attribute selection \\nmeasure \\n2GaWS.A) - 1 \\n(Cost(A) + \\nwhere w E [0, 11 is a constant that determines the relative importance of cost \\nversus information gain. Nunez (1991) presents an empirical comparison of these \\ntwo approaches over a range of tasks. \\n3.8 SUMMARY AND FURTHER READING \\nThe main points of this chapter include: \\nDecision tree learning provides a practical method for concept learning and \\nfor learning other discrete-valued functions. The ID3 family of algorithms \\ninfers decision trees by growing them from the root downward, greedily \\nselecting the next best attribute for each new decision branch added to the \\ntree. \\nID3 searches a complete hypothesis space (i.e., the space of decision trees \\ncan represent any discrete-valued function defined over discrete-valued in- \\nstances). It thereby avoids the major difficulty associated with approaches \\nthat consider only restricted sets of hypotheses: that the target function might \\nnot be present in the hypothesis space. \\nThe inductive bias implicit in ID3 includes a preference for smaller trees; \\nthat is, its search through the hypothesis space grows the tree only as large \\nas needed in order to classify the available training examples. \\nOverfitting the training data is an important issue in decision tree learning. \\nBecause the training examples are only a sample of all possible instances, \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 88}, page_content=\"CHAFER 3 DECISION TREE LEARNING 77 \\nit is possible to add branches to the tree that improve performance on the \\ntraining examples while decreasing performance on other instances outside \\nthis set. Methods for post-pruning the decision tree are therefore important \\nto avoid overfitting in decision tree learning (and other inductive inference \\nmethods that employ a preference bias). \\nA large variety of extensions to the basic ID3 algorithm has been developed \\nby different researchers. These include methods for post-pruning trees, han- \\ndling real-valued attributes, accommodating training examples with miss- \\ning attribute values, incrementally refining decision trees as new training \\nexamples become available, using attribute selection measures other than \\ninformation gain, and considering costs associated with instance attributes. \\nAmong the earliest work on decision tree learning is Hunt's Concept Learn- \\ning System (CLS) (Hunt et al. 1966) and \\nFriedman \\nand Breiman's work resulting \\nin the CART system (Friedman 1977; Breiman et al. 1984). Quinlan's ID3 sys- \\ntem (Quinlan 1979, 1983) forms the basis for the discussion in this chapter. Other \\nearly work on decision tree learning includes ASSISTANT (Kononenko et al. 1984; \\nCestnik et al. 1987). Implementations of decision tree induction algorithms are \\nnow commercially available on many computer platforms. \\nFor further details on decision tree induction, an excellent book by Quinlan \\n(1993) discusses many practical issues and provides executable code for C4.5. \\nMingers \\n(1989a) \\nand \\nBuntine \\nand Niblett (1992) provide two experimental studies \\ncomparing different attribute-selection measures. Mingers \\n(1989b) \\nand Malerba et \\nal. (1995) provide studies of different pruning strategies. Experiments comparing \\ndecision tree learning and other learning methods can be found in numerous \\npapers, including (Dietterich et al. 1995; Fisher and \\nMcKusick \\n1989; Quinlan \\n1988a; \\nShavlik et al. 1991; \\nThrun \\net al. 1991; Weiss and Kapouleas 1989). \\nEXERCISES \\nGive decision trees to represent the following boolean functions: \\n(a) A A -B \\n(b) A V [B A C] \\n(c) A XOR B \\n(d) [A A B] v [C A Dl \\nConsider the following set of training examples: \\nInstance Classification a1 a2 \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 89}, page_content='(a) What is the entropy of this collection of training examples with respect to the \\ntarget function classification? \\n(b) What is the information gain of a2 relative to these training examples? \\n3.3. True or false: If decision tree D2 is an elaboration of tree Dl, then Dl is more- \\ngeneral-than D2. Assume Dl and D2 are decision trees representing arbitrary boolean \\nfunctions, and that D2 is an elaboration of \\nDl \\nif ID3 could extend \\nDl \\ninto D2. If true, \\ngive a proof; if false, a counterexample. (More-general-than is defined in Chapter 2.) \\n3.4. ID3 searches for just one consistent hypothesis, whereas the CANDIDATE- \\nELIMINATION algorithm finds all consistent hypotheses. Consider the correspondence \\nbetween these two learning algorithms. \\n(a) Show the decision tree that would be learned by ID3 assuming it is given the \\nfour training examples for the Enjoy Sport? target concept shown in Table 2.1 \\nof Chapter 2. \\n(b) What is the relationship between the learned decision tree and the version space \\n(shown in Figure 2.3 of Chapter 2) that is learned from these same examples? \\nIs the learned tree equivalent to one of the members of the version space? \\n(c) Add the following training example, and compute the new decision tree. This \\ntime, show the value of the information gain for each candidate attribute at each \\nstep in growing the tree. \\nSky Air-Temp Humidity Wind Water Forecast Enjoy-Sport? \\nSunny Warm Normal Weak Warm Same No \\n(d) Suppose we wish to design a learner that (like ID3) searches a space of decision \\ntree hypotheses and (like CANDIDATE-ELIMINATION) finds all hypotheses con- \\nsistent with the data. In short, we wish to apply the CANDIDATE-ELIMINATION \\nalgorithm to searching the space of decision tree hypotheses. Show the S and \\nG sets that result from the first training example from Table 2.1. Note S must \\ncontain the most specific decision trees consistent with the data, whereas G must \\ncontain the most general. Show how the S and G sets are refined by thesecond \\ntraining example (you may omit syntactically distinct trees that describe the same \\nconcept). What difficulties do you foresee in applying CANDIDATE-ELIMINATION \\nto a decision tree hypothesis space? \\nREFERENCES \\nBreiman, L., Friedman, J. H., Olshen, R. A., & Stone, P. 1. (1984). ClassiJication and regression \\ntrees. Belmont, CA: Wadsworth International Group. \\nBrodley, C. E., & Utgoff, P. E. (1995). Multivariate decision trees. Machine Learning, 19, 45-77. \\nBuntine, \\nW., & Niblett, T. (1992). A further comparison of splitting rules for decision-tree induction. \\nMachine Learning, 8, 75-86. \\nCestnik, B., Kononenko, I., & Bratko, I. (1987). ASSISTANT-86: A knowledge-elicitation tool for \\nsophisticated users. In I. Bratko & N. LavraE (Eds.), Progress in machine learning. Bled, \\nYugoslavia: Sigma Press. \\nDietterich, T. G., Hild, H., & Bakiri, G. (1995). A comparison of ID3 and BACKPROPAGATION for \\nEnglish text-to-speech mapping. Machine Learning, 18(1), 51-80. \\nDietterich, T. G., Kearns, M., & Mansour, Y. (1996). Applying the weak learning framework to \\nunderstand and improve C4.5. Proceedings of the 13th International Conference on Machine \\nLearning (pp. 96104). San Francisco: Morgan Kaufmann. \\nFayyad, \\nU. M. (1991). On the induction of decision trees for multiple concept leaning, (Ph.D. dis- \\nsertation). EECS Department, University of Michigan. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 90}, page_content=\"Cm 3 DECISION TREE LEARNING 79 \\nFayyad, U. M., & Irani, K. B. (1992). On the handling of continuous-valued attributes in decision \\ntree generation. Machine Learning, 8, 87-102. \\nFayyad, \\nU. M., & Irani, K. B. (1993). Multi-interval discretization of continuous-valued attributes \\nfor classification learning. In R. Bajcsy (Ed.), Proceedings of the 13th International Joint \\nConference on \\nArtiJcial \\nIntelligence (pp. 1022-1027). Morgan-Kaufmann. \\nFayyad, \\nU. M., Weir, N., & Djorgovski, S. (1993). SKICAT: A machine learning system for auto- \\nmated cataloging of large scale sky surveys. Proceedings of the Tenth International Conference \\non Machine Learning (pp. 112-1 19). Amherst, MA: Morgan Kaufmann. \\nFisher, D. H., and McKusick, K. B. (1989). An empirical comparison of ID3 and back-propagation. \\nProceedings of the Eleventh International Joint Conference on A1 (pp. 788-793). Morgan \\nKaufmann. \\nFnedman, \\nJ. \\nH. (1977). A recursive partitioning decision rule for non-parametric classification. IEEE \\nTransactions on Computers @p. 404408). \\nHunt, E. B. (1975). Art$cial Intelligence. New Yorc Academic Press. \\nHunt, E. B., \\nMarin, \\nJ., & Stone, P. J. (1966). Experiments in Induction. New York: Academic Press. \\nKearns, \\nM., & Mansour, Y. (1996). On the boosting ability of top-down decision tree learning \\nalgorithms. Proceedings of the 28th ACM Symposium on the Theory of Computing. New York: \\nACM Press. \\nKononenko, I., Bratko, I., & Roskar, E. (1984). Experiments in automatic learning of medical diag- \\nnostic rules (Technical report). Jozef Stefan Institute, Ljubljana, Yugoslavia. \\nLopez de Mantaras, R. (1991). A distance-based attribute selection measure for decision tree induc- \\ntion. Machine Learning, 6(1), 81-92. \\nMalerba, D., \\nFloriana, \\nE., & Semeraro, G. (1995). A further comparison of simplification methods for \\ndecision tree. induction. In D. Fisher & H. Lenz (Eds.), Learningfrom data: AI and statistics. \\nSpringer-Verlag. \\nMehta, M., Rissanen, J., & Agrawal, R. (1995). MDL-based decision tree pruning. Proceedings of \\nthe First International Conference on Knowledge Discovery and Data Mining (pp. 216-221). \\nMenlo \\nPark, CA: AAAI Press. \\nMingers, \\nJ. \\n(1989a). \\nAn empirical comparison of selection measures for decision-tree induction. \\nMachine Learning, 3(4), 319-342. \\nMingers, \\nJ. \\n(1989b). An empirical comparison of pruning methods for decision-tree induction. \\nMachine Learning, 4(2), 227-243. \\nMurphy, P. M., & Pazzani, M. J. (1994). Exploring the decision forest: An empirical investigation \\nof Occam's razor in decision tree induction. Journal of Artijicial Intelligence Research, 1, \\n257-275. \\nMurthy, S. K., Kasif, S., & Salzberg, S. (1994). A system for induction of oblique decision trees. \\nJournal of Art$cial Intelligence Research, 2, 1-33. \\nNunez, M. (1991). The use of background knowledge in decision tree induction. Machine Learning, \\n6(3), 23 1-250. \\nPagallo, G., & Haussler, D. (1990). Boolean feature discovery in empirical learning. Machine Learn- \\ning, 5, 71-100. \\nQulnlan, \\nJ. \\nR. (1979). Discovering rules by induction from large collections of examples. In D. \\nMichie (Ed.), Expert systems in the micro electronic age. Edinburgh Univ. Press. \\nQulnlan, J. R. (1983). Learning efficient classification procedures and their application to chess end \\ngames. In R. S. Michalski, \\nJ. G. Carbonell, & T. M. Mitchell (Eds.), Machine learning: An \\nartificial \\nintelligence approach. San Matw, CA: Morgan Kaufmann. \\nQulnlan, \\nJ. \\nR. (1986). Induction of decision trees. Machine Learning, 1(1), 81-106. \\nQulnlan, \\nJ. R. (1987). Rule induction with statistical data-a comparison with multiple regression. \\nJournal of the Operational Research Society, 38,347-352. \\nQuinlan, \\nJ.R. (1988). An empirical comparison of genetic and decision-tree classifiers. Proceedings \\nof the \\nFifrh \\nInternational Machine Learning Conference (135-141). San Matw, CA: Morgan \\nKaufmann. \\nQuinlan, \\nJ.R. \\n(1988b). \\nDecision trees and multi-valued attributes. In Hayes, Michie, & Richards \\n(Eds.), Machine Intelligence 11, (pp. 305-318). Oxford, England: Oxford University Press. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 91}, page_content=\"80 MACHINE LEARNING \\nQuinlan, J. R., & Rivest, R. (1989). Information and Computation, (go), 227-248. \\nQuinlan, J. R. (1993). C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann. \\nRissanen, J. (1983). A universal prior for integers and estimation by minimum description length. \\nAnnals of Statistics 11 (2), 416-431. \\nRivest, R. L. (1987). Learning decision lists. Machine Learning, 2(3), 229-246. \\nSchaffer, C. (1993). Overfitting avoidance as bias. Machine Learning, 10, 113-152. \\nShavlik, J. W., Mooney, R. J., & Towell, G. G. (1991). Symbolic and neural learning algorithms: an \\nexperimental comparison. Machine kaming, 6(2), 11 1-144. \\nTan, M. (1993). Cost-sensitive learning of classification knowledge and its applications in robotics. \\nMachine Learning, 13(1), 1-33. \\nTan, M., & Schlimmer, J. C. (1990). Two case studies in cost-sensitive concept acquisition. Pro- \\nceedings of the AAAZ-90. \\nThrun, S. B. et al. (1991). The Monk's problems: A pe~ormance comparison of different learn- \\ning algorithms, (Technical report CMU-FS-91-197). Computer Science Department, Carnegie \\nMellon Univ., Pittsburgh, PA. \\nTurney, P. D. (1995). Cost-sensitive classification: empirical evaluation of a hybrid genetic decision \\ntree induction algorithm. Journal of A1 Research, 2, 369409. \\nUtgoff, P. E. (1989). Incremental induction of decision trees. Machine Learning, 4(2), 161-186. \\nUtgoff, P. E., & Brodley, C. E. (1991). Linear machine decision trees, (COINS Technical Report \\n91-10). University of Massachusetts, \\nAmherst, \\nMA. \\nWeiss, S., & Kapouleas, I. (1989). An empirical comparison of pattern recognition, neural nets, \\nand machine learning classification methods. Proceedings of the Eleventh IJCAI, (781-787), \\nMorgan Kaufmann. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 92}, page_content='CHAPTER \\nARTIFICIAL \\nNEURAL \\nNETWORKS \\nArtificial neural networks (ANNs) provide a general, practical method for learning \\nreal-valued, discrete-valued, and vector-valued functions from examples. Algorithms \\nsuch as \\nBACKPROPAGATION \\nuse gradient descent to tune network parameters to best \\nfit a training set of input-output pairs. ANN learning is robust to errors in the training \\ndata and has been successfully applied to problems such as interpreting visual scenes, \\nspeech recognition, and learning robot control strategies. \\n4.1 INTRODUCTION \\nNeural network learning methods provide a robust approach to approximating \\nreal-valued, discrete-valued, and vector-valued target functions. For certain types \\nof problems, such as learning to interpret complex real-world sensor data, artificial \\nneural networks are among the most effective learning methods currently known. \\nFor \\nexample, the \\nBACKPROPAGATION \\nalgorithm described in this chapter has proven \\nsurprisingly successful in many practical problems such as learning to recognize \\nhandwritten characters \\n(LeCun \\net al. \\n1989), \\nlearning to recognize spoken words \\n(Lang \\net \\nal. \\n1990), \\nand learning to recognize faces (Cottrell 1990). One survey of \\npractical applications is provided by Rumelhart et al. (1994). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 93}, page_content=\"4.1.1 Biological Motivation \\nThe study of artificial neural networks (ANNs) has been inspired in part by the \\nobservation that biological learning systems are built of very complex webs of \\ninterconnected neurons. In rough analogy, artificial neural networks are built out \\nof a densely interconnected set of simple units, where each unit takes a number \\nof real-valued inputs (possibly the outputs of other units) and produces a single \\nreal-valued output (which may become the input to many other units). \\nTo develop a feel for this analogy, let us consider a few facts from neuro- \\nbiology. The human brain, for example, is estimated to contain a densely inter- \\nconnected network of approximately \\n1011 \\nneurons, each connected, on average, to \\nlo4 others. Neuron activity is typically excited or inhibited through connections to \\nother neurons. The fastest neuron switching times are known to be on the order of \\nloe3 \\nseconds--quite slow compared to computer switching speeds of 10-lo sec- \\nonds. Yet humans are able to make surprisingly complex decisions, surprisingly \\nquickly. For example, it requires approximately lo-' seconds to visually recognize \\nyour mother. Notice the sequence of neuron firings that can take place during this \\n10-'-second interval cannot possibly be longer than a few hundred steps, given \\nthe switching speed of single neurons. This observation has led many to speculate \\nthat the information-processing abilities of biological neural systems must follow \\nfrom highly parallel processes operating on representations that are distributed \\nover many neurons. One motivation for ANN systems is to capture this kind \\nof highly parallel computation based on distributed representations. Most ANN \\nsoftware runs on sequential machines emulating distributed processes, although \\nfaster versions of the algorithms have also been implemented on highly parallel \\nmachines and on specialized hardware designed specifically for ANN applications. \\nWhile ANNs are loosely motivated by biological neural systems, there are \\nmany complexities to biological neural systems that are not modeled by ANNs, \\nand many features of the ANNs we discuss here are known to be inconsistent \\nwith biological systems. For example, we consider here ANNs whose individual \\nunits output a single constant value, whereas biological neurons output a complex \\ntime series of spikes. \\nHistorically, two groups of researchers have worked with artificial neural \\nnetworks. One group has been motivated by the goal of using ANNs to study \\nand model biological learning processes. A second group has been motivated by \\nthe goal of obtaining highly effective machine learning algorithms, independent of \\nwhether these algorithms mirror biological processes. Within this book our interest \\nfits the latter group, and therefore we will not dwell further on biological modeling. \\nFor more information on attempts to model biological systems using ANNs, see, \\nfor example, Churchland and Sejnowski (1992); Zornetzer et al. (1994); Gabriel \\nand Moore (1990). \\n4.2 NEURAL NETWORK REPRESENTATIONS \\nA prototypical example of ANN learning is provided by Pomerleau's (1993) sys- \\ntem ALVINN, which uses a learned ANN to steer an autonomous vehicle driving \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 94}, page_content='at normal speeds on public highways. The input to the neural network is a 30 x 32 \\ngrid of pixel intensities obtained from a forward-pointed camera mounted on the \\nvehicle. The network output is the direction in which the vehicle is steered. The \\nANN is trained to mimic the observed steering commands of a human driving the \\nvehicle for approximately 5 minutes. ALVINN has used its learned networks to \\nsuccessfully drive at speeds up to 70 miles per hour and for distances of 90 miles \\non \\npublic highways (driving in the left lane of a divided public highway, with \\nother vehicles present). \\nFigure 4.1 illustrates the neural network representation used in one version \\nof the \\nALVINN \\nsystem, and illustrates the kind of representation typical of many \\nANN systems. The network is shown on the left side of the figure, with the input \\ncamera image depicted below it. Each node \\n(i.e., \\ncircle) in the network diagram \\ncorresponds to the output of a single network unit, and the lines entering the node \\nfrom \\nbelow are its inputs. As can be seen, there are four units that receive inputs \\ndirectly from all of the 30 x 32 pixels in the image. These are called \"hidden\" \\nunits because their output is available only within the network and is not available \\nas part of the global network output. Each of these four hidden units computes a \\nsingle real-valued output based on a weighted combination of its 960 inputs. These \\nhidden unit outputs are then used as inputs to a second layer of 30 \"output\" units. \\nEach output unit corresponds to a particular steering direction, and the output \\nvalues of these units determine which steering direction is recommended most \\nstrongly. \\nThe diagrams on the right side of the figure depict the learned weight values \\nassociated with one of the four hidden units in this ANN. The large matrix of \\nblack and white boxes on the lower right depicts the weights from the 30 x 32 pixel \\ninputs into the hidden unit. Here, a white box indicates a positive weight, a black \\nbox a negative weight, and the size of the box indicates the weight magnitude. \\nThe smaller rectangular diagram directly above the large matrix shows the weights \\nfrom this hidden unit to each of the 30 output units. \\nThe network structure of ALYINN is typical of many ANNs. Here the in- \\ndividual units are interconnected in layers that form a directed acyclic graph. In \\ngeneral, \\nANNs \\ncan be graphs with many types of structures-acyclic or cyclic, \\ndirected or undirected. This chapter will focus on the most common and practical \\nANN \\napproaches, which are based on the \\nBACKPROPAGATION \\nalgorithm. The \\nBACK- \\nPROPAGATION algorithm assumes the network is a fixed structure that corresponds \\nto \\na directed graph, possibly containing cycles. Learning corresponds to choosing \\na \\nweight value for each edge in the graph. \\nAlthough \\ncertain types of cycles are \\nallowed, \\nthe vast majority of practical applications involve acyclic feed-forward \\nnetworks, similar to the network structure used by ALVINN. \\n4.3 APPROPRIATE PROBLEMS FOR NEURAL NETWORK \\nLEARNING \\nANN learning is well-suited to problems in which the training data corresponds \\nto \\nnoisy, complex sensor data, such as inputs from cameras and microphones. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 95}, page_content=\"E2' \\nStraight \\nAhead \\n1 1 1 30 Output \\nUnits \\nn \\n30x32 Sensor \\nInput Retina \\n1 \\nFIGURE 4.1 \\nNeural network learning to steer an autonomous vehicle. The ALVINN system uses BACKPROPAGA- \\nTION to learn to steer an autonomous vehicle (photo at top) driving at speeds up to 70 miles per hour. \\nThe diagram on the left shows how the image of a forward-mounted camera is mapped to 960 neural \\nnetwork inputs, which are fed forward to 4 hidden units, connected to 30 output units. Network \\noutputs encode the commanded steering direction. The figure on the right shows weight values for \\none of the hidden units in this network. The 30 x 32 weights into the hidden unit are displayed in \\nthe large matrix, with white blocks indicating positive and black indicating negative weights. The \\nweights from this hidden unit to the 30 output units are depicted by the smaller rectangular block \\ndirectly above the large block. As can be seen from these output weights, activation of this particular \\nhidden unit encourages a turn toward the left. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 96}, page_content='~t is also applicable to problems for which more symbolic representations are \\noften used, such as the decision tree learning tasks discussed in Chapter 3. In \\nthese cases ANN and decision tree learning often produce results of comparable \\naccuracy. See Shavlik et al. (1991) and Weiss and Kapouleas (1989) for exper- \\nimental comparisons of decision tree and ANN learning. The BACKPROPAGATION \\nalgorithm is the most commonly used ANN learning technique. It is appropriate \\nfor \\nproblems with the following characteristics: \\n0 Instances are represented by many attribute-value pairs. The target function \\nto be learned is defined over instances that can be described by a vector of \\npredefined features, such as the pixel values in the ALVINN example. These \\ninput attributes may be highly correlated or independent of one another. \\nInput values can be any real values. \\nThe target function output may be discrete-valued, real-valued, or a vector \\nof several real- or discrete-valued attributes. For example, in the ALVINN \\nsystem the output is a vector of 30 attributes, each corresponding to a rec- \\nommendation regarding the steering direction. The value of each output is \\nsome real number between 0 and 1, which in this case corresponds to the \\nconfidence in predicting the corresponding steering direction. We can also \\ntrain a single network to output both the steering command and suggested \\nacceleration, simply by concatenating the vectors that encode these two out- \\nput predictions. \\nThe training examples may contain errors. ANN learning methods are quite \\nrobust to noise in the training data. \\nLong training times are acceptable. Network training algorithms typically \\nrequire longer training times than, say, decision tree learning algorithms. \\nTraining times can range from a few seconds to many hours, depending \\non factors such as the number of weights in the network, the number of \\ntraining examples considered, and the settings of various learning algorithm \\nparameters. \\nFast evaluation of the learned target function may be required. Although \\nANN learning times are relatively long, evaluating the learned network, in \\norder to apply it to a subsequent instance, is typically very fast. For example, \\nALVINN applies its neural network several times per second to continually \\nupdate its steering command as the vehicle drives forward. \\nI \\nThe ability of humans to understand the learned target function is not impor- \\ntant. The weights learned by neural networks are often difficult for humans to \\ninterpret. Learned neural networks are less easily communicated to humans \\nthan learned rules. \\nThe rest of this chapter is organized as follows: We first consider several \\nalternative designs for the primitive units that make up artificial neural networks \\n(perce~trons, \\nlinear units, and sigmoid units), \\nalong \\nwith learning algorithms for \\ntraining single units. We then present the \\nBACKPROPAGATION \\nalgorithm for training '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 97}, page_content='multilayer networks of such units and consider several general issues such as the \\nrepresentational capabilities of \\nANNs, \\nnature of the hypothesis space search, \\nover- \\nfitting problems, and alternatives to the \\nBACKPROPAGATION \\nalgorithm. A detailed \\nexample is also presented applying \\nBACKPROPAGATION \\nto face recognition, and \\ndirections are provided for the reader to obtain the data and code to experiment \\nfurther with this application. \\n4.4 PERCEPTRONS \\nOne type of ANN system is based on a unit called a perceptron, illustrated in \\nFigure 4.2. A perceptron takes a vector of real-valued inputs, calculates a linear \\ncombination of these inputs, then outputs a 1 if the result is greater than some \\nthreshold and -1 otherwise. More precisely, given inputs xl through x,, the output \\no(x1, . . . , x,) computed by the perceptron is \\no(x1,. ..,x,) = 1 if wo + wlxl+ ~2x2 + - . + W,X, > 0 \\n-1 otherwise \\nwhere each wi is a real-valued constant, or weight, that determines the contribution \\nof input xi to the perceptron output. Notice the quantity (-wO) is a threshold that \\nthe weighted combination of inputs wlxl + . . . + wnxn must surpass in order for \\nthe perceptron to output a 1. \\nTo simplify notation, we imagine an additional constant input xo = 1, al- \\nlowing us to write the above inequality as C:=o wixi > 0, or in vector form as \\niir ..i! > 0. For brevity, we will sometimes write the perceptron function as \\nwhere \\nLearning a perceptron involves choosing values for the weights wo, . . . , w,. \\nTherefore, the space H of candidate hypotheses considered in perceptron learning \\nis the set of all possible real-valued weight vectors. \\n4.4.1 Representational Power of Perceptrons \\nWe can view the perceptron as representing a hyperplane decision surface in the \\nn-dimensional space of instances \\n(i.e., \\npoints). The perceptron outputs a 1 for \\ninstances lying on one side of the hyperplane and outputs a -1 for instances \\nlying on the other side, as illustrated in Figure 4.3. The equation for this decision \\nhyperplane is iir . .i! = 0. Of course, some sets of positive and negative examples \\ncannot be separated by any hyperplane. Those that can be separated are called \\nlinearly separable sets of examples. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 98}, page_content='FIGURE 43 \\nA perceptron. \\nA single perceptron can be used to represent many boolean functions. For \\nexample, if we assume boolean values of 1 (true) and -1 (false), then one way to \\nuse a two-input perceptron to implement the AND function is to set the weights \\nwo = -3, and wl = wz = .5. This perceptron can be made to represent the OR \\nfunction instead by altering the threshold to wo = -.3. In fact, AND and OR can \\nbe viewed as special cases of m-of-n functions: that is, functions where at least \\nm of the n inputs to the perceptron must be true. The OR function corresponds to \\nrn = 1 and the AND function to m = n. Any m-of-n function is easily represented \\nusing a perceptron by setting all input weights to the same value \\n(e.g., 0.5) and \\nthen setting the threshold wo accordingly. \\nPerceptrons can represent all of the primitive boolean functions AND, OR, \\nNAND (1 AND), and NOR (1 OR). Unfortunately, however, some boolean func- \\ntions cannot be represented by a single perceptron, such as the XOR function \\nwhose value is 1 if and only if xl # xz. Note the set of linearly nonseparable \\ntraining examples shown in Figure \\n4.3(b) \\ncorresponds to this XOR function. \\nThe ability of perceptrons to represent AND, OR, NAND, and NOR is \\nimportant because every boolean function can be represented by some network of \\ninterconnected units based on these primitives. In fact, every boolean function can \\nbe represented by some network of perceptrons only two levels deep, in which \\nFIGURE 4.3 \\nThe decision surface represented by a two-input perceptron. (a) A set of training examples and the \\ndecision surface of a perceptron that classifies them correctly. (b) A set of training examples that is \\nnot \\nlinearly separable \\n(i.e., \\nthat cannot be correctly classified by any straight line). xl and x2 are the \\nPerceptron \\ninputs. Positive examples are indicated by \"+\", negative by \"-\". '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 99}, page_content='the inputs are fed to multiple units, and the outputs of these units are then input to \\na second, final stage. One way is to represent the boolean function in disjunctive \\nnormal form \\n(i.e., \\nas the disjunction (OR) of a set of conjunctions \\n(ANDs) \\nof \\nthe inputs and their negations). Note that the input to an AND perceptron can be \\nnegated simply by changing the sign of the corresponding input weight. \\nBecause networks of threshold units can represent a rich variety of functions \\nand because single units alone cannot, we will generally be interested in learning \\nmultilayer networks of threshold units. \\n4.4.2 The Perceptron Training Rule \\nAlthough we are interested in learning networks of many interconnected units, let \\nus begin by understanding how to learn the weights for a single perceptron. Here \\nthe precise learning problem is to determine a weight vector that causes the per- \\nceptron to produce the correct f 1 output for each of the given training examples. \\nSeveral algorithms are known to solve this learning problem. Here we con- \\nsider two: the perceptron rule and the delta rule (a variant of the LMS rule used \\nin Chapter 1 for learning evaluation functions). These two algorithms are guaran- \\nteed to converge to somewhat different acceptable hypotheses, under somewhat \\ndifferent conditions. They are important to \\nANNs \\nbecause they provide the basis \\nfor learning networks of many units. \\nOne way to learn an acceptable weight vector is to begin with random \\nweights, then iteratively apply the perceptron to each training example, modify- \\ning the perceptron weights whenever it misclassifies an example. This process is \\nrepeated, iterating through the training examples as many times as needed until \\nthe perceptron classifies all training examples correctly. Weights are modified at \\neach step according to the perceptron training rule, which revises the weight wi \\nassociated with input xi according to the rule \\nwhere \\nHere t is the target output for the current training example, o is the output generated \\nby the perceptron, and q is a positive constant called the learning rate. The role \\nof the learning rate is to moderate the degree to which weights are changed at \\neach step. It is usually set to some small value \\n(e.g., \\n0.1) and is sometimes made \\nto decay as the number of weight-tuning iterations increases. \\nWhy should this update rule converge toward successful weight values? To \\nget an intuitive feel, consider some specific cases. Suppose the training example is \\ncorrectly classified already by the perceptron. In this case, (t - o) is zero, making \\nAwi zero, so that no weights are updated. Suppose the perceptron outputs a -1, \\nwhen the target output is + 1. To make the perceptron output a + 1 instead of - 1 in \\nthis case, the weights must be altered to increase the value of G.2. For example, if \\nxi r 0, then increasing wi will bring the perceptron closer to correctly classifying '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 100}, page_content='this example. Notice the training rule will increase w, in this case, because (t - o), \\n7, and Xi are all positive. For example, if xi = .8, q = 0.1, t = 1, and o = - 1, \\nthen the weight update will be Awi = q(t - o)xi = O.1(1 - (-1))0.8 = 0.16. On \\nthe other hand, if t = -1 and o = 1, then weights associated with positive xi will \\nbe \\ndecreased rather than increased. \\nIn fact, the above learning procedure can be proven to converge within a \\nfinite number of applications of the perceptron training rule to a weight vec- \\ntor that correctly classifies all training examples, provided the training examples \\nare linearly separable and provided a sufficiently small 7 is used (see Minsky \\nand \\nPapert 1969). If the data are not linearly separable, convergence is not as- \\nsured. \\n4.4.3 Gradient Descent and the Delta Rule \\nAlthough the perceptron rule finds a successful weight vector when the training \\nexamples are linearly separable, it can fail to converge if the examples are not \\nlinearly separable. A second training rule, called the delta rule, is designed to \\novercome this difficulty. If the training examples are not linearly separable, the \\ndelta rule converges toward a best-fit approximation to the target concept. \\nThe key idea behind the delta rule is to use gradient descent to search the hy- \\npothesis space of possible weight vectors to find the weights that best fit the train- \\ning examples. This rule is important because gradient descent provides the basis \\nfor the \\nBACKPROPAGATION \\nalgorithm, which can learn networks with many inter- \\nconnected units. It is also important because gradient descent can serve as the \\nbasis for learning algorithms that must search through hypothesis spaces contain- \\ning many different types of continuously parameterized hypotheses. \\nThe delta training rule is best understood by considering the task of training \\nan unthresholded perceptron; that is, a linear unit for which the output o is given by \\nThus, a linear unit corresponds to the first stage of a perceptron, without the \\nthreshold. \\nIn order to derive a weight learning rule for linear units, let us begin by \\nspecifying a measure for the training error of a hypothesis (weight vector), relative \\nto \\nthe training examples. Although there are many ways to define this error, one \\ncommon measure that will turn out to be especially convenient is \\nwhere D is the set of training examples, td is the target output for training example \\nd, and od is the output of the linear unit for training example d. By this definition, \\nE(6) is simply half the squared difference between the target output td and the \\nhear \\nunit output od, summed over all training examples. Here we characterize \\nE as a function of 27, because the linear unit output o depends on this weight \\nvector. \\nOf \\ncourse E also depends on the particular set of training examples, but '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 101}, page_content='we assume these are fixed during training, so we do not bother to write E as an \\nexplicit function of these. Chapter 6 provides a Bayesian justification for choosing \\nthis particular definition of E. In particular, there we show that under certain \\nconditions the hypothesis that minimizes E is also the most probable hypothesis \\nin H given the training data. \\n4.4.3.1 VISUALIZING THE HYPOTHESIS SPACE \\nTo understand the gradient descent algorithm, it is helpful to visualize the entire \\nhypothesis space of possible weight vectors and their associated E values, as \\nillustrated in Figure 4.4. Here the axes wo and wl represent possible values for \\nthe two weights of a simple linear unit. The wo, wl plane therefore represents \\nthe entire hypothesis space. The vertical axis indicates the error E relative to \\nsome fixed set of training examples. The error surface shown in the figure thus \\nsummarizes the desirability of every weight vector in the hypothesis space (we \\ndesire a hypothesis with minimum error). Given the way in which we chose to \\ndefine E, for linear units this error surface must always be parabolic with a single \\nglobal minimum. The specific parabola will depend, of course, on the particular \\nset of training examples. \\nFIGURE 4.4 \\nError of different hypotheses. For a linear unit with two weights, the hypothesis space H is the \\nwg, wl plane. The vertical axis indicates tk error of the corresponding weight vector hypothesis, \\nrelative to a fixed set of training examples. The arrow shows the negated gradient at one partic- \\nular point, indicating the direction in the wo, wl plane producing steepest descent along the error \\nsurface. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 102}, page_content='Gradient descent search determines a weight vector that minimizes E by \\nstarting with an arbitrary initial weight vector, then repeatedly modifying it in \\nsmall steps. At each step, the weight vector is altered in the direction that produces \\nthe \\nsteepest descent along the error surface depicted in Figure 4.4. This process \\ncontinues \\nuntil the global minimum error is reached. \\n4.4.3.2 DERIVATION OF THE GRADIENT DESCENT RULE \\nHow can we calculate the direction of steepest descent along the error surface? \\nThis direction can be found by computing the derivative of E with respect to each \\ncomponent of the vector 2. This vector derivative is called the gradient of E with \\nrespect to 221, written ~~(iir). \\nNotice VE(221) is itself a vector, whose components are the partial derivatives \\nof E with respect to each of the wi. When interpreted as a vector in weight \\nspace, the gradient \\nspecijies \\nthe direction that produces the steepest increase in \\nE. The negative of this vector therefore gives the direction of steepest decrease. \\nFor example, the arrow in Figure 4.4 shows the negated gradient -VE(G) for a \\nparticular point in the wo, wl plane. \\nSince the gradient specifies the direction of steepest increase of E, the train- \\ning rule for gradient descent is \\nwhere \\nHere r] is a positive constant called the learning rate, which determines the step \\nsize in the gradient descent search. The negative sign is present because we want \\nto \\nmove the weight vector in the direction that decreases E. This training rule \\ncan also be written in its component form \\nwhere \\nwhich makes it clear that steepest descent is achieved by altering each component \\nw, of ii in proportion to E. \\nTo construct a practical algorithm for iteratively updating weights according \\nto \\nEquation (44, we need an efficient way of calculating the gradient at each \\nstep. Fortunately, this is not difficult. The vector of derivatives that form the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 103}, page_content='gradient can be obtained by differentiating E from Equation (4.2), as \\nwhere \\nxid \\ndenotes the single input component \\nxi \\nfor training example d. We now \\nhave an equation that gives in terms of the linear unit inputs \\nxid, \\noutputs \\nOd, \\nand target values \\ntd \\nassociated with the training examples. Substituting Equa- \\ntion (4.6) into Equation (4.5) yields the weight update rule for gradient descent \\nTo summarize, the gradient descent algorithm for training linear units is as \\nfollows: Pick an initial random weight vector. Apply the linear unit to all training \\nexamples, then compute \\nAwi \\nfor each weight according to Equation (4.7). Update \\neach weight wi by adding Awi, then repeat this process. This algorithm is given \\nin Table 4.1. Because the error surface contains only a single global minimum, \\nthis algorithm will converge to a weight vector with minimum error, regardless \\nof whether the training examples are linearly separable, given a sufficiently small \\nlearning rate q is used. If r) is too large, the gradient descent search runs the risk \\nof overstepping the minimum in the error surface rather than settling into it. For \\nthis reason, one common modification to the algorithm is to gradually reduce the \\nvalue of r) as the number of gradient descent steps grows. \\n4.4.3.3 STOCHASTIC APPROXIMATION TO GRADIENT DESCENT \\nGradient descent is an important general paradigm for learning. It is a strategy for \\nsearching through a large or infinite hypothesis space that can be applied whenever \\n(1) the hypothesis space contains continuously parameterized hypotheses \\n(e.g., \\nthe \\nweights in a linear unit), and (2) the error can be differentiated with respect to \\nthese hypothesis parameters. The key practical difficulties in applying gradient \\ndescent are (1) converging to a local minimum can sometimes be quite slow (i.e., \\nit can require many thousands of gradient descent steps), and (2) if there are \\nmultiple local minima in the error surface, then there is no guarantee that the \\nprocedure will find the global minimum. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 104}, page_content=\"CHAF'l'ER 4 ARTIFICIAL NEURAL NETWORKS 93 \\n- - \\n~~ADIENT-DEscENT(~~~~~~~~~x~~~~~s, q) \\nEach training example is a pair of the \\nform (2, t), where x' is the vector of input values, and \\nt is the target output value. q is the learning rate (e.g., .05). \\n. Initialize each w, to some small random value \\n. Until the termination condition is met, Do \\n0 Initialize each Awi to zero. \\n0 For each (2, t) in trainingaxamples, Do \\nw Input the instance x' to the unit and compute the output o \\nFor each linear unit weight w,, Do \\nFor each linear unit weight wi, Do \\nTABLE 4.1 \\nGRADIENT DESCENT algorithm for training a linear unit. To implement the stochastic approximation \\nto gradient descent, Equation (T4.2) is deleted, and Equation (T4.1) replaced by wi c wi +q(t -obi. \\nOne common variation on gradient descent intended to alleviate these diffi- \\nculties is called incremental gradient descent, or alternatively stochastic gradient \\ndescent. Whereas the gradient descent training rule presented in Equation (4.7) \\ncomputes weight updates after summing over a22 the training examples in D, the \\nidea behind stochastic gradient descent is to approximate this gradient descent \\nsearch by updating weights incrementally, following the calculation of the error \\nfor each individual example. The modified training rule is like the training rule \\ngiven by Equation (4.7) except that as we iterate through each training example \\nwe update the weight according to \\nwhere t, o, and xi are the target value, unit output, and ith input for the training \\nexample in question. To modify the gradient descent algorithm of Table 4.1 to \\nimplement this stochastic approximation, Equation (T4.2) is simply deleted and \\nEquation (T4.1) replaced by wi t wi + v (t - o) xi. One way to view this stochastic \\ngradient descent is to consider a distinct error function ~~(6) defined for each \\nindividual training example d as follows \\n1 Ed (6) = - (td - 0d) 2 \\n2 (4.11) \\nwhere t, and od are the target value and the unit output value for training ex- \\nample d. Stochastic gradient descent iterates over the training examples d in D, \\nat each iteration altering the weights according to the gradient with respect to \\nEd(;). The sequence of these weight updates, when iterated over all training \\nexamples, provides a reasonable approximation to descending the gradient with \\nrespect to our original error function E(G). By making the value of 7 (the gradient \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 105}, page_content=\"94 MACHINE LEARNING \\ndescent step size) sufficiently small, stochastic gradient descent can be made to \\napproximate true gradient descent arbitrarily closely. The key differences between \\nstandard gradient descent and stochastic gradient descent are: \\n0 In standard gradient descent, the error is summed over all examples before \\nupdating weights, whereas in stochastic gradient descent weights are updated \\nupon examining each training example. \\n. Summing over multiple examples in standard gradient descent requires more \\ncomputation per weight update step. On the other hand, because it uses the \\ntrue gradient, standard gradient descent is often used with a larger step size \\nper weight update than stochastic gradient descent. \\nr, In cases where there are multiple local minima with respect to E($, stochas- \\ntic gradient descent can sometimes avoid falling into these local minima \\nbecause it uses the various VEd(G) rather than VE(6) to guide its search. \\nBoth stochastic and standard gradient descent methods are commonly used in \\npractice. \\nThe training rule in Equation (4.10) is known as the delta rule, or sometimes \\nthe LMS (least-mean-square) rule, \\nAdaline \\nrule, or Widrow-Hoff rule (after its \\ninventors). In Chapter 1 we referred to it as the LMS weight-update rule when \\ndescribing its use for learning an evaluation function for game playing. Notice \\nthe delta rule in Equation (4.10) is similar to the perceptron training rule in \\nEquation (4.4.2). In fact, the two expressions appear to be identical. However, \\nthe rules are different because in the delta rule o refers to the linear unit output \\no(2) = i;) .?, whereas for the perceptron rule o refers to the thresholded output \\no(2) = sgn($ .2). \\nAlthough we have presented the delta rule as a method for learning weights \\nfor unthresholded linear units, it can easily be used to train thresholded perceptron \\nunits, as well. Suppose that o = i;) . x' is the unthresholded linear unit output as \\nabove, and of = sgn(G.2) is the result of thresholding o as in the perceptron. Now \\nif we wish to train a perceptron to fit training examples with target values \\noff 1 for \\no', we can use these same target values and examples to train o instead, using the \\ndelta rule. Clearly, if the unthresholded output o can be trained to fit these values \\nperfectly, then the threshold output of will fit them as well (because sgn(1) = 1, \\nand sgn(-1) = -1). Even when the target values cannot be fit perfectly, the \\nthresholded of value will correctly fit the f 1 target value whenever the linear \\nunit output o has the correct sign. Notice, however, that while this procedure will \\nlearn weights that minimize the error in the linear unit output o, these weights \\nwill not necessarily minimize the number of training examples misclassified by \\nthe thresholded output 0'. \\n4.4.4 Remarks \\nWe have considered two similar algorithms for iteratively learning perceptron \\nweights. The key difference between these algorithms is that the perceptron train- \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 106}, page_content='CHmR 4 ARTIFICIAL NEURAL NETWORKS 95 \\ning rule updates weights based on the error in the thresholded perceptron output, \\nwhereas the delta rule updates weights based on the error in the unthresholded \\nlinear combination of inputs. \\nThe difference between these two training rules is reflected in different con- \\nvergence properties. The perceptron training rule converges after a finite number \\nof iterations to a hypothesis that perfectly classifies the training data, provided the \\ntraining examples are linearly separable. The delta rule converges only asymp- \\ntotically toward the minimum error hypothesis, possibly requiring unbounded \\ntime, but converges regardless of whether the training data are linearly sepa- \\nrable. A detailed presentation of the convergence proofs can be found in Hertz et \\nal. (1991). \\nA third possible algorithm for learning the weight vector is linear program- \\nming. Linear programming is a general, efficient method for solving sets of linear \\ninequalities. Notice each training example corresponds to an inequality of the \\nform zZI - x\\' > 0 or G . x\\' 5 0, and their solution is the desired weight vector. Un- \\nfortunately, this approach yields a solution only when the training examples are \\nlinearly separable; however, Duda and Hart (1973, p. 168) suggest a more subtle \\nformulation that accommodates the nonseparable case. In any case, the approach \\nof linear programming does not scale to training multilayer networks, which is \\nour primary concern. In contrast, the gradient descent approach, on which the \\ndelta rule is based, can be easily extended to multilayer networks, as shown in \\nthe following section. \\n4.5 MULTILAYER NETWORKS AND THE BACKPROPAGATION \\nALGORITHM \\nAs noted in Section 4.4.1, single perceptrons can only express linear decision \\nsurfaces. In contrast, the kind of multilayer networks learned by the BACKPROPA- \\nCATION algorithm are capable of expressing a rich variety of nonlinear decision \\nsurfaces. For example, a typical multilayer network and decision surface is de- \\npicted in Figure 4.5. Here the speech recognition task involves distinguishing \\namong 10 possible vowels, all spoken in the context of \\n\"h-d\" \\n(i.e., \\n\"hid,\" \"had,\" \\n\"head,\" \"hood,\" etc.). The input speech signal is represented by two numerical \\nparameters obtained from a spectral analysis of the sound, allowing us to easily \\nvisualize the decision surface over the two-dimensional instance space. As shown \\nin the figure, it is possible for the multilayer network to represent highly nonlinear \\ndecision surfaces that are much more expressive than the linear decision surfaces \\nof single units shown earlier in Figure 4.3. \\nThis section discusses how to learn such multilayer networks using a gradient \\ndescent algorithm similar to that discussed in the previous section. \\n4.5.1 A Differentiable Threshold Unit \\nWhat type of unit shall we use as the basis for constructing multilayer networks? \\nAt first we might be tempted to choose the linear units discussed in the previous '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 107}, page_content='head hid 4 who\\'d hood \\n0 bad \\n. hid \\n+ hod \\nr had \\nr hawed \\n. hoard \\no \\nheed \\nc hud \\n, who\\'d \\nhood \\nFIGURE 4.5 \\nDecision regions of a multilayer feedforward network. The network shown here was trained to \\nrecognize 1 of 10 vowel sounds occurring in the context \"hd\" (e.g., \"had,\" \"hid\"). The network \\ninput consists of two \\nparameters, F1 and F2, obtained from a spectral analysis of the sound. The \\n10 network outputs correspond to the 10 possible vowel sounds. The network prediction is the \\noutput whose \\nvalue \\nis highest. The plot on the right illustrates the highly nonlinear decision surface \\nrepresented by the learned network. Points shown on the plot are test examples distinct from the \\nexamples used to train the network. (Reprinted by permission from Haung and Lippmann (1988).) \\nsection, for which we have already derived a gradient descent learning rule. How- \\never, multiple layers of cascaded linear units still produce only linear functions, \\nand we prefer networks capable of representing highly nonlinear functions. The \\nperceptron unit is another possible choice, but its discontinuous threshold makes \\nit undifferentiable and hence unsuitable for gradient descent. What we need is a \\nunit whose output is a nonlinear function of its inputs, but whose output is also \\na differentiable function of its inputs. One solution is the sigmoid unit-a unit \\nvery much like a perceptron, but based on a smoothed, differentiable threshold \\nfunction. \\nThe sigmoid unit is illustrated in Figure 4.6. Like the perceptron, the sigmoid \\nunit first computes a linear combination of its inputs, then applies a threshold to \\nthe result. In the case of the sigmoid unit, however, the threshold output is a \\nnet = C wi xi 1 o = @net) = - \\n1 + kMf \\nFIGURE 4.6 \\nThe sigmoid threshold unit. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 108}, page_content=\"CHAPTER 4 ARTIFICIAL NEURAL NETWORKS 97 \\ncontinuous function of its input. More precisely, the sigmoid unit computes its \\noutput o as \\nwhere \\na is often called the sigmoid function or, alternatively, the logistic function. Note \\nits \\noutput ranges between 0 and 1, increasing monotonically with its input (see the \\nthreshold function plot in Figure 4.6.). Because it maps a very large input domain \\nto a small range of outputs, it is often referred to as the squashingfunction of \\nthe unit. The sigmoid function has the useful property that its derivative is easily \\nexpressed in terms of its output [in particular, = \\ndy O(Y) . (1 - dy))]. As \\nwe shall see, the gradient descent learning rule makes use of this derivative. \\nOther differentiable functions with easily calculated derivatives are sometimes \\nused in place of a. For example, the term e-y in the sigmoid function definition \\nis sometimes replaced by e-k'y where k is some positive constant that determines \\nthe steepness of the threshold. The function tanh is also sometimes used in place \\nof the sigmoid function (see Exercise 4.8). \\n4.5.2 The BACKPROPAGATION Algorithm \\nThe BACKPROPAGATION algorithm learns the weights for a multilayer network, \\ngiven a network with a fixed set of units and interconnections. It employs gradi- \\nent descent to attempt to minimize the squared error between the network output \\nvalues and the target values for these outputs. This section presents the \\nBACKPROP- \\nAGATION algorithm, and the following section gives the derivation for the gradient \\ndescent weight update rule used by BACKPROPAGATION. \\nBecause we are considering networks with multiple output units rather than \\nsingle units as before, we begin by redefining E to sum the errors over all of the \\nnetwork output units \\nwhere outputs is the set of output units in the network, and tkd and OM are the \\nI target and output values associated with the kth output unit and training example d. \\nThe learning problem faced by BACKPROPAGATION is to search a large hypoth- \\nesis space defined by all possible weight values for all the units in the network. \\nThe situation can be visualized in terms of an error surface similar to that shown \\nfor linear units in Figure 4.4. The error in that diagram is replaced by our new \\ndefinition of E, and the other dimensions of the space correspond now to all of \\nthe weights associated with all of the units in the network. As in the case of \\ntraining a single unit, gradient descent can be used to attempt to find a hypothesis \\nto \\nminimize E. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 109}, page_content=\"B~c~~~o~~GATIO~(trainingaxamp~es, q, ni, , no,, , nhidden) \\nEach training example is a pair of the form (2, i ), where x' is the vector of network input \\nvalues, and is the vector of target network output values. \\nq is the learning rate (e.g., .O5). ni, is the number of network inputs, nhidden the number of \\nunits in the hidden layer, and \\nno,, \\nthe number of output units. \\nThe \\ninputfiom \\nunit i into unit j is denoted xji, and the weight from unit i to unit j is denoted \\nwji. \\na Create a feed-forward network with ni, inputs, midden hidden units, and nour output units. \\na Initialize all network weights to small random numbers (e.g., between -.05 and .05). \\nr Until the termination condition is met, Do \\na For each (2, i ) in trainingaxamples, Do \\nPropagate the input forward through the network: \\n1, Input the instance x' to the network and compute the output o, of every unit u in \\nthe network. \\nPropagate the errors backward through the network: \\n2. For each network output unit k, calculate its error term Sk \\n6k 4- ok(l - ok)(tk - 0k) \\n3. For each hidden unit h, calculate its error term 6h \\n4. Update each network weight wji \\nwhere \\nAw.. - Jl - I 11 \\nTABLE 4.2 \\nThe stochastic gradient descent version of the BACKPROPAGATION algorithm for feedforward networks \\ncontaining two layers of sigmoid units. \\nOne major difference in the case of multilayer networks is that the error sur- \\nface can have multiple local minima, in contrast to the single-minimum parabolic \\nerror surface shown in Figure 4.4. Unfortunately, this means that gradient descent \\nis guaranteed only to converge toward some local minimum, and not necessarily \\nthe global minimum error. Despite this obstacle, in practice \\nBACKPROPAGATION \\nhas \\nbeen found to produce excellent results in many real-world applications. \\nThe \\nBACKPROPAGATION \\nalgorithm is presented in Table 4.2. The algorithm as \\ndescribed here applies to layered feedforward networks containing two layers of \\nsigmoid units, with units at each layer connected to all units from the preceding \\nlayer. This is the incremental, or stochastic, gradient descent version of \\nBACK- \\nPROPAGATION. The notation used here is the same as that used in earlier sections, \\nwith the following extensions: \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 110}, page_content='CHAPTER 4 ARTIFICIAL NEURAL NETWORKS 99 \\nAn index (e.g., an integer) is assigned to each node in the network,where \\na \"node\" is either an input to the network or the output of some unit in the \\nnetwork. \\n0 xji denotes the input from node i to unit j, and wji denotes the corresponding \\nweight. \\n0 6, denotes the error term associated with unit n. It plays a role analogous \\nto the quantity (t - o) in our earlier discussion of the delta training rule. As \\nwe shall see later, 6, = - s. \\nNotice the algorithm in Table 4.2 begins by constructing a network with the \\ndesired number of hidden and output units and initializing all network weights \\nto small random values. Given this fixed network structure, the main loop of the \\nalgorithm then repeatedly iterates over the training examples. For each training \\nexample, it applies the network to the example, calculates the error of the network \\noutput for this example, computes the gradient with respect to the error on this \\nexample, then updates all weights in the network. This gradient descent step is \\niterated (often thousands of times, using the same training examples multiple \\ntimes) until the network performs acceptably well. \\nThe gradient descent weight-update rule (Equation \\n[T4.5] \\nin Table 4.2) is \\nsimilar to the delta training rule (Equation \\n[4.10]). \\nLike the delta rule, it updates \\neach weight in proportion to the learning rate r], the input value xji to which \\nthe weight is applied, and the error in the output of the unit. The only differ- \\nence is that the error (t - o) in the delta rule is replaced by a more complex \\nerror term, aj. The exact form of aj follows from the derivation of the weight- \\ntuning rule given in Section 4.5.3. To understand it intuitively, first consider \\nhow ak is computed for each network output unit k (Equation [T4.3] in the al- \\ngorithm). ak is simply the familiar (tk - ok) from the delta rule, multiplied by \\nthe factor ok(l - ok), which is the derivative of the sigmoid squashing function. \\nThe ah value for each hidden unit h has a similar form (Equation [T4.4] in the \\nalgorithm). However, since training examples provide target values tk only for \\nnetwork outputs, no target values are directly available to indicate the error of \\nhidden units\\' values. Instead, the error term for hidden unit h is calculated by \\nsumming the error terms Jk for each output unit influenced by h, weighting each \\nof the ak\\'s by wkh, the weight from hidden unit h to output unit k. This weight \\ncharacterizes the degree to which hidden unit h is \"responsible for\" the error in \\noutput unit k. \\nI \\nThe algorithm in Table 4.2 updates weights incrementally, following the \\nI \\nPresentation of each training example. This corresponds to a stochastic approxi- \\nmation to gradient descent. To obtain the true gradient of E one would sum the \\n6, x,, values over all training examples before altering weight values. \\nThe weight-update loop in \\nBACKPROPAGATION \\nmay be iterated thousands of \\ntimes \\nin a typical application. A variety of termination conditions can be used \\nto \\nhalt the procedure. One may choose to halt after a fixed number of iterations \\nthrough the loop, or once the error on the training examples falls below some \\nthreshold, or once the error on a separate validation set of examples meets some '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 111}, page_content='100 MACHINE LEARNING \\ncriterion. The choice of termination criterion is an important one, because too few \\niterations can fail to reduce error sufficiently, and too many can lead to overfitting \\nthe training data. This issue is discussed in greater detail in Section 4.6.5. \\n4.5.2.1 ADDING MOMENTUM \\nBecause BACKPROPAGATION is such a widely used algorithm, many variations have \\nbeen developed. Perhaps the most common is to alter the weight-update rule in \\nEquation (T4.5) in the algorithm by making the weight update on the nth iteration \\ndepend partially on the update that occurred during the (n - 1)th iteration, as \\nfollows: \\nHere \\nAwji(n) \\nis the weight update performed during the nth iteration through the \\nmain loop of the algorithm, and 0 5 a < 1 is a constant called the momentum. \\nNotice the first term on the right of this equation is just the weight-update rule of \\nEquation (T4.5) in the BACKPROPAGATION algorithm. The second term on the right \\nis new and is called the momentum term. To see the effect of this momentum \\nterm, consider that the gradient descent search trajectory is analogous to that \\nof a (momentumless) ball rolling down the error surface. The effect of a! is to \\nadd momentum that tends to keep the ball rolling in the same direction from \\none iteration to the next. This can sometimes have the effect of keeping the ball \\nrolling through small local minima in the error surface, or along flat regions in \\nthe surface where the ball would stop if there were no momentum. It also has \\nthe effect of gradually increasing the step size of the search in regions where the \\ngradient is unchanging, thereby speeding convergence. \\n4.5.2.2 LEARNING IN ARBITRARY ACYCLIC NETWORKS \\nThe definition of BACKPROPAGATION presented in Table 4.2 applies ohy to two- \\nlayer networks. However, the algorithm given there easily generalizes to \\nfeedfor- \\nward networks of arbitrary depth. The weight update rule seen in Equation (T4.5) \\nis retained, and the only change is to the procedure for computing 6 values. In \\ngeneral, the 6, value for a unit r in layer rn is computed from the 6 values at the \\nnext deeper layer rn + 1 according to \\nNotice this is identical to Step 3 in the algorithm of Table 4.2, so all we are really \\nsaying here is that this step may be repeated for any number of hidden layers in \\nthe network. \\nIt is equally straightforward to generalize the algorithm to any directed \\nacyclic graph, regardless of whether the network units are arranged in uniform \\nlayers as we have assumed up to now. In the case that they are not, the rule for \\ncalculating 6 for any internal unit (i.e., any unit that is not an output) is '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 112}, page_content='CHAPTER 4 ARTIFICIAL NEURAL NETWORKS 101 \\nwhere Downstream(r) is the set of units immediately downstream from unit r in \\nthe \\nnetwork: that is, all units whose inputs include the output of unit r. It is this \\ngneral \\nform of the weight-update rule that we derive in Section 4.5.3. \\n4.5.3 Derivation of the BACKPROPAGATION Rule \\nThis section presents the derivation of the BACKPROPAGATION weight-tuning rule. \\nIt may be skipped on a first reading, without loss of continuity. \\nThe specific problem we address here is deriving the stochastic gradient de- \\nscent rule implemented by the algorithm in Table 4.2. Recall from Equation (4. ll) \\nthat stochastic gradient descent involves iterating through the training examples \\none at a time, for each training example d descending the gradient of the error \\nEd with respect to this single example. In other words, for each training example \\nd every weight wji is updated by adding to it Awji \\nwhere Ed is the error on training example d, summed over all output units in the \\nnetwork \\nHere outputs is the set of output units in the network, tk is the target value of unit \\nk for training example d, and ok is the output of unit k given training example d. \\nThe derivation of the stochastic gradient descent rule is conceptually straight- \\nforward, but requires keeping track of a number of subscripts and variables. We \\nwill follow the notation shown in Figure 4.6, adding a subscript j to denote to \\nthe jth unit of the network as follows: \\nxji = the ith input to unit j \\nwji = the weight associated with the ith input to unit j \\nnetj = xi wjixji (the weighted sum of inputs for unit j) \\noj = the output computed by unit j \\nt, = the target output for unit j \\na = the sigmoid function \\noutputs = the set of units in the final layer of the network \\nDownstream(j) = the set of units whose immediate inputs include the \\noutput of unit j \\nWe now derive an expression for 2 in order to implement the stochastic \\ngradient descent rule seen in Equation (4:2l). To begin, notice that weight wji \\ncan influence the rest of the network only through netj. Therefore, we can use the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 113}, page_content='102 MACHINE LEARNING \\nchain rule to write \\nGiven Equation (4.22), our remaining task is to derive a convenient expression \\nfor z. We consider two cases in turn: the case where unit j is an output unit \\nfor the network, and the case where j is an internal unit. \\nCase 1:  raini in^ Rule for Output Unit Weights. Just as wji can influence the \\nrest of the network only through net,, net, can influence the network only through \\noj. Therefore, we can invoke the chain rule again to write \\nTo begin, consider just the first term in Equation (4.23) \\nThe derivatives &(tk - ok12 will be zero for all output units k except when k = j. \\nWe therefore drop the summation over output units and simply set k = j. \\nNext consider the second term in Equation (4.23). Since oj = a(netj), the \\nderivative $ is just the derivative of the sigmoid function, which we have \\nalready noted is equal to a(netj)(l - a(netj)). Therefore, \\nSubstituting expressions (4.24) and (4.25) into (4.23), we obtain '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 114}, page_content='and combining this with Equations (4.21) and (4.22), we have the stochastic \\ngradient \\ndescent rule for output units \\nNote this training rule is exactly the weight update rule implemented by Equa- \\ntions (T4.3) and (T4.5) in the algorithm of Table 4.2. Furthermore, we can see \\nnow that Sk in Equation (T4.3) is equal to the quantity -$. In the remainder \\nof this section we will use Si to denote the quantity -% for an arbitrary unit i. \\nCase 2: Training Rule for Hidden Unit Weights. In the case where j is an \\ninternal, or hidden unit in the network, the derivation of the training rule for wji \\nmust take into account the indirect ways in which wji can influence the network \\noutputs and hence Ed. For this reason, we will find it useful to refer to the \\nset of all units immediately downstream of unit j in the network (i.e., all units \\nwhose direct inputs include the output of unit j). We denote this set of units by \\nDownstream( j). Notice that netj can influence the network outputs (and therefore \\nEd) only through the units in Downstream(j). Therefore, we can write \\nRearranging terms and using Sj to denote -$, we have \\nand \\nwhich is precisely the general rule from Equation (4.20) for updating internal \\nunit weights in arbitrary acyclic \\ndirected \\ngraphs. Notice Equation (T4.4) from \\nTable 4.2 is just a special case of this rule, in which Downstream(j) = outputs. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 115}, page_content='4.6 REMARKS ON THE BACKPROPAGATION ALGORITHM \\n4.6.1 \\nConvergence and Local Minima \\nAs shown above, the BACKPROPAGATION algorithm implements a gradient descent \\nsearch through the space of possible network weights, iteratively reducing the \\nerror E between the training example target values and the network outputs. \\nBecause the error surface for multilayer networks may contain many different \\nlocal minima, gradient descent can become trapped in any of these. As a result, \\nBACKPROPAGATION \\nover multilayer networks is only guaranteed to converge toward \\nsome local minimum in E and not necessarily to the global minimum error. \\nDespite the lack of assured convergence to the global minimum error, \\nBACK- \\nPROPAGATION is a highly effective function approximation method in practice. In \\nmany practical applications the problem of local minima has not been found to \\nbe as severe as one might fear. To develop some intuition here, consider that \\nnetworks with large numbers of weights correspond to error surfaces in very high \\ndimensional spaces (one dimension per weight). When gradient descent falls into \\na local minimum with respect to one of these weights, it will not necessarily be \\nin a local minimum with respect to the other weights. In fact, the more weights in \\nthe network, the more dimensions that might provide \"escape routes\" for gradient \\ndescent to fall away from the local minimum with respect to this single weight. \\nA second perspective on local minima can be gained by considering the \\nmanner in which network weights evolve as the number of training iterations \\nincreases. Notice that if network weights are initialized to values near zero, then \\nduring early gradient descent steps the network will represent a very smooth \\nfunction that is approximately linear in its inputs. \\nThis \\nis because the sigmoid \\nthreshold function itself is approximately linear when the weights are close to \\nzero (see the plot of the sigmoid function in Figure 4.6). Only after the weights \\nhave had time to grow will they reach a point where they can represent highly \\nnonlinear network functions. One might expect more local minima to exist in the \\nregion of the weight space that represents these more complex functions. One \\nhopes that by the time the weights reach this point they have already moved \\nclose enough to the global minimum that even local minima in this region are \\nacceptable. \\nDespite the above comments, gradient descent over the complex error sur- \\nfaces represented by \\nANNs \\nis still poorly understood, and no methods are known to \\npredict with certainty when local minima will cause difficulties. Common heuris- \\ntics to attempt to alleviate the problem of local minima include: \\nAdd a momentum term to the weight-update rule as described in Equa- \\ntion (4.18). Momentum can sometimes carry the gradient descent procedure \\nthrough narrow local minima (though in principle it can also carry it through \\nnarrow global minima into other local minima!). \\nUse stochastic gradient descent rather than true gradient descent. As dis- \\ncussed in Section 4.4.3.3, the stochastic approximation to gradient descent \\neffectively descends a different error surface for each training example, \\nre- '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 116}, page_content='CHAPTER 4 ARTIFICIAL NEURAL NETWORKS 105 \\nlying on the average of these to approximate the gradient with respect to the \\nfull training set. These different error surfaces typically will have different \\nlocal minima, making it less likely that the process will get stuck in any one \\nof them. \\n0 Train multiple networks using the same data, but initializing each network \\nwith different random weights. If the different training efforts lead to dif- \\nferent local minima, then the network with the best performance over a \\nseparate validation data set can be selected. Alternatively, all networks can \\nbe retained and treated as a \"committee\" of networks whose output is the \\n(possibly weighted) average of the individual network outputs. \\n4.6.2 Representational Power of Feedforward Networks \\nWhat set of functions can be represented by feedfonvard networks? Of course \\nthe answer depends on the width and depth of the networks. Although much is \\nstill unknown about which function classes can be described by which types of \\nnetworks, three quite general results are known: \\nBoolean functions. Every boolean function can be represented exactly by \\nsome network with two layers of units, although the number of hidden units \\nrequired grows exponentially in the worst case with the number of network \\ninputs. To see how this can be done, consider the following general scheme \\nfor representing an arbitrary boolean function: For each possible input vector, \\ncreate a distinct hidden unit and set its weights so that it activates if and only \\nif this specific vector is input to the network. This produces a hidden layer \\nthat will always have exactly one unit active. Now implement the output \\nunit as an OR gate that activates just for the desired input patterns. \\n0 Continuous functions. Every bounded continuous function can be approxi- \\nmated with arbitrarily small error (under a finite norm) by a network with \\ntwo layers of units (Cybenko 1989; \\nHornik \\net al. 1989). The theorem in \\nthis case applies to networks that use sigmoid units at the hidden layer and \\n(unthresholded) linear units at the output layer. The number of hidden units \\nrequired depends on the function to be approximated. \\nArbitraryfunctions. Any function can be approximated to arbitrary accuracy \\nby a network with three layers of units (Cybenko 1988). Again, the output \\nlayer uses linear units, the two hidden layers use sigmoid units, and the \\nnumber of units required at each layer is not known in general. The proof \\nof this involves showing that any function can be approximated by a lin- \\near combination of many localized functions that have value 0 everywhere \\nexcept for some small region, and then showing that two layers of sigmoid \\nunits are sufficient to produce good local approximations. \\nThese results show that limited depth feedfonvard networks provide a very \\nexpressive hypothesis space for BACKPROPAGATION. However, it is important to '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 117}, page_content='keep in mind that the network weight vectors reachable by gradient descent from \\nthe initial weight values may not include all possible weight vectors. Hertz et al. \\n(1991) provide a more detailed discussion of the above results. \\n4.6.3 Hypothesis Space Search and Inductive Bias \\nIt is interesting to compare the hypothesis space search of BACKPROPAGATION to \\nthe search performed by other learning algorithms. For BACKPROPAGATION, every \\npossible assignment of network weights represents a syntactically distinct hy- \\npothesis that in principle can be considered by the learner. In other words, the \\nhypothesis space is the n-dimensional Euclidean space of the n network weights. \\nNotice this hypothesis space is continuous, in contrast to the hypothesis spaces \\nof decision tree learning and other methods based on discrete representations. \\nThe fact that it is continuous, together with the fact that E is differentiable with \\nrespect to the continuous parameters of the hypothesis, results in a well-defined \\nerror gradient that provides a very useful structure for organizing the search for \\nthe best hypothesis. This structure is quite different from the general-to-specific \\nordering used to organize the search for symbolic concept learning algorithms, \\nor the simple-to-complex ordering over decision trees used by the ID3 and C4.5 \\nalgorithms. \\nWhat is the inductive bias by which \\nBACKPROPAGATION \\ngeneralizes beyond \\nthe observed data? It is difficult to characterize precisely the inductive bias of \\nBACKPROPAGATION \\nlearning, because it depends on the interplay between the gra- \\ndient descent search and the way in which the weight space spans the space of \\nrepresentable functions. However, one can roughly characterize it as smooth in- \\nterpolation between data points. Given two positive training examples with no \\nnegative examples between them, \\nBACKPROPAGATION \\nwill tend to label points in \\nbetween as positive examples as well. This can be seen, for example, in the de- \\ncision surface illustrated in Figure 4.5, in which the specific sample of training \\nexamples gives rise to smoothly varying decision regions. \\n4.6.4 Hidden Layer Representations \\nOne intriguing property of BACKPROPAGATION is its ability to discover useful in- \\ntermediate representations at the hidden unit layers inside the network. Because \\ntraining examples constrain only the network inputs and outputs, the weight-tuning \\nprocedure is free to set weights that define whatever hidden unit representation is \\nmost effective at minimizing the squared error E. This can lead BACKPROPAGATION \\nto define new hidden layer features that are not explicit in the input representa- \\ntion, but which capture properties of the input instances that are most relevant to \\nlearning the target function. \\nConsider, for example, the network shown in Figure 4.7. Here, the eight \\nnetwork inputs are connected to three hidden units, which are in turn connected \\nto the eight output units. Because of this structure, the three hidden units will \\nbe forced to re-represent the eight input values in some way that captures their '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 118}, page_content=\"Inputs Outputs Input \\n10000000 \\n0 1000000 \\n00 100000 \\n00010000 \\n00001000 \\n00000 100 \\nooOOOo 10 \\n0000000 1 \\nHidden \\nValues \\n.89 .04 .08 + \\n.15 .99 .99 + \\n.01 .97 .27 + \\n.99 .97 .71 + \\n.03 .05 .02 + \\n.01 .ll .88 + \\n.80 .01 .98 + \\n.60 .94 .01 + \\noutput \\n10000000 \\n0 1000000 \\n00 100000 \\n000 10000 \\n0000 1000 \\n00000 100 \\n000000 10 \\n0000000 1 \\nFIGURE 4.7 \\nLearned Hidden Layer Representation. This 8 x 3 x 8 network was trained to learn the identity \\nfunction, using the eight training examples shown. After 5000 training epochs, the three hidden unit \\nvalues encode the eight distinct inputs using the encoding shown on the right. Notice if the encoded \\nvalues are rounded to zero or one, the result is the standard binary encoding for eight distinct values. \\nrelevant features, so that this hidden layer representation can be used by the output \\nunits to compute the correct target values. \\nConsider training the network shown in Figure 4.7 to learn the simple target \\nfunction f (2) = 2, where 2 is a vector containing seven 0's and a single 1. The \\nnetwork must learn to reproduce the eight inputs at the corresponding eight output \\nunits. Although this is a simple function, the network in this case is constrained \\nto use only three hidden units. Therefore, the essential information from all eight \\ninput units must be captured by the three learned hidden units. \\nWhen \\nBACKPROPAGATION \\nis applied to this task, using each of the eight pos- \\nsible vectors as training examples, it \\nsuccessfully \\nlearns the target function. What \\nhidden layer representation is created by the gradient descent \\nBACKPROPAGATION \\nalgorithm? By examining the hidden unit values generated by the learned network \\nfor each of the eight possible input vectors, it is easy to see that the learned en- \\ncoding is similar to the familiar standard binary encoding of eight values using \\nthree bits \\n(e.g., 000,001,010,. . . , 111). The exact values of the hidden units for \\none typical \\nrun \\nof \\nBACKPROPAGATION \\nare shown in Figure 4.7. \\nThis ability of multilayer networks to automatically discover useful repre- \\nsentations at the hidden layers is a key feature of ANN learning. In contrast to \\nlearning methods that are constrained to use only predefined features provided by \\nthe \\nhuman designer, this provides an important degree of flexibility that allows \\nthe \\nlearner to invent features not explicitly \\nintroduced \\nby the human designer. Of \\ncourse \\nthese invented features must still be computable as sigmoid unit functions \\nof \\nthe provided network inputs. Note when more layers of units are used in the \\nnetwork, more complex features can be invented. Another example of hidden layer \\nfeatures is provided in the face recognition application discussed in Section 4.7. \\nIn order to develop a better intuition for the operation of BACKPROPAGATION \\nin \\nthis example, let us examine the operation of the gradient descent procedure in \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 119}, page_content=\"greater detailt. The network in Figure 4.7 was trained using the algorithm shown \\nin Table 4.2, with initial weights set to random values in the interval (-0.1,0.1), \\nlearning rate q = 0.3, and no weight momentum (i.e., a! = 0). Similar results \\nwere obtained by using other learning rates and by including nonzero momentum. \\nThe hidden unit encoding shown in Figure 4.7 was obtained after 5000 training \\niterations through the outer loop of the algorithm \\n(i.e., \\n5000 iterations through each \\nof the eight training examples). Most of the interesting weight changes occurred, \\nhowever, during the first 2500 iterations. \\nWe can directly observe the effect of \\nBACKPROPAGATION'S \\ngradient descent \\nsearch by plotting the squared output error as a function of the number of gradient \\ndescent search steps. This is shown in the top plot of Figure 4.8. Each line in \\nthis plot shows the squared output error summed over all training examples, for \\none \\nof the eight network outputs. The horizontal axis indicates the number of \\niterations through the outermost loop of the \\nBACKPROPAGATION \\nalgorithm. As this \\nplot indicates, the sum of squared errors for each output decreases as the gradient \\ndescent procedure proceeds, more quickly for some output units and less quickly \\nfor others. \\nThe evolution of the hidden layer representation can be seen in the second \\nplot of Figure 4.8. This plot shows the three hidden unit values computed by the \\nlearned network for one of the possible inputs (in particular, 01000000). Again, the \\nhorizontal axis indicates the number of training iterations. As this plot indicates, \\nthe network passes through a number of different encodings before converging to \\nthe final encoding given in Figure 4.7. \\nFinally, the evolution of individual weights within the network is illustrated \\nin the third plot of Figure 4.8. This plot displays the evolution of weights con- \\nnecting the eight input units (and the constant 1 bias input) to one of the three \\nhidden units. Notice that significant changes in the weight values for this hidden \\nunit coincide with significant changes in the hidden layer encoding and output \\nsquared errors. The weight that converges to a value near zero in this case is the \\nbias weight wo. \\n4.6.5 Generalization, Overfitting, and Stopping Criterion \\nIn the description of t'le BACKPROPAGATION algorithm in Table 4.2, the termination \\ncondition for the \\nalgcrithm \\nhas been left unspecified. What is an appropriate con- \\ndition for \\nterrninatinp \\nthe weight update loop? One obvious choice is to continue \\ntraining until the errcr E on the training examples falls below some predetermined \\nthreshold. In fact, this is a poor strategy because \\nBACKPROPAGATION \\nis \\nsuscepti- \\nble to overfitting the training examples at the cost of decreasing generalization \\naccuracy over other unseen examples. \\nTo see the dangers of minimizing the error over the training data, consider \\nhow the error E varies with the number of weight iterations. Figure 4.9 shows \\nt~he source code to reproduce this example is available at http://www.cs.cmu.edu/-tom/mlbook.hhnl. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 120}, page_content='Sum of squared errors for each output unit \\nHidden unit encoding for input 01000000 \\nFIGURE 4.8 \\nLearning the 8 x 3 x 8 Network. The top plot shows the evolving sum of squared errors for each of \\nthe eight output units, as the number of training iterations (epochs) increases. The middle plot shows \\nthe evolving hidden layer representation for the input string \"01000000.\" The bottom plot shows the \\nevolving weights for one of the three hidden units. \\nWeights from inputs to one hidden \\nunit \\n4 \\n3 \\n2 \\n1 \\n-1 \\n-2 \\nI .................. ...:........... ......... .:siii..... ziiii \\n.. ..................... ...... .... ....-- --- .. ...---- \\n....---.-- .... - _.. __-. .->-.------ \\n/-,-.<-- \\n........... ,*\\' ,.. ... - , . ... .>, \\n... ,\\'... ... ,,,.- -.. \\n................................................ .. ....;, - ..< , . , \\n,I\\' ,I \\n./;. /- ,/\\' \\n&:>::.--= <, \\n\" -I-- \\n... \\'.,.. ....... - . ... .. \\'.. \\n. .:. \\n- , - -. -- . - - - - - . . _ .., . . . _ . . _ . . \\n.... ..................... - \\n.-.. .......\"... ..... -_ .._ -_ . -. \\n-- _ _ _ _ _ _ _ ...... ......................................... 1 '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 121}, page_content='110 MACHINE LEARNING \\nError versus weight updates (example 1) \\nValidation set error \\n0.008 \\n0.007 \\n0 \\n5000 loo00 15000 20000 \\nNumber of weight updates \\n0 lo00 2000 3000 4000 5000 6000 \\nNumber of weight updates \\nError versus weight updates (example 2) \\n0.08 %** I r 8 \\nFIGURE 4.9 \\nPlots of error E as a function of the number of weight updates, for two different robot perception \\ntasks. In both learning cases, error E over the training examples decreases monotonically, as gradient \\ndescent minimizes this measure of error. Error over the separate \"validation\" set of examples typically \\ndecreases at first, then may later increase due to overfitting the training examples. The network most \\nIikeIy \\nto generalize correctly to unseen data is the network with the lowest error over the validation \\nset. Notice in the second plot, one must be careful to not stop training too soon when the validation \\nset error begins to increase. \\n0.07 \\n0.06 \\nthis variation for two fairly typical applications of BACKPROPAGATION. Consider \\nfirst the top plot in this figure. The lower of the two lines shows the monotoni- \\ncally decreasing error E over the training set, as the number of gradient descent \\niterations grows. The upper line shows the error E measured over a different vali- \\ndation set of examples, distinct from the training examples. This line measures the \\ngeneralization accuracy of the network-the accuracy with which it fits examples \\nbeyond the training data. \\n- Training set error * - \\nValidation set error + \\ny+:L '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 122}, page_content='CHAPTER 4 ARTIFICIAL NEURAL NETWORKS 111 \\nNotice the generalization accuracy measured over the validation examples \\nfirst decreases, then increases, even as the error over the training examples contin- \\nues to decrease. How can this occur? This occurs because the weights are being \\ntuned \\nto fit idiosyncrasies of the training examples that are not representative of \\nthe \\ngeneral distribution of examples. The large number of weight parameters in \\nANNs \\nprovides many degrees of freedom for fitting such idiosyncrasies. \\nWhy does overfitting tend to occur during later iterations, but not during ear- \\nlier iterations? Consider that network weights are initialized to small random val- \\nues. With weights of nearly identical value, only very smooth decision surfaces are \\ndescribable. As training proceeds, some weights begin to grow in order to reduce \\nthe \\nerror over the training data, and the complexity of the learned decision surface \\nincreases. Thus, the effective complexity of the hypotheses that can be reached by \\nBACKPROPAGATION increases with the number of weight-tuning iterations. Given \\nenough weight-tuning iterations, BACKPROPAGATION will often be able to create \\noverly complex decision surfaces that fit noise in the training data or unrepresen- \\ntative characteristics of the particular training sample. This overfitting problem is \\nanalogous to the overfitting problem in decision tree learning (see Chapter 3). \\nSeveral techniques are available to address the overfitting problem for BACK- \\nPROPAGATION learning. One approach, known as weight decay, is to decrease each \\nweight by some small factor during each iteration. This is equivalent to modifying \\nthe definition of E to include a penalty term corresponding to the total magnitude \\nof the network weights. The motivation for this approach is to keep weight values \\nsmall, to bias learning against complex decision surfaces. \\nOne of the most successful methods for overcoming the overfitting problem \\nis to simply provide a set of validation data to the algorithm in addition to the \\ntraining data. The algorithm monitors the error with respect to this validation set, \\nwhile using the training set to drive the gradient descent search. In essence, this \\nallows the algorithm itself to plot the two curves shown in Figure 4.9. How many \\nweight-tuning iterations should the algorithm perform? Clearly, it should use the \\nnumber of iterations that produces the lowest error over the validation set, since \\nthis is the best indicator of network performance over unseen examples. In typical \\nimplementations of this approach, two copies of the network weights are kept: \\none copy for training and a separate copy of the best-performing weights thus far, \\nmeasured by their error over the validation set. Once the trained weights reach a \\nsignificantly higher error over the validation set than the stored weights, training \\nis \\nterminated and the stored weights are returned as the final hypothesis. When \\nthis procedure is applied in the case of the top plot of Figure 4.9, it outputs the \\nnetwork weights obtained after 9100 iterations. The second plot in Figure 4.9 \\nshows \\nthat it is not always obvious when the lowest error on the validation set \\nhas \\nbeen reached. In this plot, the validation set error decreases, then increases, \\nthen \\ndecreases again. Care must be taken to avoid the mistaken conclusion that \\nthe \\nnetwork has reached its lowest validation set error at iteration 850. \\nIn general, the issue of overfitting and how to overcome it is a subtle one. \\nThe above cross-validation approach works best when extra data are available to \\nprovide a validation set. Unfortunately, however, the problem of overfitting is most '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 123}, page_content=\"112 MACHINE LEARNWG \\nI \\nsevere for small training sets. In these cases, a k-fold cross-validation approach \\nis sometimes used, in which cross validation is performed k different times, each \\ntime using a different partitioning of the data into training and validation sets, \\nand the results are then averaged. In one version of this approach, the m available \\nexamples are partitioned into k disjoint subsets, each of size \\nm/k. \\nThe \\ncross- \\nvalidation procedure is then run k times, each time using a different one of these \\nsubsets as the validation set and combining the other subsets for the training set. \\nThus, each example is used in the validation set for one of the experiments and \\nin the training set for the other k - 1 experiments. On each experiment the above \\ncross-validation approach is used to determine the number of iterations i that yield \\nthe best performance on the validation set. The mean i of these estimates for i \\nis then calculated, and a final run of BACKPROPAGATION is performed training on \\nall n examples for i iterations, with no validation set. This procedure is closely \\nrelated to the procedure for comparing two learning methods based on limited \\ndata, described in Chapter 5. \\n4.7 AN ILLUSTRATIVE EXAMPLE: FACE RECOGNITION \\nTo illustrate some of the practical design choices involved in applying BACKPROPA- \\nGATION, \\nthis section discusses applying it to a learning task involving face recogni- \\ntion. All image data and code used to produce the examples described in this sec- \\ntion are available at World Wide Web site http://www.cs.cmu.edu/-tomlmlbook. \\nhtml, along with complete documentation on how to use the code. Why not try it \\nyourself? \\n4.7.1 The Task \\nThe learning task here involves classifying camera images of faces of various \\npeople in various poses. Images of 20 different people were collected, including \\napproximately 32 images per person, varying the person's expression (happy, sad, \\nangry, neutral), the direction in which they were looking (left, right, straight ahead, \\nup), and whether or not they were wearing sunglasses. As can be seen from the \\nexample images in Figure 4.10, there is also variation in the background behind \\nthe person, the clothing worn by the person, and the position of the person's \\nface within the image. In total, 624 greyscale images were collected, each with a \\nresolution of 120 x 128, with each image pixel described by a greyscale intensity \\nvalue between \\n0 \\n(black) and 255 (white). \\nA variety of target functions can be learned from this image data. For ex- \\nample, given an image as input we could train an ANN to output the identity of \\nthe person, the direction in which the person is facing, the gender of the person, \\nwhether or not they are wearing sunglasses, etc. \\nAll \\nof these target functions can \\nbe learned to high accuracy from this image data, and the reader is encouraged \\nto try out these experiments. In the remainder of this section we consider one \\nparticular task: learning the direction in which the person is facing (to their left, \\nright, straight ahead, or upward). I \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 124}, page_content='30 x 32 resolution input images \\nleft straight right \\nL \\nNetwork weights after 1 iteration through each training example \\nleft \\nNetwork weights after 100 iterations through each training example \\nFIGURE 4.10 \\nLearning an artificial neural network to recognize face pose. Here a 960 x 3 x 4 network is trained \\non grey-level images of faces (see top), to predict whether a person is looking to their left, right, \\nahead, or up. After training on 260 such images, the network achieves an accuracy of 90% over a \\nseparate test set. The learned network weights are shown after one weight-tuning iteration through \\nthe training examples and after 100 iterations. Each output unit (left, straight, right, up) has four \\nweights, shown by dark (negative) and light (positive) blocks. The leftmost block corresponds to \\nthe \\nweight wg, which determines the unit threshold, and the three blocks to the right correspond to \\nweights on inputs from the three hidden units. The weights from the image pixels into each hidden \\nunit \\nare also shown, with each weight plotted in the position of the corresponding image pixel. \\n4.7.2 Design Choices \\nIn applying BACKPROPAGATION to any given task, a number of design choices \\nmust be made. We summarize these choices below for our task of learning the \\ndirection in which a person is facing. Although no attempt was made to determine \\nthe precise optimal design choices for this task, the design described here learns '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 125}, page_content='the target function quite well. After training on a set of 260 images, classification \\naccuracy over a separate test set is 90%. In contrast, the default accuracy achieved \\nby randomly guessing one of the four possible face directions is 25%. \\nInput encoding. Given that the ANN input is to be some representation of the \\nimage, one key design choice is how to encode this image. For example, we could \\npreprocess the image to extract edges, regions of uniform intensity, or other local \\nimage features, then input these features to the network. One difficulty with this \\ndesign option is that it would lead to a variable number of features \\n(e.g., \\nedges) \\nper image, whereas the ANN has a fixed number of input units. The design option \\nchosen in this case was instead to encode the image as a fixed set of 30 x 32 pixel \\nintensity values, with one network input per pixel. The pixel intensity values \\nranging from \\n0 \\nto 255 were linearly scaled to range from \\n0 \\nto 1 so that network \\ninputs would have values in the same interval as the hidden unit and output unit \\nactivations. The 30 x 32 pixel image is, in fact, a coarse resolution summary of \\nthe original 120 x 128 captured image, with each coarse pixel intensity calculated \\nas the mean of the corresponding high-resolution pixel intensities. Using this \\ncoarse-resolution image reduces the number of inputs and network weights to \\na much more manageable size, thereby reducing computational demands, while \\nmaintaining sufficient resolution to correctly classify the images. Recall from \\nFigure 4.1 that the ALVINN system uses a similar coarse-resolution image as \\ninput to the network. One interesting difference is that in ALVINN, each coarse \\nresolution pixel intensity is obtained by selecting the intensity of a single pixel at \\nrandom from the appropriate region within the high-resolution image, rather than \\ntaking the mean of all pixel intensities within this region. The motivation for this \\nic \\nALVINN is that it significantly reduces the computation required to produce the \\ncoarse-resolution image from the available high-resolution image. This efficiency \\nis especially important when the network must be used to process many images \\nper second while autonomously driving the vehicle. \\nOutput encoding. The ANN must output one of four values indicating the di- \\nrection in which the person is looking (left, right, up, or straight). Note we could \\nencode this four-way classification using a single output unit, assigning outputs \\nof, say, \\n0.2,0.4,0.6, \\nand 0.8 to encode these four possible values. Instead, we \\nuse four distinct output units, each representing one of the four possible face di- \\nrections, with the highest-valued output taken as the network prediction. This is \\noften called a 1 -0f-n output encoding. There are two motivations for choosing the \\n1-of-n output encoding over the single unit option. First, it provides more degrees \\nof freedom to the network for representing the target function \\n(i.e., \\nthere are n \\ntimes as many weights available in the output layer of units). Second, in the 1-of-n \\nencoding the difference between the highest-valued output and the second-highest \\ncan be used as a measure of the confidence in the network prediction (ambiguous \\nclassifications may result in near or exact ties). A further design choice here is \\n\"what should be the target values for these four output units?\\' One obvious choice \\nwould be to use the four target values \\n(1,0,0,O) \\nto encode a face looking to the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 126}, page_content='left, (0,1,0,O) to encode a face looking straight, etc. Instead of 0 and 1 values, \\nwe \\nuse values of 0.1 and 0.9, so that \\n(0.9,O. \\n1,0.1,0.1) \\nis the target output vector \\nfor \\na face looking to the left. The reason for avoiding target values of \\n0 \\nand 1 \\nis \\nthat sigmoid units cannot produce these output values given finite weights. If \\nwe attempt to train the network to fit target values of exactly \\n0 \\nand 1, gradient \\ndescent will force the weights to grow without bound. On the other hand, values \\nof 0.1 and 0.9 are achievable using a sigmoid unit with finite weights. \\nNetwork graph structure. As described earlier, BACKPROPAGATION can be ap- \\nplied to any acyclic directed graph of sigmoid units. Therefore, another design \\nchoice we face is how many units to include in the network and how to inter- \\nconnect them. The most common network structure is a layered network with \\nfeedforward \\nconnections from every unit in one layer to every unit in the next. \\nIn \\nthe current design we chose this standard structure, using two layers of \\nsig- \\nmoid \\nunits (one hidden layer and one output layer). It is common to use one or \\ntwo layers of sigmoid units and, occasionally, three layers. It is not common to \\nuse more layers than this because training times become very long and because \\nnetworks with three layers of sigmoid units can already express a rich variety of \\ntarget functions (see Section 4.6.2). Given our choice of a layered feedforward \\nnetwork with one hidden layer, how many hidden units should we include? In the \\nresults reported in Figure 4.10, only three hidden units were used, yielding a test \\nset accuracy of 90%. In other experiments 30 hidden units were used, yielding a \\ntest set accuracy one to two percent higher. Although the generalization accuracy \\nvaried only a small amount between these two experiments, the second experiment \\nrequired significantly more training time. Using 260 training images, the training \\ntime was approximately 1 hour on a Sun \\nSparc5 \\nworkstation for the 30 hidden unit \\nnetwork, compared to approximately 5 minutes for the 3 hidden unit network. In \\nmany applications it has been found that some minimum number of hidden units \\nis required in order to learn the target function accurately and that extra hidden \\nunits above this number do not dramatically affect generalization accuracy, pro- \\nvided cross-validation methods are used to determine how many gradient descent \\niterations should be performed. If such methods are not used, then increasing the \\nnumber of hidden units often increases the tendency to \\noverfit \\nthe training data, \\nthereby reducing generalization accuracy. \\nOther learning algorithm parameters. In these learning experiments the learn- \\ning rate r] was set to 0.3, and the momentum a! was set to 0.3. Lower values for both \\nparameters produced roughly equivalent generalization accuracy, but longer train- \\ning times. If these values are set too high, training fails to converge to a network \\nwith acceptable error over the training set. Full gradient descent was used in all \\nthese experiments (in contrast to the stochastic approximation to gradient descent \\nin \\nthe algorithm of Table 4.2). Network weights in the output units were initial- \\nized to small random values. However, input unit weights were initialized to zero, \\nbecause this yields much more intelligible visualizations of the learned weights \\n(see Figure \\n4.10), \\nwithout any noticeable impact on generalization accuracy. The '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 127}, page_content='number of training iterations was selected by partitioning the available data into \\na training set and a separate validation set. Gradient descent was used to min- \\nimize the error over the training set, and after every 50 gradient descent steps \\nthe performance of the network was evaluated over the validation set. The final \\nselected network was the one with the highest accuracy over the validation set. \\nSee Section 4.6.5 for an explanation and justification of this procedure. The final \\nreported accuracy \\n(e-g., \\n90% for the network in Figure 4.10) was measured over \\nyet a third set of test examples that were not used in any way to influence training. \\n4.7.3 Learned Hidden Representations \\nIt is interesting to examine the learned weight values for the 2899 weights in the \\nnetwork. Figure 4.10 depicts the values of each of these weights after one iteration \\nthrough the weight update for all training examples, and again after 100 iterations. \\nTo understand this diagram, consider first the four rectangular blocks just \\nbelow the face images in the figure. Each of these rectangles depicts the weights \\nfor one of the four output units in the network (encoding left, straight, right, and \\nup). The four squares within each rectangle indicate the four weights associated \\nwith this output unit-the weight wo, which determines the unit threshold (on \\nthe left), followed by the three weights connecting the three hidden units to this \\noutput. The brightness of the square indicates the weight value, with bright white \\nindicating a large positive weight, dark black indicating a large negative weight, \\nand intermediate shades of grey indicating intermediate weight values. For ex- \\nample, the output unit labeled \"up\" has a near zero wo threshold weight, a large \\npositive weight from the first hidden unit, and a large negative weight from the \\nsecond hidden unit. \\nThe weights of the hidden units are shown directly below those for the output \\nunits. Recall that each hidden unit receives an input from each of the 30 x 32 \\nimage pixels. The 30 x 32 weights associated with these inputs are displayed so \\nthat each weight is in the position of the corresponding image pixel (with the wo \\nthreshold weight superimposed in the top left of the array). Interestingly, one can \\nsee that the weights have taken on values that are especially sensitive to features \\nin the region of the image in which the face and body typically appear. \\nThe values of the network weights after 100 gradient descent iterations \\nthrough each training example are shown at the bottom of the figure. Notice the \\nleftmost \\nhidden unit has very different weights than it had after the first iteration, \\nand the other two hidden units have changed as well. It is possible to understand \\nto some degree the encoding in this final set of weights. For example, consider the \\noutput unit that indicates a person is \\nlooking \\nto his right. This unit has a strong \\npositive weight from the second hidden unit and a strong negative weight from \\nthe third hidden unit. Examining the weights of these two hidden units, it is easy \\nto see that if the person\\'s face is turned to his right \\n(i.e., \\nour left), then his bright \\nskin \\nwill roughly align with strong positive weights in this hidden unit, and his \\ndark hair will roughly align with negative weights, resulting in this unit outputting \\na large value. The same image will cause the third hidden unit to output a value '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 128}, page_content=\"close to zero, as the bright face will tend to align with the large negative weights \\nin this case. \\n4.8 ADVANCED TOPICS IN ARTIFICIAL NEURAL NETWORKS \\n4.8.1 Alternative Error Functions \\nAs noted earlier, gradient descent can be performed for any function E that is \\ndifferentiable with respect to the parameterized hypothesis space. While the basic \\nBAcWROPAGATION \\nalgorithm defines E in terms of the sum of squared errors \\nof the network, other definitions have been suggested in order to incorporate \\nother constraints into the weight-tuning rule. For each new definition of E a new \\nweight-tuning rule for gradient descent must be derived. Examples of alternative \\ndefinitions of E include \\na Adding a penalty term for weight magnitude. As discussed above, we can \\nadd a term to E that increases with the magnitude of the weight vector. \\nThis causes the gradient descent search to seek weight vectors with small \\nmagnitudes, thereby reducing the risk of \\noverfitting. \\nOne way to do this is \\nto redefine E as \\nwhich yields a weight update rule identical to the \\nBACKPROPAGATION \\nrule, \\nexcept that each weight is multiplied by the constant (1 - 2yq) upon each \\niteration. Thus, choosing this definition of E is equivalent to using a weight \\ndecay strategy (see Exercise 4.10.) \\na Adding a term for errors in the slope, or derivative of the target func- \\ntion. In some cases, training information may be available regarding desired \\nderivatives of the target function, as well as desired values. For example, \\nSimard et al. (1992) describe an application to character recognition in which \\ncertain training derivatives are used to constrain the network to learn char- \\nacter recognition functions that are invariant of translation within the im- \\nage. Mitchell and \\nThrun \\n(1993) describe methods for calculating training \\nderivatives based on the learner's prior knowledge. In both of these sys- \\ntems (described in Chapter 12), the error function is modified to add a term \\nmeasuring the discrepancy between these training derivatives and the actual \\nderivatives of the learned network. One example of such an error function is \\nHere x: denotes the value of the jth input unit for training example d. \\nThus, 2 is the training derivative describing how the target output value \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 129}, page_content='118 MACHINE LEARNING \\ntkd should vary with a change in the input xi. Similarly, 9 denotes the \\nax, \\ncorresponding derivative of the actual learned network. The constant ,u de- \\ntermines the relative weight placed on fitting the training values versus the \\ntraining derivatives. \\n0 Minimizing the cross entropy of the network with respect to the target values. \\nConsider learning a probabilistic function, such as predicting whether a loan \\napplicant will pay back a loan based on attributes such as the applicant\\'s age \\nand bank balance. Although the training examples exhibit only boolean target \\nvalues (either a 1 or 0, depending on whether this applicant paid back the \\nloan), the underlying target function might be best modeled by outputting the \\nprobability that the given applicant will repay the loan, rather than attempting \\nto output the actual 1 and \\n0 \\nvalue for each input instance. Given such \\nsituations in which we wish for the network to output probability estimates, \\nit can be shown that the best \\n(i.e., \\nmaximum likelihood) probability estimates \\nare given by the network that minimizes the cross entropy, defined as \\nHere od is the probability estimate output by the network for training ex- \\nample d, and td is the 1 or 0 target value for training example d. Chapter 6 \\ndiscusses when and why the most probable network hypothesis is the one \\nthat minimizes this cross entropy and derives the corresponding gradient \\ndescent weight-tuning rule for sigmoid units. That chapter also describes \\nother conditions under which the most probable hypothesis is the one that \\nminimizes the sum of squared errors. \\n0 Altering the effective error function can also be accomplished by weight \\nsharing, or \"tying together\" weights associated with different units or inputs. \\nThe idea here is that different network weights are forced to take on identical \\nvalues, usually to enforce some constraint known in advance to the human \\ndesigner. For example, Waibel et al. (1989) and Lang et al. (1990) describe \\nan application of neural networks to speech recognition, in which the net- \\nwork inputs are the speech frequency components at different times within a \\n144 millisecond time window. One assumption that can be made in this ap- \\nplication is that the frequency components that identify a specific sound \\n(e.g., \\n\"eee\") should be independent of the exact time that the sound occurs within \\nthe 144 millisecond window. To enforce this constraint, the various units that \\nreceive input from different portions of the time window are forced to share \\nweights. The net effect is to constrain the space of potential hypotheses, \\nthereby reducing the risk of overfitting and improving the chances for accu- \\nrately generalizing to unseen situations. Such weight sharing is typically im- \\nplemented by first updating each of the shared weights separately within each \\nunit that uses the weight, then replacing each instance of the shared weight by \\nthe mean of their values. The result of this procedure is that shared weights \\neffectively adapt to a different error function than do the unshared weights. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 130}, page_content=\"4.8.2 Alternative Error Minimization Procedures \\nWhile gradient descent is one of the most general search methods for finding a \\nhypothesis to minimize the error function, it is not always the most efficient. It \\nis \\nnot uncommon for \\nBACKPROPAGATION \\nto require tens of thousands of iterations \\nthrough the weight update loop when training complex networks. For this reason, \\na number of alternative weight optimization algorithms have been proposed and \\nexplored. To see some of the other possibilities, it is helpful to think of a weight- \\nupdate method as involving two decisions: choosing a direction in which to alter \\nthe current weight vector and choosing a distance to move. In BACKPROPAGATION, \\nthe direction is chosen by taking the negative of the gradient, and the distance is \\ndetermined by the learning rate constant q. \\nOne optimization method, known as line search, involves a different ap- \\nproach to choosing the distance for the weight update. In particular, once a line is \\nchosen that specifies the direction of the update, the update distance is chosen by \\nfinding the minimum of the error function along this line. Notice this can result \\nin a very large or very small weight update, depending on the position of the \\npoint along the line that minimizes error. A second method, that builds on the \\nidea of line search, is called the conjugate gradient method. Here, a sequence of \\nline \\nsearshes \\nis performed to search for a minimum in the error surface. On the \\nfirst step in this sequence, the direction chosen is the negative of the gradient. \\nOn each subsequent step, a new direction is chosen so that the component of the \\nerror gradient that has just been made zero, remains zero. \\nWhile alternative error-minimization methods sometimes lead to improved \\nefficiency in training the network, methods such as conjugate gradient tend to \\nhave no significant impact on the generalization error of the final network. The \\nonly likely impact on the final error is that different error-minimization procedures \\nmay fall into different local minima. Bishop (1996) contains a general discussion \\nof several parameter optimization methods for training networks. \\n4.8.3 Recurrent Networks \\nUp to this point we have considered only network topologies that correspond \\nto acyclic directed graphs. Recurrent networks are artificial neural networks that \\napply to time series data and that use outputs of network units at time t as the \\ninput to other units at time t + 1. In this way, they support a form of directed \\ncycles in the network. To illustrate, consider the time series prediction task of \\npredicting the next day's stock market average y(t + 1) based on the current day's \\neconomic indicators x(t). Given a time series of such data, one obvious approach \\nis \\nto train a feedforward network to predict y(t + 1) as its output, based on the \\ninput values x(t). Such a network is shown in Figure 4.11(a). \\nOne limitation of such a network is that the prediction of y(t + 1) depends \\nonly \\non x(t) and cannot capture possible dependencies of y (t + 1) on earlier values \\nof x. This might be necessary, for example, if tomorrow's stock market average \\n~(t + 1) depends on the difference between today's economic indicator values \\nx(t) and yesterday's values x(t - 1). Of course we could remedy this difficulty \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 131}, page_content='I 120 MACHINE LEARNING \\n(4 Feedforward network (b) Recurrent network \\nx(t - 2) c(t - 2) \\n(d Recurrent network \\nunfolded in time \\nFIGURE 4.11 \\nRecurrent networks. \\nby making both x(t) and x(t - 1) inputs to the feedforward network. However, \\nif we wish the network to consider an arbitrary window of time in the past when \\npredicting \\ny(t + l), then a different solution is required. The recurrent network \\nshown in Figure 4.1 \\n1(b) \\nprovides one such solution. Here, we have added a new \\nunit b to the hidden layer, and new input unit c(t). The value of c(t) is defined \\nas the value of unit b at time t - 1; that is, the input value c(t) to the network at \\none time step is simply copied from the value of unit b on the previous time step. \\nNotice this implements a recurrence relation, in which b represents information \\nabout the history of network inputs. Because b depends on both x(t) and on c(t), \\nit is possible for b to summarize information from earlier values of x that are \\narbitrarily distant in time. Many other network topologies also can be used to '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 132}, page_content='CHAPTER 4 ARTIFICIAL NEURAL NETWORKS 121 \\nrepresent recurrence relations. For example, we could have inserted several layers \\nof units between the input and unit b, and we could have added several context \\nin parallel where we added the single units b and c. \\nHow can such recurrent networks be trained? There are several variants of \\nrecurrent \\nnetworks, and several training methods have been proposed (see, for \\nexample, Jordan 1986; \\nElman \\n1990; Mozer 1995; Williams and Zipser 1995). \\nInterestingly, recurrent networks such as the one shown in Figure 4.1 \\n1(b) \\ncan be \\ntrained using a simple variant of BACKPROPAGATION. TO understand how, consider \\nFigure \\n4.11(c), \\nwhich shows the data flow of the recurrent network \"unfolded \\nin \\ntime. Here we have made several copies of the recurrent network, replacing \\nthe feedback loop by connections between the various copies. Notice that this \\nlarge unfolded network contains no cycles. Therefore, the weights in the unfolded \\nnetwork can be trained directly using BACKPROPAGATION. Of course in practice \\nwe wish to keep only one copy of the recurrent network and one set of weights. \\nTherefore, after training the unfolded network, the final weight wji in the recurrent \\nnetwork can be taken to be the mean value of the corresponding wji weights in \\nthe various copies. Mozer (1995) describes this training process in greater detail. \\nIn practice, recurrent networks are more difficult to train than networks with no \\nfeedback loops and do not generalize as reliably. However, they remain important \\ndue to their increased representational power. \\n4.8.4 Dynamically Modifying Network Structure \\nUp to this point we have considered neural network learning as a problem of \\nadjusting weights within a fixed graph structure. A variety of methods have been \\nproposed to dynamically grow or shrink the number of network units and intercon- \\nnections in an attempt to improve generalization accuracy and training efficiency. \\nOne idea is to begin with a network containing no hidden units, then grow \\nthe network as needed by adding hidden units until the training error is reduced \\nto some acceptable level. The CASCADE-CORRELATION algorithm \\n(Fahlman \\nand \\nLebiere 1990) is one such algorithm. CASCADE-CORRELATION begins by construct- \\ning a network with no hidden units. In the case of our face-direction learning task, \\nfor example, it would construct a network containing only the four output units \\ncompletely connected to the 30 x 32 input nodes. After this network is trained for \\nsome time, we \\nmay \\nwell find that there remains a significant residual error due \\nto \\nthe fact that the target function cannot be perfectly represented by a network \\nwith this single-layer structure. In this case, the algorithm adds a hidden unit, \\nchoosing its weight values to maximize the correlation between the hidden unit \\nvalue and the residual error of the overall network. The new unit is now installed \\ninto \\nthe network, with its weight values held fixed, and a new connection from \\nthis \\nnew unit is added to each output unit. The process is now repeated. The \\noriginal weights are retrained (holding the hidden unit weights fixed), the residual \\nerror is checked, and a second hidden unit added if the residual error is still above \\nthreshold. Whenever a new hidden unit is added, its inputs include all of the orig- \\ninal network inputs plus the outputs of any existing hidden units. The network is '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 133}, page_content='122 MACHINE LEARNING \\ngrown in this fashion, accumulating hidden units until the network residual enor \\nis reduced to some acceptable level. \\nFahlman \\nand Lebiere (1990) report cases in \\nwhich CASCADE-CORRELATION significantly reduces training times, due to the fact \\nthat only a single layer of units is trained at each step. One practical difficulty \\nis that because the algorithm can add units indefinitely, it is quite easy for it to \\noverfit \\nthe training data, and precautions to avoid overfitting must be taken. \\nA second idea for dynamically altering network structure is to take the \\nopposite approach. Instead of beginning with the simplest possible network and \\nadding complexity, we begin with a complex network and prune it as we find that \\ncertain connections are inessential. One way to decide whether a particular weight \\nis inessential is to see whether its value is close to zero. A second way, which \\nappears to be more successful in practice, is to consider the effect that a small \\nvariation in the weight has on the error E. The effect on E of varying w (i.e., g) \\ncan be taken as a measure of the salience of the connection. LeCun et al. (1990) \\ndescribe a process in which a network is trained, the least salient connections \\nremoved, and this process iterated until some termination condition is met. They \\nrefer to this as the \"optimal brain damage\" approach, because at each step the \\nalgorithm attempts to remove the least useful connections. They report that in \\na character recognition application this approach reduced the number of weights \\nin a large network by a factor of 4, with a slight improvement in generalization \\naccuracy and a significant improvement in subsequent training efficiency. \\nIn general, techniques for dynamically modifying network structure have \\nmet with mixed success. It remains to be seen whether they can reliably improve \\non the generalization accuracy of BACKPROPAGATION. However, they have been \\nshown in some cases to provide significant improvements in training times. \\n4.9 SUMMARY AND FURTHER READING \\nMain points of this chapter include: \\n0 Artificial neural network learning provides a practical method for learning \\nreal-valued and vector-valued functions over continuous and discrete-valued \\nattributes, in a way that is robust to noise in the training data. The \\nBACKPROP- \\nAGATION algorithm is the most common network learning method and has \\nbeen successfully applied to a variety of learning tasks, such as handwriting \\nrecognition and robot control. \\n0 The hypothesis space considered by the BACKPROPAGATION algorithm is the \\nspace of all functions that can be represented by assigning weights to the \\ngiven, fixed network of interconnected units. \\nFeedforward \\nnetworks contain- \\ning three layers of units are able to approximate any function to arbitrary \\naccuracy, given a sufficient (potentially very large) number of units in each \\nlayer. Even networks of practical size are capable of representing a rich \\nspace of highly nonlinear functions, making feedforward networks a good \\nchoice for learning discrete and continuous functions whose general form is \\nunknown in advance. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 134}, page_content='BACKPROPAGATION searches the space of possible hypotheses using gradient \\ndescent to iteratively reduce the error in the network fit to the training \\nexamples. Gradient descent converges to a local minimum in the training \\nerror with respect to the network weights. More generally, gradient descent is \\na potentially useful method for searching many continuously parameterized \\nhypothesis spaces where the training error is a differentiable function of \\nhypothesis parameters. \\nOne of the most intriguing properties of BACKPROPAGATION is its ability to \\ninvent new features that are not explicit in the input to the network. In par- \\nticular, the internal (hidden) layers of multilayer networks learn to represent \\nintermediate features that are useful for learning the target function and that \\nare only implicit in the network inputs. This capability is illustrated, for ex- \\nample, by the ability of the 8 x 3 x 8 network in Section 4.6.4 to invent the \\nboolean encoding of digits from 1 to 8 and by the image features represented \\nby the hidden layer in the face-recognition application of Section 4.7. \\nOverfitting the training data is an important issue in ANN learning. Overfit- \\nting results in networks that generalize poorly to new data despite excellent \\nperformance over the training data. Cross-validation methods can be used to \\nestimate an appropriate stopping point for gradient descent search and thus \\nto minimize the risk of overfitting. \\n0 Although BACKPROPAGATION is the most common ANN learning algorithm, \\nmany others have been proposed, including algorithms for more specialized \\ntasks. For example, recurrent neural network methods train networks con- \\ntaining directed cycles, and algorithms such as CASCADE CORRELATION alter \\nthe network structure as well as the network weights. \\nAdditional information on ANN learning can be found in several other chap- \\nters in this book. A Bayesian justification for choosing to minimize the sum of \\nsquared errors is given in Chapter 6, along with a justification for minimizing \\nthe cross-entropy instead of the sum of squared errors in other cases. Theoretical \\nresults characterizing the number of training examples needed to reliably learn \\nboolean functions and the Vapnik-Chervonenkis dimension of certain types of \\nnetworks can be found in Chapter 7. A discussion of overfitting and how to avoid \\nit can be found in Chapter 5. Methods for using prior knowledge to improve the \\ngeneralization accuracy of ANN learning are discussed in Chapter 12. \\nWork on artificial neural networks dates back to the very early days of \\ncomputer science. \\nMcCulloch \\nand Pitts (1943) proposed a model of a neuron \\nthat \\ncorresponds to the perceptron, and a good deal of work through the 1960s \\nexplored variations of this model. During the early 1960s \\nWidrow \\nand Hoff (1960) \\nexplored perceptron networks (which they called \"adelines\") and the delta rule, \\nand \\nRosenblatt (1962) proved the convergence of the perceptron training rule. \\nHowever, by the late 1960s it became clear that single-layer perceptron networks \\nhad limited representational capabilities, and no effective algorithms were known \\nfor training multilayer networks. Minsky and \\nPapert \\n(1969) showed that even '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 135}, page_content='simple functions such as XOR could not be represented or learned with single- \\nlayer perceptron networks, and work on ANNs receded during the 1970s. \\nDuring the mid-1980s work on \\nANNs \\nexperienced a resurgence, caused in \\nlarge part by the invention of \\nBACKPROPAGATION \\nand related algorithms for train- \\ning multilayer networks (Rumelhart and McClelland 1986; Parker 1985). These \\nideas can be traced to related earlier work \\n(e.g., \\nWerbos 1975). Since the \\n1980s, \\nBACKPROPAGATION \\nhas become a widely used learning method, and many other \\nANN approaches have been actively explored. The advent of inexpensive com- \\nputers during this same period has allowed experimenting with computationally \\nintensive algorithms that could not be thoroughly explored during the 1960s. \\nA number of textbooks are devoted to the topic of neural network learning. \\nAn early but still useful book on parameter learning methods for pattern recog- \\nnition is Duda and Hart (1973). The text by \\nWidrow \\nand \\nStearns \\n(1985) covers \\nperceptrons and related single-layer networks and their applications. Rumelhart \\nand McClelland (1986) produced an edited collection of papers that helped gen- \\nerate the increased interest in these methods beginning in the mid-1980s. Recent \\nbooks on neural network learning include Bishop (1996); Chauvin and Rumelhart \\n(1995); Freeman and Skapina (1991); Fu (1994); Hecht-Nielsen (1990); and Hertz \\net al. (1991). \\nEXERCISES \\n4.1. What are the values of weights wo, wl, and w2 for the perceptron whose decision \\nsurface is illustrated in Figure 4.3? Assume the surface crosses the xl axis at -1, \\nand the x2 axis at 2. \\n4.2. Design a two-input perceptron that implements the boolean function A A -. B. Design \\na two-layer network of perceptrons that implements A XO R B. \\n4.3. Consider two perceptrons defined by the threshold expression wo + wlxl+ ~2x2 > 0. \\nPerceptron A has weight values \\nand perceptron B has the weight values \\nTrue or false? Perceptron A is more-general~han perceptron B. (more-general~han \\nis defined in Chapter 2.) \\n4.4. Implement the delta training rule for a two-input linear unit. Train it to fit the target \\nconcept -2 + XI+ 2x2 > 0. Plot the error E as a function of the number of training \\niterations. Plot the decision surface after 5, 10, 50, 100, . . . , iterations. \\n(a) Try this using various constant values for 17 and using a decaying learning rate \\nof qo/i for the ith iteration. Which works better? \\n(b) Try incremental and batch learning. Which converges more quickly? Consider \\nboth number of weight updates and total execution time. \\n4.5. Derive a gradient descent training rule for a single unit with output o, where '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 136}, page_content='4.6. Explain informally why the delta training rule in Equation (4.10) is only an approx- \\nimation to the true gradient descent rule of Equation (4.7). \\n4.7. Consider a two-layer feedforward ANN with two inputs a and b, one hidden unit c, \\nand one output unit d. This network has five weights (w,, web, wd, wdc, wdO), where \\nw,o \\nrepresents the threshold weight for unit x. Initialize these weights to the values \\n(. 1, .l, .l, .l, .I), then give their values after each of the first two training iterations of \\nthe BACKPROPAGATION algorithm. Assume learning rate 17 = .3, momentum a! = 0.9, \\nincremental weight updates, and the following training examples: \\nabd \\n101 \\n010 \\n4.8. Revise the BACKPROPAGATION algorithm in Table 4.2 so that it operates on units \\nusing the squashing function tanh in place of the sigmoid function. That is, assume \\nthe output of a single unit is o = tanh(6.x\\'). Give the weight update rule for output \\nlayer weights and hidden layer weights. Hint: tanh\\'(x) = 1 - tanh2(x). \\n4.9. Recall the 8 x 3 x 8 network described in Figure 4.7. Consider trying to train a 8 x 1 x 8 \\nnetwork for the same task; that is, a network with just one hidden unit. Notice the \\neight training examples in Figure 4.7 could be represented by eight distinct values for \\nthe single hidden unit \\n(e.g., \\n0.1,0.2, . . . ,0.8). Could a network with just one hidden \\nunit therefore learn the identity function defined over these training examples? Hint: \\nConsider questions such as \"do there exist values for the hidden unit weights that \\ncan create the hidden unit encoding suggested \\nabove?\\'\"do \\nthere exist values for \\nthe output unit weights that could correctly decode this encoding of the \\ninput?\\'and \\n\"is gradient descent likely to find such weights?\\' \\n4.10. Consider the alternative error function described in Section 4.8.1 \\nDerive the gradient descent update rule for this definition of E. Show that it can be \\nimplemented by multiplying each weight by some constant before performing the \\nstandard gradient descent update given in Table 4.2. \\n4.11. Apply BACKPROPAGATION to the task of face recognition. See World Wide Web \\nURL http://www.cs.cmu.edu/-tomlbook.html for details, including face-image data, \\nBACKPROPAGATION code, and specific tasks. \\n4.12. Consider deriving a gradient descent algorithm to learn target concepts corresponding \\nto rectangles in the x, y plane. Describe each hypothesis by the x and y coordinates \\nof the lower-left and upper-right comers of the rectangle - Ilx, Ily, urn, and ury \\nrespectively. An instance (x, y) is labeled positive by hypothesis (llx, lly, urx, ury) \\nif and only if the point (x, y) lies inside the corresponding rectangle. Define error \\nE as in the chapter. Can you devise a gradient descent algorithm to learn such \\nrectangle hypotheses? Notice that E is not a continuous function of llx, Ily, urx, \\nand ury, just as in the case of perceptron learning. (Hint: Consider the two solutions \\nused for perceptrons: (1) changing the classification rule to make output predictions \\ncontinuous functions of the inputs, and (2) defining an alternative error-such as \\ndistance to the rectangle center-as in using the delta rule to train perceptrons.) \\nDoes your algorithm converge to the \\nminimum \\nerror hypothesis when the positive \\nand negative examples are separable by a rectangle? When they are not? Do you '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 137}, page_content='have problems with local minima? How does your algorithm compare to symbolic \\nmethods for learning conjunctions of feature constraints? \\nREFERENCES \\nBishop, C. M. (1996). Neural networks for pattern recognition. Oxford, England: Oxford University \\nPress. \\nChauvin, Y., & Rumelhart, D. (1995). BACKPROPAGATION: Theory, architectures, and applications \\n(edited collection). Hillsdale, NJ: Lawrence Erlbaum Assoc. \\nChurchland, \\nP. S., & Sejnowski, T. J. (1992). The computational brain. Cambridge, MA: The MIT \\nPress. \\nCyhenko, \\nG. (1988). Continuous valued neural networks with two hidden layers are sufficient (Tech- \\nnical Report). Department of Computer Science, Tufts University, Medford, MA. \\nCybenko, \\nG. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Con- \\ntrol, Signals, and Systems, 2, 303-3 14. \\nCottrell, G. W. (1990). Extracting features from faces using compression networks: Face, identity, \\nemotion and gender recognition using holons. In D. Touretzky (Ed.), Connection Models: \\nProceedings of the 1990 Summer School. San Mateo, CA: Morgan Kaufmann. \\nDietterich, T. G., Hild, H., & Bakiri, G. (1995). A comparison of ID3 and BACKPROPAGATION for \\nEnglish text-to-speech mapping. Machine Learning, 18(1), 51-80. \\nDuda, R., & Hart, P. (1973). Pattern class@cation and scene analysis. New York: John Wiley & \\nSons. \\nElman, \\nJ. L. (1990). Finding structure in time. Cognitive Science, 14, 179-21 1. \\nFahlman, \\nS., & Lebiere, C. (1990). The CASCADE-CORRELATION learning architecture (Technical \\nReport CMU-CS-90-100). Computer Science Department, \\nCarnegie \\nMellon University, Pitts- \\nburgh, PA. \\nFreeman, J. A., & Skapura, D. M. (1991). Neural networks. Reading, MA: Addison Wesley. \\nFu, L. (1994). Neural networks in computer intelligence. New York: McGraw Hill. \\nGabriel, M. & Moore, J. (1990). Learning and computational neuroscience: Foundations of adaptive \\nnetworks (edited collection). Cambridge, MA: The MIT Press. \\nHecht-Nielsen, R. (1990). Neurocomputing. Reading, MA: Addison Wesley. \\nHertz, J., Krogh, A., & Palmer, R.G. (1991). Introduction to the theory of neural computation. Read- \\ning, MA: Addison Wesley. \\nHomick, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward networks are universal \\napproximators. Neural Networks, 2, 359-366. \\nHuang, W. Y., & Lippmann, R. P. (1988). Neural net and traditional classifiers. In Anderson (Ed.), \\nNeural Information Processing Systems (pp. 387-396). \\nJordan, M. (1986). Attractor dynamics and parallelism in a connectionist sequential machine. Pro- \\nceedings of the Eighth Annual Conference of the Cognitive Science Society (pp. 531-546). \\nKohonen, T. (1984). Self-organization and associative memory. Berlin: Springer-Verlag. \\nLang, K. J., Waibel, A. H., & Hinton, G. E. (1990). A time-delay neural network architecture for \\nisolated word recognition. Neural Networks, 3, 3343. \\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L.D. \\n(1989). BACKPROPAGATION applied to handwritten zip code recognition. Neural Computa- \\ntion, l(4). \\nLeCun, Y., Denker, J. S., & Solla, S. A. (1990). Optimal brain damage. In D. Touretzky (Ed.), \\nAdvances in Neural Information Processing Systems (Vol. 2, pp. 598405). San Mateo, CA: \\nMorgan Kaufmann. \\nManke, \\nS., Finke, M. & Waibel, A. (1995). NPEN++: a writer independent, large vocabulary on- \\nline cursive handwriting recognition system. Proceedings of the International Conference on \\nDocument Analysis and Recognition. Montreal, Canada: IEEE Computer Society. \\nMcCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. \\nBulletin of Mathematical Biophysics, 5, 115-133. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 138}, page_content='Mitchell, T. M., & Thrun, S. B. (1993). Explanation-based neural network learning for robot control. \\nIn Hanson, \\nCowan, & Giles (Eds.), Advances in neural informution processing systems 5 \\n(pp. 287-294). San Francisco: Morgan Kaufmann. \\nMozer, \\nM. \\n(1995). A focused \\nBACKPROPAGATION \\nalgorithm for temporal pattern recognition. In \\nY. Chauvin & D. Rumelhart (Eds.), Backpropagation: Theory, architectures, and applications \\n(pp. 137-169). Hillsdale, NJ: Lawrence Erlbaum Associates. \\nMinsky, \\nM., & Papert, S. (1969). Perceptrons. Cambridge, MA: MIT Press. \\nNilsson, \\nN. J. (1965). Learning machines. New York: McGraw Hill. \\nParker, D. (1985). Learning logic (MIT Technical Report TR-47). MIT Center for Research in \\nComputational Economics and Management Science. \\npomerleau, \\nD. A. (1993). Knowledge-based training of artificial neural networks for autonomous \\nrobot driving. In J. \\nCome11 & S. Mahadevan (Eds.), Robot Learning (pp. 19-43). Boston: \\nKluwer \\nAcademic Publishers. \\nRosenblatt, F. (1959). The perceptron: a probabilistic model for information storage and organization \\nin the brain. Psychological Review, 65, 386-408. \\nRosenblatt, F. (1962). Principles of neurodynamics. New York: Spartan Books. \\nRumelhart, D. E., & McClelland, J. L. (1986). Parallel distributed processing: exploration in the \\nmicrostructure of cognition (Vols. 1 & 2). Cambridge, MA: MIT Press. \\nRumelhart, D., \\nWidrow, \\nB., & Lehr, M. (1994). The basic ideas in neural networks. Communications \\nof the ACM, 37(3), 87-92. \\nShavlik, J. W., Mooney, R. J., & Towell, G. G. (1991). Symbolic and neural learning algorithms: \\nAn experimental comparison. Machine Learning, 6(2), 11 1-144. \\nSimard, \\nP. S., Victorri, B., \\nLeCun, \\nY., & Denker, J. (1992). Tangent prop--A formalism for specifying \\nselected invariances in an adaptive network. In Moody, et al. (Eds.), Advances in Neural \\nInformation Processing Systems 4 (pp. 895-903). San Francisco: Morgan Kaufmann. \\nWaibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. (1989). Phoneme recognition using \\ntime-delay neural networks. ZEEE Transactions on Acoustics, Speech and Signal Processing. \\nWeiss, S., & Kapouleas, I. (1989). An empirical comparison of pattern recognition, neural nets, and \\nmachine learning classification methods. Proceedings of the Eleventh ZJCAI @p. 781-787). \\nSan Francisco: Morgan Kaufmann. \\nWerbos, P. (1975). Beyond regression: New tools for prediction and analysis in the behavioral sciences \\n(Ph.D. dissertation). Harvard University. \\nWidrow, \\nB., & Hoff, M. E. (1960). Adaptive switching circuits. IRE WESCON Convention Record, \\n4,96104. \\nWidrow, \\nB., & Stearns, S. D. (1985). Adaptive signalprocessing. Signal Processing Series. Englewood \\nCliffs, NJ: Prentice Hall. \\nWilliams, R., & Zipser, D. (1995). Gradient-based learning algorithms for recurrent networks and their \\ncomputational complexity. In Y. Chauvin & D. Rumelhart (Eds.), Backpropagation: Theory, \\narchitectures, and applications (pp. 433-486). Hillsdale, NJ: Lawrence Erlbaum Associates. \\nZometzer, \\nS. F., Davis, J. L., & Lau, C. (1994). An introduction to neural and electronic neiworks \\n(edited collection) (2nd ed.). New York: Academic Press. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 139}, page_content='CHAPTER \\nEVALUATING \\nHYPOTHESES \\nEmpirically evaluating the accuracy of hypotheses is fundamental to machine learn- \\ning. This chapter presents an introduction to statistical methods for estimating hy- \\npothesis accuracy, focusing on three questions. First, given the observed accuracy \\nof a hypothesis over a limited sample of data, how well does this estimate its ac- \\ncuracy over additional examples? Second, given that one hypothesis outperforms \\nanother over some sample of data, how probable is it that this hypothesis is more \\naccurate in general? Third, when data is limited what is the best way to use this \\ndata to both learn a hypothesis and estimate its accuracy? Because limited samples \\nof data might misrepresent the general distribution of data, estimating true accuracy \\nfrom such samples can be misleading. Statistical methods, together with assump- \\ntions about the underlying distributions of data, allow one to bound the difference \\nbetween observed accuracy over the sample of available data and the true accuracy \\nover the entire distribution of data. \\n5.1 MOTIVATION \\nIn many cases it is important to evaluate the performance of learned hypotheses \\nas precisely as possible. One reason is simply to understand whether to use the \\nhypothesis. For instance, when learning from a limited-size database indicating \\nthe effectiveness of different medical treatments, it is important to understand as \\nprecisely as possible the accuracy of the learned hypotheses. A second reason is \\nthat evaluating hypotheses is an integral component of many learning methods. \\nFor example, in post-pruning decision trees to avoid \\noverfitting, we must evaluate '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 140}, page_content='the impact of possible pruning steps on the accuracy of the resulting decision tree. \\nTherefore it is important to understand the likely errors inherent in estimating the \\naccuracy of the pruned and unpruned tree. \\nEstimating the accuracy of a hypothesis is relatively straightforward when \\ndata is plentiful. However, when we must learn a hypothesis and estimate its \\nfuture accuracy given only a limited set of data, two key difficulties arise: \\nBias in the estimate. First, the observed accuracy of the learned hypothesis \\nover the training examples is often a poor estimator of its accuracy over \\nfuture examples. Because the learned hypothesis was derived from these \\nexamples, they will typically provide an optimistically biased estimate of \\nhypothesis accuracy over future examples. This is especially likely when \\nthe learner considers a very rich hypothesis space, enabling it to \\noverfit \\nthe \\ntraining examples. To obtain an unbiased estimate of future accuracy, we \\ntypically test the hypothesis on some set of test examples chosen indepen- \\ndently of the training examples and the hypothesis. \\na Variance in the estimate. Second, even if the hypothesis accuracy is mea- \\nsured over an unbiased set of test examples independent of the training \\nexamples, the measured accuracy can still vary from the true accuracy, de- \\npending on the makeup of the particular set of test examples. The smaller \\nthe set of test examples, the greater the expected variance. \\nThis chapter discusses methods for evaluating learned hypotheses, methods \\nfor comparing the accuracy of two hypotheses, and methods for comparing the \\naccuracy of two learning algorithms when only limited data is available. Much \\nof the discussion centers on basic principles from statistics and sampling theory, \\nthough the chapter assumes no special background in statistics on the part of the \\nreader. The literature on statistical tests for hypotheses is very large. This chapter \\nprovides an introductory overview that focuses only on the issues most directly \\nrelevant to learning, evaluating, and comparing hypotheses. \\n5.2 ESTIMATING HYPOTHESIS ACCURACY \\nWhen evaluating a learned hypothesis we are most often interested in estimating \\nthe accuracy with which it will classify future instances. At the same time, we \\nwould like to know the probable error in this accuracy estimate \\n(i.e., \\nwhat error \\nbars to associate with this estimate). \\nThroughout this chapter we consider the following setting for the learning \\nproblem. There is some space of possible instances X (e.g., the set of all people) \\nover which various target functions may be defined \\n(e.g., \\npeople who plan to \\npurchase new skis this year). We assume that different instances in X may be en- \\ncountered with different frequencies. A convenient way to model this is to assume \\nthere is some unknown probability distribution D that defines the probability of \\nencountering each instance in X (e-g., 23 might assign a higher probability to en- \\ncountering 19-year-old people than 109-year-old people). Notice 23 says nothing '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 141}, page_content='about whether x is a positive or negative example; it only detennines the proba- \\nbility that x will be encountered. The learning task is to learn the target concept \\nor target function f by considering a space H of possible hypotheses. Training \\nexamples of the target function f are provided to the learner by a trainer who \\ndraws each instance independently, according to the distribution \\nD, \\nand who then \\nforwards the instance x along with its correct target value f (x) to the learner. \\nTo illustrate, consider learning the target function \"people who plan to pur- \\nchase new \\nskis \\nthis year,\" given a sample of training data collected by surveying \\npeople as they arrive at a ski resort. In this case the instance space X is the space \\nof all people, who might be described by attributes such as their age, occupation, \\nhow many times they \\nskied \\nlast year, etc. The distribution \\nD \\nspecifies for each \\nperson x the probability that x will be encountered as the next person arriving at \\nthe \\nski \\nresort. The target function f : X + {O,1) classifies each person according \\nto whether or not they plan to purchase skis this year. \\nWithin this general setting we are interested in the following two questions: \\n1. Given a hypothesis h and a data sample containing n examples drawn at \\nrandom according to the distribution \\nD, \\nwhat is the best estimate of the \\naccuracy of h over future instances drawn from the same distribution? \\n2. What is the probable error in this accuracy estimate? \\n5.2.1 Sample Error and True Error \\nTo answer these questions, we need to distinguish carefully between two notions \\nof accuracy or, equivalently, error. One is the error rate of the hypothesis over the \\nsample of data that is available. The other is the error rate of the hypothesis over \\nthe entire unknown distribution D of examples. We will call these the sample \\nerror and the true error respectively. \\nThe sample error of a hypothesis with respect to some sample S of instances \\ndrawn from X is the fraction of S that it misclassifies: \\nDefinition: The sample error (denoted errors(h)) of hypothesis h with respect to \\ntarget function f and data sample S is \\nWhere n is the number of examples in S, and the quantity S(f (x), h(x)) is 1 if \\nf (x) # h(x), and 0 otherwise. \\nThe true error of a hypothesis is the probability that it will misclassify a \\nsingle randomly drawn instance from the distribution D. \\nDefinition: The true error (denoted errorv(h)) of hypothesis h with respect to target \\nfunction f and distribution D, is the probability that h will misclassify an instance \\ndrawn at random according to D. \\nerrorv (h) = Pr [ f (x) # h(x)] \\nXED '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 142}, page_content='Here the notation Pr denotes that the probability is taken over the instance \\nXGV \\ndistribution V. \\nWhat we usually wish to know is the true error errorv(h) of the hypothesis, \\nbecause this is the error we can expect when applying the hypothesis to future \\nexamples. All we can measure, however, is the sample error errors(h) of the \\nhypothesis for the data sample S that we happen to have in hand. The main \\nquestion considered in this section is \"How good an estimate of errorD(h) is \\nprovided by errors (h)?\" \\n5.2.2 Confidence Intervals for Discrete-Valued Hypotheses \\nHere we give an answer to the question \"How good an estimate of errorv(h) is \\nprovided by errors(h)?\\' for the case in which h is a discrete-valued hypothesis. \\nMore specifically, suppose we wish to estimate the true error for some \\ndiscrete- \\nvalued hypothesis h, based on its observed sample error over a sample S, where \\n0 the sample S contains n examples drawn independent of one another, and \\nindependent of h, according to the probability distribution V \\n0 nz30 \\n0 hypothesis h commits r errors over these n examples (i.e., errors(h) = rln). \\nUnder these conditions, statistical theory allows us to make the following asser- \\ntions: \\n1. Given no other information, the most probable value of errorD(h) is errors(h) \\n2. With approximately 95% probability, the true error errorv(h) lies in the \\ninterval \\nerrors(h)(l - errors (h)) errors(h) f 1.96 7 \\nTo illustrate, suppose the data sample S contains n = 40 examples and that \\nhypothesis h commits r = 12 errors over this data. In this case, the sample error \\nerrors(h) = 12/40 = .30. Given no other information, the best estimate of the true \\nerror errorD(h) is the observed sample error .30. However, we do not expect this \\nto be a perfect estimate of the true error. If we were to collect a second sample \\nS\\' containing 40 new randomly drawn examples, we might expect the sample \\nerror errors,(h) to vary slightly from the sample error errors(h). We expect a \\ndifference due to the random differences in the makeup of S and S\\'. In fact, if \\nwe repeated this experiment over and over, each time drawing a new sample \\nS, containing 40 new examples, we would find that for approximately 95% of \\nthese experiments, the calculated interval would contain the true error. For this \\nreason, we call this interval the 95% confidence interval estimate for errorv(h). \\nIn the current example, where r = 12 and n = 40, the 95% confidence interval is, \\naccording to the above expression, 0.30 f (1.96 - .07) = 0.30 f .14. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 143}, page_content='ConfidencelevelN%: 50% 68% 80% 90% 95% 98% 99% \\nConstant ZN: 0.67 1.00 1.28 1.64 1.96 2.33 2.58 \\nTABLE 5.1 \\nValues of z~ for two-sided N% confidence intervals. \\nThe above expression for the 95% confidence interval can be generalized to \\nany desired confidence level. The constant 1.96 is used in case we desire a 95% \\nconfidence interval. A different constant, ZN, is used to calculate the N% confi- \\ndence interval. The general expression for approximate N% confidence intervals \\nfor errorv(h) is \\nwhere the constant ZN is chosen depending on the desired confidence level, using \\nthe values of z~ given in Table 5.1. \\nThus, just as we could calculate the 95% confidence interval for errorv(h) to \\nbe 0.305 (1.96. .07) (when r = 12, n = 40), we can calculate the 68% confidence \\ninterval in this case to be 0.30 f (1.0 - .07). Note it makes intuitive sense that the \\n68% confidence interval is smaller than the 95% confidence interval, because we \\nhave reduced the probability with which we demand that errorv(h) fall into the \\ninterval. \\nEquation (5.1) describes how to calculate the confidence intervals, or error \\nbars, for estimates of errorv(h) that are based on errors(h). In using this ex- \\npression, it is important to keep in mind that this applies only to discrete-valued \\nhypotheses, that it assumes the sample S is drawn at random using the same \\ndistribution from which future data will be drawn, and that it assumes the data \\nis independent of the hypothesis being tested. We should also keep in mind that \\nthe expression provides only an approximate confidence interval, though the ap- \\nproximation is quite good when the sample contains at least 30 examples, and \\nerrors(h) is not too close to 0 or 1. A more accurate rule of thumb is that the \\nabove approximation works well when \\nAbove we summarized the procedure for calculating confidence intervals for \\ndiscrete-valued hypotheses. The following section presents the underlying statis- \\ntical justification for this procedure. \\n5.3 BASICS OF SAMPLING THEORY \\nThis section introduces basic notions from statistics and sampling theory, in- \\ncluding probability distributions, expected value, variance, Binomial and Normal \\ndistributions, and two-sided and one-sided intervals. A basic familiarity with these '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 144}, page_content='a A random variable can be viewed as the name of an experiment with a probabilistic outcome. Its \\nvalue is the outcome of the experiment. \\nA probability distribution for a random variable Y specifies the probability Pr(Y = yi) that Y will \\ntake on the value yi, for each possible value yi. \\nThe expected value, or mean, of a random variable Y is E[Y] = Ci yi Pr(Y = yi). The symbol \\np)~ is commonly used to represent E[Y]. \\nThe variance of a random variable is Var(Y) = E[(Y - p~)~]. The variance characterizes the \\nwidth or dispersion of the distribution about its mean. \\na The standard deviation of Y is JVar(Y). The symbol uy is often used used to represent the \\nstandard deviation of Y. \\nThe Binomial distribution gives the probability of observing r heads in a series of n independent \\ncoin tosses, if the probability of heads in a single toss is p. \\na The Normal distribution is a bell-shaped probability distribution that covers many natural \\nphenomena. \\nThe Central Limit Theorem is a theorem stating that the sum of a large number of independent, \\nidentically distributed random variables approximately follows a Normal distribution. \\nAn estimator is a random variable Y used to estimate some parameter p of an underlying popu- \\nlation. \\na The estimation bias of Y as an estimator for p is the quantity (E[Y] - p). An unbiased estimator \\nis one for which the bias is zero. \\na A N% conjidence interval estimate for parameter p is an interval that includes p with probabil- \\nity N%. \\nTABLE 5.2 , \\nBasic definitions and facts from statistics. \\nconcepts is important to understanding how to evaluate hypotheses and learning \\nalgorithms. Even more important, these same notions provide an important con- \\nceptual framework for understanding machine learning issues such as \\noverfitting \\nand the relationship between successful generalization and the number of training \\nexamples considered. The reader who is already familiar with these notions may \\nskip or skim this section without loss of continuity. The key concepts introduced \\nin this section are summarized in Table 5.2. \\n5.3.1 Error Estimation and Estimating Binomial Proportions \\nPrecisely how does the deviation between sample error and true error depend \\non the size of the data sample? This question is an instance of a well-studied \\nproblem in statistics: the problem of estimating the proportion of a population that \\nexhibits some property, given the observed proportion over some random sample \\nof the population. In our case, the property of interest is that h misclassifies the \\nexample. \\nThe key to answering this question is to note that when we measure the \\nsample error we are performing an experiment with a random outcome. We first \\ncollect a random sample S of n independently drawn instances from the distribu- \\ntion D, and then measure the sample error errors(h). As noted in the previous '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 145}, page_content='section, if we were to repeat this experiment many times, each time drawing a \\ndifferent random sample Si of size n, we would expect to observe different values \\nfor the various \\nerrors,(h), \\ndepending on random differences in the makeup of \\nthe various Si. We say in such cases that errors, (h), the outcome of the ith such \\nexperiment, is a random variable. In general, one can think of a random variable \\nas the name of an experiment with a random outcome. The value of the random \\nvariable is the observed outcome of the random experiment. \\nImagine that we were to run k such random experiments, measuring the ran- \\ndom variables errors, (h), errors, (h) . . . errors, (h). Imagine further that we then \\nplotted a histogram displaying the frequency with which we observed each possi- \\nble error value. As we allowed k to grow, the histogram would approach the form \\nof the distribution shown in Table 5.3. This table describes a particular probability \\ndistribution called the Binomial distribution. \\nBinomial dishibution for n = 40, p =0.3 \\n0.14 \\n0.12 \\n0.1 \\n0.08 \\n\\'F 0.06 \\n0.04 \\n0.02 \\n0 \\n0 5 10 15 20 25 30 35 40 \\nA Binomial distribution gives the probability of observing r heads in a sample of n independent \\ncoin tosses, when the probability of heads on a single coin toss is p. It is defined by the probability \\nfunction \\nn! P(r) = - pr(l - p)\"-\\' r!(n - r)! \\nIf the random variable X follows a Binomial distribution, then: \\n0 The probability Pr(X = r) that X will take on the value r is given by P(r) \\n0 The expected, or mean value of X, E[X], is \\n0 The variance of X, Var(X), is \\nVar (X) = np(1- p) \\n0 The standard deviation of X, ax, is \\nFor sufficiently large values of n the Binomial distribution is closely approximated by a Normal \\ndistribution (see Table 5.4) with the same mean and variance. Most statisticians recommend using \\nthe Normal approximation only when np(1- p) 2 5. \\nTABLE 53 \\nThe Binomial distribution. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 146}, page_content='5.3.2 The Binomial Distribution \\nA good way to understand the Binomial distribution is to consider the following \\nproblem. You are given a worn and bent coin and asked to estimate the probability \\nthat the coin will turn up heads when tossed. Let us call this unknown probability \\nof heads p. You toss the coin n times and record the number of times r that it \\nturns up heads. A reasonable estimate of p is rln. Note that if the experiment \\nwere rerun, generating a new set of n coin tosses, we might expect the number \\nof heads r to vary somewhat from the value measured in the first experiment, \\nyielding a somewhat different estimate for p. The Binomial distribution describes \\nfor each possible value of r \\n(i.e., \\nfrom 0 to n), the probability of observing exactly \\nr heads given a sample of n independent tosses of a coin whose true probability \\nof heads is p. \\nInterestingly, estimating p from a random sample of coin tosses is equivalent \\nto estimating \\nerrorv(h) \\nfrom testing h on a random sample of instances. A single \\ntoss of the coin corresponds to drawing a single random instance from 23 and \\ndetermining whether it is misclassified by h. The probability p that a single random \\ncoin toss will turn up heads corresponds to the probability that a single instance \\ndrawn at random will be misclassified \\n(i.e., \\np corresponds to \\nerrorv(h)). \\nThe \\nnumber r of heads observed over a sample of n coin tosses corresponds to the \\nnumber of misclassifications observed over n randomly drawn instances. Thus \\nrln \\ncorresponds to \\nerrors(h). \\nThe problem of estimating p for coins is identical to \\nthe problem of estimating \\nerrorv(h) \\nfor hypotheses. The Binomial distribution \\ngives the general form of the probability distribution for the random variable r, \\nwhether it represents the number of heads in n coin tosses or the number of \\nhypothesis errors in a sample of n examples. The detailed form of the Binomial \\ndistribution depends on the specific sample size n and the specific probability p \\nor \\nerrorv(h). \\nThe general setting to which the Binomial distribution applies is: \\n1. There is a base, or underlying, experiment (e.g., toss of the coin) whose \\noutcome can be described by a random variable, say Y. The random variable \\nY can take on two possible values (e.g., Y = 1 if heads, Y = 0 if tails). \\n2. The probability that Y = 1 on any single trial of the underlying experiment \\nis given by some constant p, independent of the outcome of any other \\nexperiment. The probability that Y = 0 is therefore (1 - p). Typically, p is \\nnot known in advance, and the problem is to estimate it. \\n3. A series of n independent trials of the underlying experiment is performed \\n(e.g., \\nn independent coin tosses), producing the sequence of independent, \\nidentically distributed random variables Yl, Yz, . . . , Yn. Let R denote the \\nnumber of trials for which Yi = 1 in this series of n experiments '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 147}, page_content='4. The probability that the random variable R will take on a specific value r \\n(e.g., the probability of observing exactly r heads) is given by the Binomial \\ndistribution \\nn! \\nPr(R = r) = pr(l - p)\"-\\' r!(n - r)! \\nA plot of this probability distribution is shown in Table 5.3. \\nThe Binomial distribution characterizes the probability of observing r heads from \\nn coin flip experiments, as well as the probability of observing r errors in a data \\nsample containing n randomly drawn instances. \\n5.3.3 Mean and Variance \\nTwo properties of a random variable that are often of interest are its expected \\nvalue (also called its mean value) and its variance. The expected value \\nis_the \\naverage of the values taken on by repeatedly sampling the random variable. More \\nprecisely \\nDefinition: Consider a random variable Y that takes on the possible values yl, . . . yn. \\nThe expected value of Y, E[Y], is \\nFor example, if Y takes on the value 1 with probability .7 and the value 2 with \\nprobability .3, then its expected value is (1 \\n.0.7 + 2.0.3 = 1.3). In case the random \\nvariable Y is governed by a Binomial distribution, then it can be shown that \\nE [Y] = np (5.4) \\nwhere n and p are the parameters of the Binomial distribution defined in Equa- \\ntion (5.2). \\nA second property, the variance, captures the \"width or \"spread\" of the \\nprobability distribution; that is, it captures how far the random variable is expected \\nto vary from its mean value. \\nDefinition: The variance of a random variable Y, Var[Y], is \\nVar[Y] = E[(Y - E[Y])~] (5.5) \\nThe variance describes the expected squared error in using a single obser- \\nvation of Y to estimate its mean E[Y]. The square root of the variance is called \\nthe standard deviation of Y, denoted oy . \\nDefinition: The standard deviation of a random variable Y, uy, is '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 148}, page_content='In case the random variable Y is governed by a Binomial distribution, then the \\nvariance and standard deviation are given by \\n5.3.4 Estimators, Bias, and Variance \\nNow that we have shown that the random variable errors(h) obeys a Binomial \\ndistribution, we return to our primary question: What is the likely difference \\nbetween errors(h) and the true error errorv(h)? \\nLet us describe errors(h) and errorv(h) using the terms in Equation (5.2) \\ndefining the Binomial distribution. We then have \\nwhere n is the number of instances in the sample S, r is the number of instances \\nfrom S misclassified by h, and p is the probability of misclassifying a single \\ninstance drawn from 23. \\nStatisticians call errors(h) an estimator for the true error errorv(h). In \\ngeneral, an estimator is any random variable used to estimate some parameter of \\nthe underlying population from which the sample is drawn. An obvious question \\nto ask about any estimator is whether on average it gives the right estimate. We \\ndefine the estimation bias to be the difference between the expected value of the \\nestimator and the true value of the parameter. \\nDefinition: The estimation bias of an estimator Y for an arbitrary parameter p is \\nIf the estimation bias is zero, we say that Y is an unbiased estimator for p. Notice \\nthis will be the case if the average of many random values of Y generated by \\nrepeated random experiments \\n(i.e., \\nE[Y]) \\nconverges toward p. \\nIs errors(h) an unbiased estimator for errorv(h)? Yes, because for a Bi- \\nnomial distribution the expected value of r is equal to np (Equation r5.41). It \\nfollows, given that n is a constant, that the expected value of rln is p. \\nTwo quick remarks are in order regarding the estimation bias. First, when \\nwe mentioned at the beginning of this chapter that testing the hypothesis on the \\ntraining examples provides an optimistically biased estimate of hypothesis error, \\nit is exactly this notion of estimation bias to which we were referring. In order for \\nerrors(h) to give an unbiased estimate of errorv(h), the hypothesis h and sample \\nS must be chosen independently. Second, this notion of estimation bias should \\nnot be confused with the inductive bias of a learner introduced in Chapter 2. The '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 149}, page_content='estimation bias is a numerical quantity, whereas the inductive bias is a set of \\nassertions. \\nA second important property of any estimator is its variance. Given a choice \\namong alternative unbiased estimators, it makes sense to choose the one with \\nleast variance. By our definition of variance, this choice will yield the smallest \\nexpected squared error between the estimate and the true value of the parameter. \\nTo illustrate these concepts, suppose we test a hypothesis and find that it \\ncommits r = 12 errors on a sample of n = 40 randomly drawn test examples. \\nThen an unbiased estimate for errorv(h) is given by errors(h) = rln = 0.3. \\nThe variance in this estimate arises completely from the variance in r, because \\nn is a constant. Because r is Binomially distributed, its variance is given by \\nEquation (5.7) as np(1 - p). Unfortunately p is unknown, but we can substitute \\nour estimate rln for p. This yields an estimated variance in r of 40. 0.3(1 - \\n0.3) = 8.4, or a corresponding standard deviation of a ;j: 2.9.  his implies \\nthat the standard deviation in errors(h) = rln is approximately 2.9140 = .07. To \\nsummarize, errors(h) in this case is observed to be 0.30, with a standard deviation \\nof approximately 0.07. (See Exercise 5.1 .) \\nIn general, given r errors in a sample of n independently drawn test exam- \\nples, the standard deviation for errors(h) is given by \\nwhich can be approximated by substituting rln = errors(h) for p \\n5.3.5 Confidence Intervals \\nOne common way to describe the uncertainty associated with an estimate is to \\ngive an interval within which the true value is expected to fall, along with the \\nprobability with which it is expected to fall into this interval. Such estimates are \\ncalled conjdence interval estimates. \\nDefinition: An N% confidence interval for some parameter p is an interval that is \\nexpected with probability N% to contain p. \\nFor example, if we observe r = 12 errors in a sample of n = 40 independently \\ndrawn examples, we can say with approximately 95% probability that the interval \\n0.30 f 0.14 contains the true error errorv(h). \\nHow can we derive confidence intervals for errorv(h)? The answer lies in \\nthe fact that we know the Binomial probability distribution governing the estima- \\ntor errors(h). The mean value of this distribution is errorV(h), and the standard \\ndeviation is given by Equation (5.9). Therefore, to derive a 95% confidence in- \\nterval, we need only find the interval centered around the mean value errorD(h), '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 150}, page_content='which is wide enough to contain 95% of the total probability under this distribu- \\ntion. This provides an interval surrounding errorv(h) into which errors(h) must \\nfall 95% of the time. Equivalently, it provides the size of the interval surrounding \\nerrordh) into which errorv(h) must fall 95% of the time. \\nFor a given value of N how can we find the size of the interval that con- \\ntains N% of the probability mass? Unfortunately, for the Binomial distribution \\nthis calculation can be quite tedious. Fortunately, however, an easily calculated \\nand very good approximation can be found in most cases, based on the fact that \\nfor sufficiently large sample sizes the Binomial distribution can be closely ap- \\nproximated by the Normal distribution. The Normal distribution, summarized in \\nTable 5.4, is perhaps the most well-studied probability distribution in statistics. \\nAs illustrated in Table 5.4, it is a bell-shaped distribution fully specified by its \\nNormal dismbution with mean 0. standard deviation I \\n3 -2 -1 0 1 2 3 \\nA Normal distribution (also called a Gaussian distribution) is a bell-shaped distribution defined by \\nthe probability density function \\nA Normal distribution is fully determined by two parameters in the above formula: p and a. \\nIf the random variable X follows a normal distribution, then: \\n0 The probability that X will fall into the interval (a, 6) is given by \\nThe expected, or mean value of X, E[X], is \\nThe variance of X, Var(X), is \\nVar(X) = a2 \\n0 The standard deviation of X, ax, is \\nax = a \\nThe Central Limit Theorem (Section 5.4.1) states that the sum of a large number of independent, \\nidentically distributed random variables follows a distribution that is approximately Normal. \\nTABLE 5.4 \\nThe Normal or Gaussian distribution. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 151}, page_content='mean p and standard deviation a. For large n, any Binomial distribution is very \\nclosely approximated by a Normal distribution with the same mean and variance. \\nOne reason that we prefer to work with the Normal distribution is that most \\nstatistics references give tables specifying the size of the interval about the mean \\nthat contains N% of the probability mass under the Normal distribution. This is \\nprecisely the information needed to calculate our N% confidence interval. In fact, \\nTable 5.1 is such a table. The constant ZN given in Table 5.1 defines the width \\nof the smallest interval about the mean that includes N% of the total probability \\nmass under the bell-shaped Normal distribution. More precisely, ZN gives half the \\nwidth of the interval \\n(i.e., \\nthe distance from the mean in either direction) measured \\nin standard deviations. Figure \\n5.l(a) \\nillustrates such an interval for z.80. \\nTo summarize, if a random variable Y obeys a Normal distribution with \\nmean p and standard deviation a, then the measured random value y of Y will \\nfall into the following interval N% of the time \\nEquivalently, the mean p will fall into the following interval N% of the time \\nWe can easily combine this fact with earlier facts to derive the general \\nexpression for N% confidence intervals for discrete-valued hypotheses given in \\nEquation (5.1). First, we know that errors(h) follows a Binomial distribution with \\nmean value error~(h) and standard deviation as given in Equation (5.9). Second, \\nwe know that for sufficiently large sample size n, this Binomial distribution is \\nwell approximated by a Normal distribution. Third, Equation (5.1 1) tells us how \\nto find the N% confidence interval for estimating the mean value of a Normal \\ndistribution. Therefore, substituting the mean and standard deviation of errors(h) \\ninto Equation (5.1 1) yields the expression from Equation (5.1) for N% confidence \\nFIGURE 5.1 \\nA Normal distribution with mean 0, standard deviation 1. (a) With 80% confidence, the value of \\nthe random variable will lie in the two-sided interval \\n[-1.28,1.28]. \\nNote 2.80 = 1.28. With 10% \\nconfidence it will lie to the right of this interval, and with 10% confidence it will lie to the left. \\n(b) With 90% confidence, it will lie in the one-sided interval [-oo, 1.281. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 152}, page_content='intervals for discrete-valued hypotheses \\nJ \\nerrors(h)(l - errors(h)) errors(h) zt ZN n \\nRecall that two approximations were involved in deriving this expression, namely: \\n1. in estimating the standard deviation a of errors(h), we have approximated \\nerrorv(h) \\nby \\nerrors(h) \\n[i.e., \\nin going from Equation (5.8) to \\n(5.9)], \\nand \\n2. the Binomial distribution has been approximated by the Normal distribution. \\nThe common rule of thumb in statistics is that these two approximations are very \\ngood as long as n 2 30, or when np(1- p) 2 5. For smaller values of n it is wise \\nto use a table giving exact values for the Binomial distribution. \\n5.3.6 Two-sided and One-sided Bounds \\nNotice that the above confidence interval is a two-sided bound; that is, it bounds \\nthe estimated quantity from above and from below. In some cases, we will be \\ninterested only in a one-sided bound. For example, we might be interested in the \\nquestion \"What is the probability that \\nerrorz,(h) \\nis at most U?\\' This kind of one- \\nsided question is natural when we are only interested in bounding the maximum \\nerror of h and do not mind if the true error is much smaller than estimated. \\nThere is an easy modification to the above procedure for finding such one- \\nsided error bounds. It follows from the fact that the Normal distribution is \\nsyrnrnet- \\nric \\nabout its mean. Because of this fact, any two-sided confidence interval based on \\na Normal distribution can be converted to a corresponding one-sided interval with \\ntwice the confidence (see Figure \\n5.l(b)). \\nThat is, a \\n100(1- \\na)% \\nconfidence inter- \\nval with lower bound L and upper bound U implies a 100(1- a/2)% confidence \\ninterval with lower bound L and no upper bound. It also implies a 100(1 -a/2)% \\nconfidence interval with upper bound U and no lower bound. Here a corresponds \\nto the probability that the correct value lies outside the stated interval. In other \\nwords, a is the probability that the value will fall into the unshaded region in \\nFigure \\n5.l(a), \\nand \\na/2 \\nis the probability that it will fall into the unshaded region \\nin Figure \\n5.l(b). \\nTo illustrate, consider again the example in which h commits r = 12 errors \\nover a sample of n = 40 independently drawn examples. As discussed above, \\nthis leads to a (two-sided) 95% confidence interval of 0.30 f 0.14. In this case, \\n100(1 - a) = 95%, so a! = 0.05. Thus, we can apply the above rule to say with \\n100(1 - a/2) = 97.5% confidence that errorv(h) is at most 0.30 + 0.14 = 0.44, \\nmaking \\nno assertion about the lower bound on \\nerrorv(h). \\nThus, we have a one- \\nsided error bound on \\nerrorv(h) \\nwith double the confidence that we had in the \\ncorresponding two-sided bound (see Exercise 5.3). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 153}, page_content=\"142 MACHINE LEARNING \\n5.4 A GENERAL APPROACH FOR DERIVING CONFIDENCE \\nINTERVALS \\nThe previous section described in detail how to derive confidence interval es- \\ntimates for one particular case: estimating errorv(h) for a discrete-valued hy- \\npothesis h, based on a sample of n independently drawn instances. The approach \\ndescribed there illustrates a general approach followed in many \\nestima6on \\nprob- \\nlems. In particular, we can see this as a problem of estimating the mean (expected \\nvalue) of a population based on the mean of a randomly drawn sample of size n. \\nThe general process includes the following steps: \\n1. Identify the underlying population parameter p to be estimated, for example, \\nerrorv(h). \\n2. Define the estimator Y (e.g., errors(h)). It is desirable to choose a minimum- \\nvariance, unbiased estimator. \\n3. Determine the probability distribution Vy that governs the estimator Y, in- \\ncluding its mean and variance. \\n4. Determine the N% confidence interval by finding thresholds L and U such \\nthat N% of the mass in the probability distribution Vy falls between L and U. \\nIn later sections of this chapter we apply this general approach to sev- \\neral other estimation problems common in machine learning. First, however, let \\nus discuss a fundamental result from estimation theory called the Central Limit \\nTheorem. \\n5.4.1 Central Limit Theorem \\nOne essential fact that simplifies attempts to derive confidence intervals is the \\nCentral Limit Theorem. Consider again our general setting, in which we observe \\nthe values of n independently drawn random variables Yl . . . Yn that obey the same \\nunknown underlying probability distribution \\n(e.g., n tosses of the same coin). Let \\np denote the mean of the unknown distribution governing each of the Yi and let \\na denote the standard deviation. We say that these variables Yi are independent, \\nidentically distributed random variables, because they describe independent exper- \\niments, each obeying the same underlying probability distribution. In an attempt \\nto estimate the mean p of the distribution governing the Yi, we calculate the sam- \\nple mean = '& Yi (e.g., the fraction of heads among the n coin tosses). \\nThe Central Limit Theorem states that the probability distribution governing Fn \\napproaches a Normal distribution as n + co, regardless of the distribution that \\ngoverns the underlying random variables Yi. Furthermore, the mean of the dis- \\ntribution governing Yn approaches p and the standard deviation approaches k. \\nMore precisely, \\nTheorem 5.1. Central Limit Theorem. Consider a set of independent, identically \\ndistributed random variables Yl . . . Y, governed by an arbitrary probability distribu- \\ntion with mean p and finite variance a2. Define the sample mean, = xy=, Yi. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 154}, page_content='Then as n + co, the distribution governing \\n5 \\napproaches a Normal distribution, with zero mean and standard deviation equal to 1. \\nThis is a quite surprising fact, because it states that we know the form of \\nthe distribution that governs the sample mean ? even when we do not know the \\nform of the underlying distribution that governs the individual Yi that are being \\nobserved! Furthermore, the Central Limit Theorem describes how the mean and \\nvariance of Y can be used to determine the mean and variance of the individual Yi. \\nThe Central Limit Theorem is a very useful fact, because it implies that \\nwhenever we define an estimator that is the mean of some sample \\n(e.g., errors(h) \\nis the mean error), the distribution governing this estimator can be approximated \\nby a Normal distribution for sufficiently large n. If we also know the variance \\nfor this (approximately) Normal distribution, then we can use Equation (5.1 1) to \\ncompute confidence intervals. A common rule of thumb is that we can use the \\nNormal approximation when n 2 30. Recall that in the preceding section we used \\nsuch a Normal distribution to approximate the Binomial distribution that more \\nprecisely describes errors (h) . \\n5.5 DIFFERENCE IN ERROR OF TWO HYPOTHESES \\nConsider the case where we have two hypotheses hl and h2 for some discrete- \\nvalued target function. Hypothesis hl has been tested on a samj4e S1 containing \\nnl randomly drawn examples, and ha has been tested on an indi:pendent sample \\nS2 containing n2 examples drawn from the same distribution. Suppose we wish \\nto estimate the difference d between the true errors of these two hypotheses. \\nWe will use the generic four-step procedure described at the beginning of \\nSection 5.4 to derive a confidence interval estimate for d. Having identified d as \\nthe parameter to be estimated, we next define an estimator. The obvious choice \\nfor an estimator in this case is the difference between the sample errors, which \\nwe denote by 2 \\n,. d = errors, (hl) - errors, (h2) \\nAlthough we will not prove it here, it can be shown that 2 gives an unbiased \\nestimate of d; that is E[C? ] = d. \\nWhat is the probability distribution governing the random variable 2? From \\nearlier sections, we know that for large nl and n2 (e.g., both 2 30), both errors, (hl) \\nand error& (hz) follow distributions that are approximately Normal. Because the \\ndifference of two Normal distributions is also a Normal distribution, 2 will also '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 155}, page_content='144 MACHINE LEARNING \\nr \\nfollow a distribution that is approximately Normal, with mean d. It can also \\nbe shown that the variance of this distribution is the sum of the variances of \\nerrors, (hl) and errors2(h2). Using Equation (5.9) to obtain the approximate vari- \\nance of each of these distributions, we have \\nerrorS, (hl)(l - errors, (hl)) errors2 (h2)(1 - errors,(h2)) 02 , ci + (5.12) n 1 n2 \\nNow that we have determined the probability distribution that governs the esti- \\nmator \\n2, \\nit is straightforward to derive confidence intervals that characterize the \\nlikely error in employing 2 to estimate d. For a random variable 2 obeying a \\nNormal distribution with mean d and variance a2, the N% confidence interval \\nestimate for d is 2 f z~a. Using the approximate variance a; given above, this \\napproximate N% confidence interval estimate for d is \\nJ \\nerrors, (hl)(l - errors, (h 1)) errors2 (h2)(1 - errors2(h2)) dfz~ + (5.13) nl n2 \\nwhere zN is the same constant described in Table 5.1. The above expression gives \\nthe general two-sided confidence interval for estimating the difference between \\nerrors of two hypotheses. In some situations we might be interested in one-sided \\nbounds--either bounding the largest possible difference in errors or the smallest, \\nwith some confidence level. One-sided confidence intervals can be obtained by \\nmodifying the above expression as described in Section 5.3.6. \\nAlthough the above analysis considers the case in which hl and h2 are tested \\non independent data samples, it is often acceptable to use the confidence interval \\nseen in Equation (5.13) in the setting where h 1 and h2 are tested on a single sample \\nS (where S is still independent of hl and h2). In this later case, we redefine 2 as \\nThe variance in this new 2 will usually be smaller than the variance given by \\nEquation (5.12), when we set S1 and S2 to S. This is because using a single \\nsample S eliminates the variance due to random differences in the compositions \\nof S1 and S2. In this case, the confidence interval given by Equation (5.13) will \\ngenerally be an overly conservative, but still correct, interval. \\n5.5.1 Hypothesis Testing \\nIn some cases we are interested in the probability that some specific conjecture is \\ntrue, rather than in confidence intervals for some parameter. Suppose, for example, \\nthat we are interested in the question \"what is the probability that errorv(h1) > \\nerrorv(h2)?\\' Following the setting in the previous section, suppose we measure \\nthe sample errors for hl and h2 using two independent samples S1 and S2 of size \\n100 and find that errors, (hl) = .30 and errors2(h2) = -20, hence the observed \\ndifference is 2 = .lo. Of course, due to random variation in the data sample, '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 156}, page_content='we might observe this difference in the sample errors even when errorv(hl) 5 \\nerrorv(h2). What is the probability that errorv(hl) > errorv(h2), given the \\nobserved difference in sample errors 2 = .10 in this case? Equivalently, what is \\nthe probability that d > 0, given that we observed 2 = .lo? \\nNote the probability \\nPr(d > 0) is equal to the probability that 2 has not \\noverestimated d by more than .lo. Put another way, this is the probability that 2 \\nfalls into the one-sided interval 2 < d + .lo. Since d is the mean of the distribution \\ngoverning 2, we can equivalently express this one-sided interval as 2 < p2 + .lo. \\nTo summarize, the probability Pr(d > 0) equals the probability that 2 falls \\ninto the one-sided interval 2 < pa + .lo. Since we already calculated the ap- \\nproximate distribution governing 2 in the previous section, we can determine the \\nprobability that 2 falls into this one-sided interval by calculating the probability \\nmass of the 2 distribution within this interval. \\nLet us begin this calculation by re-expressing the interval 2 < pi + .10 in \\nterms of the number of standard deviations it allows deviating from the mean. \\nUsing Equation (5.12) we find that 02 FZ .061, so we can re-express the interval \\nas approximately \\nWhat is the confidence level associated with this one-sided interval for a Normal \\ndistribution? Consulting Table 5.1, we find that 1.64 standard deviations about the \\nmean corresponds to a two-sided interval with confidence level 90%. Therefore, \\nthe one-sided interval will have an associated confidence level of 95%. \\nTherefore, given the observed 2 = .lo, the probability that errorv(h1) > \\nerrorv(h2) is approximately .95. In the terminology of the statistics literature, we \\nsay that we accept the hypothesis that \\n\"errorv(hl) > errorv(h2)\" with confidence \\n0.95. Alternatively, we may state that we reject the opposite hypothesis (often \\ncalled the null hypothesis) at a (1 - 0.95) = .05 level of significance. \\n5.6 COMPARING LEARNING ALGORITHMS \\nOften we are interested in comparing the performance of two learning algorithms \\nLA and LB, rather than two specific hypotheses. What is an appropriate test for \\ncomparing learning algorithms, and how can we determine whether an observed \\ndifference between the algorithms is statistically significant? Although there is \\nactive debate within the machine-learning research community regarding the best \\nmethod for comparison, we present here one reasonable approach. A discussion \\nof alternative methods is given by Dietterich (1996). \\nAs usual, we begin by specifying the parameter we wish to estimate. Suppose \\nwe wish to determine which of LA and LB is the better learning method on average \\nfor learning some particular target function f. A reasonable way to define \"on \\naverage\" is to consider the relative \\nperformance \\nof these two algorithms averaged \\nover all the training sets of size n that might be drawn from the underlying \\ninstance distribution V. In other words, we wish to estimate the expected value '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 157}, page_content=\"of the difference in their errors \\nwhere \\nL(S) \\ndenotes the hypothesis output by learning method L when given \\nthe sample \\nS \\nof training data and where the subscript \\nS c V indicates that \\nthe expected value is taken over samples \\nS \\ndrawn according to the underlying \\ninstance distribution V. The above expression describes the expected value of the \\ndifference in errors between learning methods LA and L B. \\nOf course in practice we have only a limited sample Do of data when \\ncomparing learning methods. In such cases, one obvious approach to estimating \\nthe above quantity is to divide \\nDo \\ninto a training set \\nSo \\nand a disjoint test set To. \\nThe training data can be used to train both \\nLA \\nand \\nLB, \\nand the test data can be \\nused to compare the accuracy of the two learned hypotheses. In other words, we \\nmeasure the quantity \\nNotice two key differences between this estimator and the quantity in Equa- \\ntion (5.14). First, we are using \\nerrorTo(h) \\nto approximate \\nerrorv(h). \\nSecond, we \\nare only measuring the difference in errors for one training set \\nSo \\nrather than tak- \\ning the expected value of this difference over all samples \\nS \\nthat might be drawn \\nfrom the distribution 2). \\nOne way to improve on the estimator given by Equation (5.15) is to repeat- \\nedly partition the data \\nDo \\ninto disjoint training and test sets and to take the mean \\nof the test set errors for these different experiments. This leads to the procedure \\nshown in Table 5.5 for estimating the difference between errors of two learning \\nmethods, based on a fixed sample \\nDo \\nof available data. This procedure first parti- \\ntions the data into k disjoint subsets of equal size, where this size is at least 30. It \\nthen trains and tests the learning algorithms k times, using each of the k subsets \\nin turn as the test set, and using all remaining data as the training set. In this \\nway, the learning algorithms are tested on k independent test sets, 'and the mean \\ndifference in errors 8 is returned as an estimate of the difference between the two \\nlearning algorithms. \\nThe quantity 8 returned by the procedure of Table 5.5 can be taken as an \\nestimate of the desired quantity from Equation 5.14. More appropriately, we can \\nview 8 as an estimate of the quantity \\nwhere \\nS \\nrepresents a random sample of size \\nID01 \\ndrawn uniformly from Do. \\nThe only difference between this expression and our original expression in Equa- \\ntion (5.14) is that this new expression takes the expected value over subsets of \\nthe available data \\nDo, \\nrather than over subsets drawn from the full instance dis- \\ntribution 2). \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 158}, page_content='1. Partition the available data Do into k disjoint subsets TI, T2, . . . , Tk of equal size, where this size \\nis at least 30. \\n2. For i from 1 to k, do \\nuse Ti for the test set, and the remaining data for training set Si \\n0 Si c {Do - Ti} \\nhA C LA(Si) \\nh~ + L~(si) \\n0 Si t errorq (hA) - errorz (hB) \\n3. Return the value 6, where \\nTABLE 5.5 \\nA procedure to estimate the difference in error between two learning methods LA and LB. Approxi- \\nmate confidence intervals for this estimate are given in the text. \\nThe approximate N% confidence interval for estimating the quantity in Equa- \\ntion (5.16) using 8 is given by \\nwhere tN,k-l is a constant that plays a role analogous to that of ZN in our ear- \\nlier confidence interval expressions, and where s,- is an estimate of the standard \\ndeviation of the distribution governing 8. In particular, sg is defined as \\nNotice the constant t~,k-l in Equation (5.17) has two subscripts. The first \\nspecifies the desired confidence level, as it did for our earlier constant ZN. The \\nsecond parameter, called the number of degrees of freedom and usually denoted by \\nv, is related to the number of independent random events that go into producing \\nthe value for the random variable 8. In the current setting, the number of degrees \\nof freedom is k - 1. Selected values for the parameter t are given in Table 5.6. \\nNotice that as k + oo, the value of t~,k-l approaches the constant ZN. \\nNote the procedure described here for comparing two learning methods in- \\nvolves testing the two learned hypotheses on identical test sets. This contrasts with \\nthe method described in Section 5.5 for comparing hypotheses that have been eval- \\nuated using two independent test sets. Tests where the hypotheses are evaluated \\nover identical samples are called paired tests. Paired tests typically produce tighter \\nconfidence intervals because any differences in observed errors in a paired test \\nare due to differences between the hypotheses. In contrast, when the hypotheses \\nare tested on separate data samples, differences in the two sample errors might be \\npartially attributable to differences in the makeup of the two samples. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 159}, page_content='Confidence level N \\n90% 95% 98% 99% \\nTABLE 5.6 \\nValues  oft^,\" for two-sided confidence intervals. As v + w, t~,\" approaches ZN. \\n5.6.1 Paired t Tests \\nAbove we described one procedure for comparing two learning methods given a \\nfixed set of data. This section discusses the statistical justification for this proce- \\ndure, and for the confidence interval defined by Equations (5.17) and (5.18). It \\ncan be skipped or skimmed on a first reading without loss of continuity. \\nThe best way to understand the justification for the confidence interval es- \\ntimate given by Equation (5.17) is to consider the following estimation problem: \\n0 \\n0 \\na \\nThis \\nWe are given the observed values of a set of independent, identically dis- \\ntributed random variables \\nYI, \\nY2, . . . , Yk. \\nWe wish to estimate the mean p of the probability distribution governing \\nthese Yi. \\nThe estimator we will use is the sample mean Y \\nproblem of estimating the distribution mean p based on the sample mean \\nY is quite general. For example, it covers the problem discussed earlier of using \\nerrors(h) \\nto estimate \\nerrorv(h). \\n(In that problem, the Yi are 1 or 0 to indicate \\nwhether h commits an error on an individual example from S, and errorv(h) is the \\nmean p of the underlying distribution.) The t test, described by Equations (5.17) \\nand \\n(5.18), \\napplies to a special case of this problem-the case in which the \\nindividual Yi follow a Normal distribution. \\nNow consider the following idealization of the method in Table 5.5 for com- \\nparing learning methods. Assume that instead of having a fixed sample of data Do, \\nwe can request new training examples drawn according to the underlying instance \\ndistribution. In particular, in this idealized method we modify the procedure of \\nTable 5.5 so that on each iteration through the loop it generates a new random \\ntraining set Si and new random test set Ti by drawing from this underlying instance \\ndistribution instead of drawing from the fixed sample Do. This idealized method '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 160}, page_content='perfectly fits the form of the above estimation problem. In particular, the Si mea- \\nsured by the procedure now correspond to the independent, identically distributed \\nrandom variables Yi. The mean p of their distribution corresponds to the expected \\ndifference in error between the two learning methods \\n[i.e., \\nEquation \\n(5.14)]. \\nThe \\nsample mean Y is the quantity 6 computed by this idealized version of the method. \\nWe wish to answer the question \"how good an estimate of p is provided by s?\\' \\nFirst, note that the size of the test sets has been chosen to contain at least \\n30 examples. Because of this, the individual Si will each follow an approximately \\nNormal distribution (due to the Central Limit Theorem). Hence, we have a special \\ncase in which the Yi are governed by an approximately Normal distribution. It \\ncan be shown in general that when the individual Yi each follow a Normal dis- \\ntribution, then the sample mean Y follows a Normal distribution as well. Given \\nthat Y is Normally distributed, we might consider using the earlier expression for \\nconfidence intervals (Equation \\n[5.11]) \\nthat applies to estimators governed by Nor- \\nmal distributions. Unfortunately, that equation requires that we know the standard \\ndeviation of this distribution, which we do not. \\nThe t test applies to precisely these situations, in which the task is to esti- \\nmate the sample mean of a collection of independent, identically and Normally \\ndistributed random variables. In this case, we can use the confidence interval given \\nby Equations (5.17) and \\n(5.18), \\nwhich can be restated using our current notation as \\nwhere sp is the estimated standard deviation of the sample mean \\nand where \\ntN,k-l \\nis \\na constant analogous to our earlier ZN. In fact, the constant \\nt~,k-l \\ncharacterizes the area under a probability distribution known as the t distri- \\nbution, just as the constant ZN characterizes the area under a Normal distribution. \\nThe t distribution is a bell-shaped distribution similar to the Normal distribu- \\ntion, but wider and shorter to reflect the greater variance introduced by using sp \\nto approximate the true standard deviation ap. The t distribution approaches the \\nNormal distribution (and therefore \\ntN,k-l \\napproaches \\nzN) \\nas k approaches infinity. \\nThis is intuitively satisfying because we expect sp to converge toward the true \\nstandard deviation ap as the sample size k grows, and because we can use ZN \\nwhen the standard deviation is known exactly. \\n5.6.2 Practical Considerations \\nNote the above discussion justifies the use of the confidence interval estimate \\ngiven by Equation (5.17) in the case where we wish to use the sample mean \\nY to estimate the mean of a sample containing k independent, identically and \\nNormally distributed random variables. This fits the idealized method described '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 161}, page_content=\"above, in which we assume unlimited access to examples of the target function. In \\npractice, given a limited set of data \\nDo \\nand the more practical method described \\nby Table 5.5, this justification does not strictly apply. In practice, the problem is \\nthat the only way to generate new Si is to resample Do, dividing it into training \\nand test sets in different ways. The 6i are not independent of one another in this \\ncase, because they are based on overlapping sets of training examples drawn from \\nthe limited subset \\nDo \\nof data, rather than from the full distribution 'D. \\nWhen only a limited sample of data Do is available, several methods can be \\nused to \\nresample \\nDo. \\nTable 5.5 describes a k-fold method in which Do is parti- \\ntioned into k disjoint, equal-sized subsets. In this k-fold approach, each example \\nfrom \\nDo \\nis used exactly once in a test set, and k - 1 times in a training set. A \\nsecond popular approach is to randomly choose a test set of at least 30 examples \\nfrom \\nDo, \\nuse the remaining examples for training, then repeat this process as \\nmany times as desired. This randomized method has the advantage that it can be \\nrepeated an indefinite number of times, to shrink the confidence interval to the \\ndesired width. In contrast, the k-fold method is limited by the total number of \\nexamples, by the use of each example only once in a test set, and by our desire \\nto use samples of size at least 30. However, the randomized method has the dis- \\nadvantage that the test sets no longer qualify as being independently drawn with \\nrespect to the underlying instance distribution D. In contrast, the test sets gener- \\nated by k-fold cross validation are independent because each instance is included \\nin only one test set. \\nTo summarize, no single procedure for comparing learning methods based \\non limited data satisfies all the constraints we would like. It is wise to keep in \\nmind that statistical models rarely fit perfectly the practical constraints in testing \\nlearning algorithms when available data is limited. Nevertheless, they do pro- \\nvide approximate confidence intervals that can be of great help in interpreting \\nexperimental comparisons of learning methods. \\n5.7 SUMMARY AND FURTHER READING \\nThe main points of this chapter include: \\n0 Statistical theory provides a basis for estimating the true error (errorv(h)) \\nof a hypothesis h, based on its observed error \\n(errors(h)) \\nover a sample S of \\ndata. For example, if h is a discrete-valued hypothesis and the data sample \\nS contains n 2 30 examples drawn independently of h and of one another, \\nthen the N% confidence interval for errorv(h) is approximately \\nwhere values for zN are given in Table 5.1. \\n0 In general, the problem of estimating confidence intervals is approached by \\nidentifying the parameter to be estimated \\n(e.g., \\nerrorD(h)) and an estimator \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 162}, page_content='CHAFER 5 EVALUATING HYPOTHESES 151 \\n(e.g., errors(h)) for this quantity. Because the estimator is a random variable \\n(e.g., \\nerrors(h) \\ndepends on the random sample S), it can be characterized \\nby the probability distribution that governs its value. Confidence intervals \\ncan then be calculated by determining the interval that contains the desired \\nprobability mass under this distribution. \\n0 One possible cause of errors in estimating hypothesis accuracy is estimation \\nbias. If Y is an estimator for some parameter p, the estimation bias of Y \\nis the difference between p and the expected value of Y. For example, if S \\nis the training data used to formulate hypothesis h, then errors(h) gives an \\noptimistically biased estimate of the true error \\nerrorD(h). \\n0 A second cause of estimation error is variance in the estimate. Even with an \\nunbiased estimator, the observed value of the estimator is likely to vary from \\none experiment to another. The variance a2 of the distribution governing the \\nestimator characterizes how widely this estimate is likely to vary from the \\ncorrect value. This variance decreases as the size of the data sample is \\nincreased. \\n0 Comparing the relative effectiveness of two learning algorithms is an esti- \\nmation problem that is relatively easy when data and time are unlimited, but \\nmore difficult when these resources are limited. One possible approach de- \\nscribed in this chapter is to run the learning algorithms on different subsets \\nof the available data, testing the learned hypotheses on the remaining data, \\nthen averaging the results of these experiments. \\n0 In most cases considered here, deriving confidence intervals involves making \\na number of assumptions and approximations. For example, the above confi- \\ndence interval for \\nerrorv(h) \\ninvolved approximating a Binomial distribution \\nby a Normal distribution, approximating the variance of this distribution, and \\nassuming instances are generated by a fixed, unchanging probability distri- \\nbution. While intervals based on such approximations are only approximate \\nconfidence intervals, they nevertheless provide useful guidance for designing \\nand interpreting experimental results in machine learning. \\nThe key statistical definitions presented in this chapter are summarized in \\nTable 5.2. \\nAn ocean of literature exists on the topic of statistical methods for estimating \\nmeans and testing significance of hypotheses. While this chapter introduces the \\nbasic concepts, more detailed treatments of these issues can be found in many \\nbooks and articles. Billingsley et al. (1986) provide a very readable introduction \\nto statistics that elaborates on the issues discussed here. Other texts on statistics \\ninclude \\nDeGroot \\n(1986); Casella and Berger (1990). Duda and Hart (1973) provide \\na treatment of these issues in the context of numerical pattern recognition. \\nSegre et al. (1991, \\n1996), \\nEtzioni and Etzioni \\n(1994), \\nand Gordon and \\nSegre \\n(1996) discuss statistical significance tests for evaluating learning algo- \\nrithms whose performance is measured by their ability to improve computational \\nefficiency. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 163}, page_content='Geman et al. (1992) discuss the tradeoff involved in attempting to minimize \\nbias and variance simultaneously. There is ongoing debate regarding the best way \\nto learn and compare hypotheses from limited data. For example, Dietterich (1996) \\ndiscusses the risks of applying the paired-difference t test repeatedly to different \\ntrain-test splits of the data. \\nEXERCISES \\n5.1. Suppose you test a hypothesis h and find that it commits r = 300 errors on a sample \\nS of n = 1000 randomly drawn test examples. What is the standard deviation in \\nerrors(h)? How does this compare to the standard deviation in the example at the \\nend of Section \\n5.3.4? \\n5.2. Consider a learned hypothesis, h, for some boolean concept. When h is tested on a \\nset of 100 examples, it classifies 83 correctly. What is the standard deviation and \\nthe 95% confidence interval for the true error rate for Errorv(h)? \\n5.3. Suppose hypothesis h commits r = 10 errors over a sample of n = 65 independently \\ndrawn examples. What is the 90% confidence interval (two-sided) for the true error \\nrate? What is the 95% one-sided interval \\n(i.e., \\nwhat is the upper bound U such that \\nerrorv(h) 5 U with 95% confidence)? What is the 90% one-sided interval? \\n5.4. You are about to test a hypothesis h whose errorV(h) is known to be in the range \\nbetween 0.2 and 0.6. What is the minimum number of examples you must collect \\nto assure that the width of the two-sided 95% confidence interval will be smaller \\nthan \\n0.1? \\n5.5. Give general expressions for the upper and lower one-sided N% confidence intervals \\nfor the difference in errors between two hypotheses tested on different samples of \\ndata. Hint: Modify the expression given in Section 5.5. \\n5.6. Explain why the confidence interval estimate given in Equation (5.17) applies to \\nestimating the quantity in Equation \\n(5.16), \\nand not the quantity in Equation (5.14). \\nREFERENCES \\nBillingsley, P., Croft, D. J., Huntsberger, D. V., & Watson, C. J. (1986). Statistical inference for \\nmanagement and economics. Boston: Allyn and Bacon, Inc. \\nCasella, \\nG., & Berger, R. L. (1990). Statistical inference. Pacific Grove, CA: Wadsworth and \\nBrooksICole. \\nDeGroot, \\nM. H. (1986). Probability and statistics. (2d ed.) Reading, MA: Addison Wesley. \\nDietterich, T. G. (1996). Proper statistical tests for comparing supervised classiJication learning \\nalgorithms (Technical Report). Department of Computer Science, Oregon State University, \\nCowallis, OR. \\nDietterich, T. G., & Kong, E. B. (1995). Machine learning bias, statistical bias, and statistical \\nvariance of decision tree algorithms (Technical Report). Department of Computer Science, \\nOregon State University, Cowallis, OR. \\nDuda, \\nR., & Hart, P. (1973). Pattern classiJication and scene analysis. New York: John Wiley & \\nSons. \\nEfron, B., & Tibshirani, R. (1991). Statistical data analysis in the computer age. Science, 253, 390- \\n395. \\nEtzioni, O., & Etzioni, R. (1994). Statistical methods for analyzing speedup learning experiments. \\nMachine Learning, 14, 333-347. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 164}, page_content=\"Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the biadvariance dilemma. \\nNeural Computation, 4, 1-58. \\nGordon, G., & Segre, A.M. (1996). Nonpararnetric statistical methods for experimental evaluations of \\nspeedup learning. Proceedings of the Thirteenth International Conference on Machine Leam- \\ning, Bari, Italy. \\nMaisel, \\nL. (1971). Probability, statistics, and random processes. Simon and Schuster Tech Outlines. \\nNew York: Simon and Schuster. \\nSegre, A., \\nElkan, \\nC., & Russell, A. (1991). A critical look at experimental evaluations of EBL. \\nMachine Learning, 6(2). \\nSegre, \\nA.M, \\nGordon G., & Elkan, C. P. (1996). Exploratory analysis of speedup learning data using \\nexpectation maximization. Artificial Intelligence, 85, 301-3 19. \\nSpeigel, M. R. (1991). Theory and problems of probability and statistics. Schaum's Outline Series. \\nNew York: \\nMcGraw \\nHill. \\nThompson, M.L., & Zucchini, W. (1989). On the statistical analysis of ROC curves. Statistics in \\nMedicine, 8, 1277-1290. \\nWhite, A. P., & Liu, W. Z. (1994). Bias in information-based measures in decision tree induction. \\nMachine Learning, 15, 321-329. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 165}, page_content='CHAPTER \\nBAYESIAN \\nLEARNING \\nBayesian reasoning provides a probabilistic approach to inference. It is based on \\nthe assumption that the quantities of interest are governed by probability distri- \\nbutions and that optimal decisions can be made by reasoning about these proba- \\nbilities together with observed data. It is important to machine learning because \\nit provides a quantitative approach to weighing the evidence supporting alterna- \\ntive hypotheses. Bayesian reasoning provides the basis for learning algorithms \\nthat directly manipulate probabilities, as well as a framework for analyzing the \\noperation of other algorithms that do not explicitly manipulate probabilities. \\n6.1 INTRODUCTION \\nBayesian learning methods are relevant to our study of machine learning for \\ntwo different reasons. First, Bayesian learning algorithms that calculate explicit \\nprobabilities for hypotheses, such as the naive Bayes classifier, are among the most \\npractical approaches to certain types of learning problems. For example, Michie \\net al. (1994) provide a detailed study comparing the naive Bayes classifier to \\nother learning algorithms, including decision tree and neural network algorithms. \\nThese researchers show that the naive Bayes classifier is competitive with these \\nother learning algorithms in many cases and that in some cases it outperforms \\nthese other methods. In this chapter we describe the naive Bayes classifier and \\nprovide a detailed example of its use. In particular, we discuss its application to \\nthe problem of learning to classify text documents such as electronic news articles. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 166}, page_content='CHAFER 6 BAYESIAN LEARNING 155 \\nFor such learning tasks, the naive Bayes classifier is among the most effective \\nalgorithms known. \\nThe second reason that Bayesian methods are important to our study of ma- \\nchine learning is that they provide a useful perspective for understanding many \\nlearning algorithms that do not explicitly manipulate probabilities. For exam- \\nple, in this chapter we analyze algorithms such as the FIND-S and \\nCANDIDATE- \\nELIMINATION algorithms of Chapter 2 to determine conditions under which they \\noutput the most probable hypothesis given the training data. We also use a \\nBayesian analysis to justify a key design choice in neural network learning al- \\ngorithms: choosing to minimize the sum of squared errors when searching the \\nspace of possible neural networks. We also derive an alternative error function, \\ncross entropy, that is more appropriate than sum of squared errors when learn- \\ning target functions that predict probabilities. We use a Bayesian perspective to \\nanalyze the inductive bias of decision tree learning algorithms that favor short \\ndecision trees and examine the closely related Minimum Description Length prin- \\nciple. A basic familiarity with Bayesian methods is important to understanding \\nU and characterizing the operation of many algorithms in machine learning. \\nFeatures of Bayesian learning methods include: \\n0 Each observed training example can incrementally decrease or increase the \\nestimated probability that a hypothesis is correct. This provides a more \\nflexible approach to learning than algorithms that completely eliminate a \\nhypothesis if it is found to be inconsistent with any single example. \\n0 Prior knowledge can be combined with observed data to determine the final \\nprobability \\n~f \\na hypothesis. In Bayesian learning, prior knowledge is pro- \\nvided by asserting (1) a prior probability for each candidate hypothesis, and \\n(2) a probability distribution over observed data for each possible hypothesis. \\nBayesian methods can accommodate hypotheses that make probabilistic pre- \\ndictions \\n(e.g., \\nhypotheses such as \"this pneumonia patient has a 93% chance \\nof complete recovery\"). \\n0 New instances can be classified by combining the predictions of multiple \\nhypotheses, weighted by their probabilities. \\n0 Even in cases where Bayesian methods prove computationally intractable, \\nthey can provide a standard of optimal decision making against which other \\npractical methods can be measured. \\nOne practical difficulty in applying Bayesian methods is that they typically \\nrequire initial knowledge of many probabilities. When these probabilities are not \\nknown in advance they are often estimated based on background knowledge, pre- \\nviously available data, and assumptions about the form of the underlying distribu- \\ntions. A second practical difficulty is the significant computational cost required to \\ndetermine the Bayes optimal hypothesis in the general case (linear in the number \\nof candidate hypotheses). In certain specialized situations, this computational cost \\ncan be significantly reduced. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 167}, page_content='The remainder of this chapter is organized as follows. Section 6.2 intro- \\nduces Bayes theorem and defines maximum likelihood and maximum a posteriori \\nprobability hypotheses. The four subsequent sections then apply this probabilistic \\nframework to analyze several issues and learning algorithms discussed in earlier \\nchapters. For example, we show that several previously described algorithms out- \\nput maximum likelihood hypotheses, under certain assumptions. The remaining \\nsections then introduce a number of learning algorithms that explicitly manip- \\nulate probabilities. These include the Bayes optimal classifier, Gibbs algorithm, \\nand naive Bayes classifier. Finally, we discuss Bayesian belief networks, a rela- \\ntively recent approach to learning based on probabilistic reasoning, and the EM \\nalgorithm, a widely used algorithm for learning in the presence of unobserved \\nvariables. \\n6.2 BAYES THEOREM \\nIn machine learning we are often interested in determining the best hypothesis \\nfrom some space H, given the observed training data D. One way to specify \\nwhat we mean by the best hypothesis is to say that we demand the most probable \\nhypothesis, given the data D plus any initial knowledge about the prior probabil- \\nities of the various hypotheses in H. Bayes theorem provides a direct method for \\ncalculating such probabilities. More precisely, Bayes theorem provides a way to \\ncalculate the probability of a hypothesis based on its prior probability, the proba- \\nbilities of observing various data given the hypothesis, and the observed data itself. \\nTo define Bayes theorem precisely, let us first introduce a little notation. We \\nshall write P(h) to denote the initial probability that hypothesis h holds, before we \\nhave observed the training data. P(h) is often called the priorprobability of h and \\nmay reflect any background knowledge we have about the chance that h is a correct \\nhypothesis. If we have no such prior knowledge, then we might simply assign \\nthe same prior probability to each candidate hypothesis. Similarly, we will write \\nP(D) to denote the prior probability that training data D will be observed (i.e., \\nthe probability of D given no knowledge about which hypothesis holds). Next, \\nwe will write P(D1h) to denote the probability of observing data D given some \\nworld in which hypothesis h holds. More generally, we write P(xly) to denote \\nthe probability of x given y. In machine learning problems we are interested in \\nthe probability P (h 1 D) that h holds given the observed training data D. P (h 1 D) is \\ncalled the posteriorprobability of h, because it reflects our confidence that h holds \\nafter we have seen the training data D. Notice the posterior probability P(h1D) \\nreflects the influence of the training data D, in contrast to the prior probability \\nP(h) , which is independent of D. \\nBayes theorem is the cornerstone of Bayesian learning methods because \\nit provides a way to calculate the posterior probability P(hlD), from the prior \\nprobability P(h), together with P(D) and P(D(h). \\nBayes theorem: '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 168}, page_content='CHAPTER 6 BAYESIAN LEARNING 157 \\nAs one might intuitively expect, P(h ID) increases with P(h) and with P(D1h) \\naccording to Bayes theorem. It is also reasonable to see that P(hl D) decreases as \\nP(D) increases, because the more probable it is that D will be observed indepen- \\ndent of h, the less evidence D provides in support of h. \\nIn many learning scenarios, the learner considers some set of candidate \\nhypotheses H and is interested in finding the most probable hypothesis h E H \\ngiven the observed data D (or at least one of the maximally probable if there \\nare several). Any such maximally probable hypothesis is called a maximum a \\nposteriori (MAP) hypothesis. We can determine the MAP hypotheses by using \\nBayes theorem to calculate the posterior probability of each candidate hypothesis. \\nMore precisely, we will say that  MAP is a MAP hypothesis provided \\nh~~p = argmax P(hlD) \\nh€H \\n= argmax P(D 1 h) P (h) \\nh€H \\n(6.2) \\nNotice in the final step above we dropped the term P(D) because it is a constant \\nindependent of h. \\nIn some cases, we will assume that every hypothesis in H is equally probable \\na priori (P(hi) = P(h;) for all hi and h; in H). In this case we can further \\nsimplify Equation (6.2) and need only consider the term P(D1h) to find the most \\nprobable hypothesis. P(Dlh) is often called the likelihood of the data D given h, \\nand any hypothesis that maximizes P(Dlh) is called a maximum likelihood (ML) \\nhypothesis, hML. \\nhML = argmax P(Dlh) \\nh€H \\nIn order to make clear the connection to machine learning problems, we \\nintroduced Bayes theorem above by referring to the data D as training examples of \\nsome target function and referring to H as the space of candidate target functions. \\nIn fact, Bayes theorem is much more general than suggested by this discussion. It \\ncan be applied equally well to any set H of mutually exclusive propositions whose \\nprobabilities sum to one \\n(e.g., \\n\"the sky is blue,\" and \"the sky is not blue\"). In this \\nchapter, we will at times consider cases where H is a hypothesis space containing \\npossible target functions and the data D are training examples. At other times we \\nwill consider cases where H is some other set of mutually exclusive propositions, \\nand D is some other kind of data. \\n6.2.1 An Example \\nTo illustrate Bayes rule, consider a medical diagnosis problem in which there are \\ntwo alternative hypotheses: (1) that the patien; has a- articular form of cancer. \\nand (2) that the patient does not. The avaiiable data is from a particular laboratory '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 169}, page_content='test with two possible outcomes: $ (positive) and 8 (negative). We have prior \\nknowledge that over the entire population of people only \\n.008 \\nhave this disease. \\nFurthermore, the lab test is only an imperfect indicator of the disease. The test \\nreturns a correct positive result in only 98% of the cases in which the disease is \\nactually present and a correct negative result in only 97% of the cases in which \\nthe disease is not present. In other cases, the test returns the opposite result. The \\nabove situation can be summarized by the following probabilities: \\nSuppose we now observe a new patient for whom the lab test returns a positive \\nresult. Should we diagnose the patient as having cancer or not? The maximum a \\nposteriori hypothesis can be found using Equation (6.2): \\nThus, h~~p = -cancer. The exact posterior hobabilities can also be determined \\nby normalizing the above quantities so that they sum to 1 \\n(e.g., \\nP(cancer($) = \\n.00;~~298 = .21). This step is warranted because Bayes theorem states that the \\nposterior probabilities are just the above quantities divided by the probability of \\nthe data, \\nP(@). \\nAlthough \\nP($) \\nwas not provided directly as part of the problem \\nstatement, we can calculate it in this fashion because we know that \\nP(cancerl$) \\nand \\nP(-cancerl$) \\nmust sum to 1 (i.e., either the patient has cancer or they do \\nnot). Notice that while the posterior probability of cancer is significantly higher \\nthan its prior probability, the most probable hypothesis is still that the patient does \\nnot have cancer. \\nAs this example illustrates, the result of Bayesian inference depends strongly \\non the prior probabilities, which must be available in order to apply the method \\ndirectly. Note also that in this example the hypotheses are not completely accepted \\nor rejected, but rather become more or less probable as more data is observed. \\nBasic formulas for calculating probabilities are summarized in Table 6.1. \\n6.3 BAYES THEOREM AND CONCEPT LEARNING \\nWhat is the relationship between Bayes theorem and the problem of concept learn- \\ning? Since Bayes theorem provides a principled way to calculate the posterior \\nprobability of each hypothesis given the training data, we can use it as the basis \\nfor a straightforward learning algorithm that calculates the probability for each \\npossible hypothesis, then outputs the most probable. This section considers such \\na brute-force Bayesian concept learning algorithm, then compares it to concept \\nlearning algorithms we considered in Chapter 2. As we shall see, one interesting \\nresult of this comparison is that under certain conditions several algorithms dis- \\ncussed in earlier chapters output the same hypotheses as this brute-force Bayesian '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 170}, page_content='CHAPTER 6 BAYESIAN LEARNING 159 \\n- \\n. Product rule: probability P(A A B) of a conjunction of two events A and B \\nSum rule: probability of a disjunction of two events A and B \\nBayes theorem: the posterior probability P(hl D) of h given D \\n. Theorem of totalprobability: if events A1,. . . , A, are mutually exclusive with xy=l P(Ai) = 1, \\nthen \\nTABLE 6.1 \\nSummary of basic probability formulas. \\n11 \\nt \\nalgorithm, despite the fact that they do not explicitly manipulate probabilities and \\nare considerably more efficient. \\n6.3.1 Brute-Force Bayes Concept Learning \\nConsider the concept learning problem first introduced in Chapter 2. In particular, \\nassume the learner considers some finite hypothesis space H defined over the \\ninstance space X, in which the task is to learn some target concept c : X + {0,1}. \\nAs usual, we assume that the learner is given some sequence of training examples \\n((x~, dl) . . . (xm, dm)) where xi is some instance from X and where di is the target \\nvalue of xi (i.e., di = c(xi)). To simplify the discussion in this section, we assume \\nthe sequence of instances (xl . . . xm) is held fixed, so that the training data D can \\nbe written simply as the sequence of target values D = (dl . . . dm). It can be shown \\n(see Exercise 6.4) that this simplification does not alter the main conclusions of \\nthis section. \\nWe can design a straightforward concept learning algorithm to output the \\nmaximum a posteriori hypothesis, based on Bayes theorem, as follows: \\nBRUTE-FORCE MAP LEARNING algorithm \\n1. For each hypothesis h in H, calculate the posterior probability \\n2. Output the hypothesis hMAP with the highest posterior probability '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 171}, page_content='160 MACHINE LEARNING \\nThis algorithm may require significant computation, because it applies Bayes theo- \\nrem to each hypothesis in H to calculate P(hJ D). While this may prove impractical \\nfor large hypothesis spaces, the algorithm is still of interest because it provides a \\nstandard against which we may judge the performance of other concept learning \\nalgorithms. \\nIn order specify a Iearning problem for the BRUTE-FORCE MAP LEARNING \\nalgorithm we must specify what values are to be used for P(h) and for P(D1h) \\n(as we shall see, P(D) will be determined once we choose the other two). We \\nmay choose the probability distributions P(h) and P(D1h) in any way we wish, \\nto describe our prior knowledge about the learning task. Here let us choose them \\nto be consistent with the following assumptions: \\n1. The training data D is noise free (i.e., di = c(xi)). \\n2. The target concept c is contained in the hypothesis space H \\n3. We have no a priori reason to believe that any hypothesis is more probable \\nthan any other. \\nGiven these assumptions, what values should we specify for P(h)? Given no \\nprior knowledge that one hypothesis is more likely than another, it is reasonable to \\nassign the same prior probability to every hypothesis h in H. Furthermore, because \\nwe assume the target concept is contained in H we should require that these prior \\nprobabilities sum to 1. Together these constraints imply that we should choose \\n1 P(h) = - for all h in H \\nIHI \\nWhat choice shall we make for P(Dlh)? P(D1h) is the probability of ob- \\nserving the target values D = (dl . . .dm) for the fixed set of instances (XI . . . x,), \\ngiven a world in which hypothesis h holds (i.e., given a world in which h is the \\ncorrect description of the target concept c). Since we assume noise-free training \\ndata, the probability of observing classification di given h is just 1 if di = h(xi) \\nand 0 if di # h(xi). Therefore, \\n1 if di = h(xi) for all di in D \\nP(D1h) = (6.4) \\n0 otherwise \\nIn other words, the probability of data D given hypothesis h is 1 if D is consistent \\nwith h, and 0 otherwise. \\nGiven these choices for P(h) and for P(Dlh) we now have a fully-defined \\nproblem for the above BRUTE-FORCE MAP LEARNING algorithm. Let us consider the \\nfirst step of this algorithm, which uses Bayes theorem to compute the posterior \\nprobability P(h1D) of each hypothesis h given the observed training data D. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 172}, page_content=\"CHAPTER 6 BAYESIAN LEARNING 161 \\nRecalling Bayes theorem, we have \\nFirst consider the case where h is inconsistent with the training data D. Since \\nEquation (6.4) defines P(D)h) to be 0 when h is inconsistent with D, we have \\nP(~(D) = - ' P(h) - - o if h is inconsistent with D \\nP(D) \\nThe posterior probability of a hypothesis inconsistent with D is zero. \\nNow consider the case where h is consistent with D. Since Equation (6.4) \\ndefines P(Dlh) to be 1 when h is consistent with D, we have \\n- 1 -- if h is consistent with D \\nIVSH,DI \\nwhere VSH,~ is the subset of hypotheses from H that are consistent with D (i.e., \\nVSH,~ is the version space of H with respect to D as defined in Chapter 2). It \\nis easy to verify that P(D) = above, because the sum over all hypotheses \\nof P(h ID) must be one and because the number of hypotheses from H consistent \\nwith D is by definition IVSH,DI. Alternatively, we can derive P(D) from the \\ntheorem of total probability (see Table 6.1) and the fact that the hypotheses are \\nmutually exclusive \\n(i.e., (Vi # j)(P(hi A hj) = 0)) \\nTo summarize, Bayes theorem implies that the posterior probability P(h ID) \\nunder our assumed P(h) and P(D1h) is \\nif h is consistent with D \\nP(hlD) = (6.3 \\n0 otherwise \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 173}, page_content='where IVSH,DI is the number of hypotheses from H consistent with D. The evo- \\nlution of probabilities associated with hypotheses is depicted schematically in \\nFigure 6.1. Initially (Figure 6.1~) all hypotheses have the same probability. As \\ntraining data accumulates (Figures 6.1 b and 6. lc), the posterior probability for \\ninconsistent hypotheses becomes zero while the total probability summing to one \\nis shared equally among the remaining consistent hypotheses. \\nThe above analysis implies that under our choice for P(h) and P(Dlh), every \\nconsistent hypothesis has posterior probability (1 /I V SH, I), and every inconsistent \\nhypothesis has posterior probability 0. Every consistent hypothesis is, therefore, \\na MAP hypothesis. \\n6.3.2 MAP Hypotheses and Consistent Learners \\nThe above analysis shows that in the given setting, every hypothesis consistent \\nwith D is a MAP hypothesis. This statement translates directly into an interesting \\nstatement about a general class of learners that we might call consistent learners. \\nWe will say that a learning algorithm is a consistent learner provided it outputs a \\nhypothesis that commits zero errors over the training examples. Given the above \\nanalysis, we can conclude that every consistent learner outputs a MAP hypothesis, \\nif we assume a uniform prior probability distribution over H (i.e., P(hi) = P(hj) \\nfor all i, j), and ifwe assume deterministic, noise free training data (i.e., P(D Ih) = \\n1 if D and h are consistent, and 0 otherwise). \\nConsider, for example, the concept learning algorithm FIND-S discussed in \\nChapter 2. FIND-S searches the hypothesis space H from specific to general hy- \\npotheses, outputting a maximally specific consistent hypothesis \\n(i.e., \\na maximally \\nspecific member of the version space). Because FIND-S outputs a consistent hy- \\npothesis, we know that it will output a MAP hypothesis under the probability \\ndistributions P(h) and P(D1h) defined above. Of course FIND-S does not explic- \\nitly manipulate probabilities at all-it simply outputs a maximally specific member \\nhypotheses hypotheses \\n(a) (4 \\nhypotheses \\n(c) \\nFIGURE 6.1 \\nEvolution of posterior probabilities P(hlD) with increasing training data. (a) Uniform priors assign \\nequal probability to each hypothesis. As training data increases first to Dl (b), then to Dl A 02 \\n(c), the posterior probability of inconsistent hypotheses becomes zero, while posterior probabilities \\nincrease for hypotheses remaining in the version space. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 174}, page_content='CHAPTER 6 BAYESIAN LEARNING 163 \\nof the version space. However, by identifying distributions for P(h) and P(D(h) \\nunder which its output hypotheses will be MAP hypotheses, we have a useful way \\nof characterizing the behavior of FIND-S. \\nAre there other probability distributions for P(h) and P(D1h) under which \\nFIND-S outputs MAP hypotheses? Yes. Because FIND-S outputs a maximally spe- \\ncz$c hypothesis from the version space, its output hypothesis will be a MAP \\nhypothesis relative to any prior probability distribution that favors more specific \\nhypotheses. More precisely, suppose 3-1 is any probability distribution P(h) over \\nH that assigns P(h1) 2 P(hz) if hl is more specific than h2. Then it can be shown \\nthat FIND-S outputs a MAP hypothesis assuming the prior distribution 3-1 and the \\nsame distribution P(D1h) discussed above. \\nTo summarize the above discussion, the Bayesian framework allows one \\nway to characterize the behavior of learning algorithms \\n(e.g., \\nFIND-S), even when \\nthe learning algorithm does not explicitly manipulate probabilities. By identifying \\nprobability distributions P(h) and P(Dlh) under which the algorithm outputs \\noptimal \\n(i.e., \\nMAP) hypotheses, we can characterize the implicit assumptions \\n, under which this algorithm behaves optimally. \\n( Using the Bayesian perspective to characterize learning algorithms in this \\nway is similar in spirit to characterizing the inductive bias of the learner. Recall \\nthat in Chapter 2 we defined the inductive bias of a learning algorithm to be \\nthe set of assumptions B sufficient to deductively justify the inductive inference \\nperformed by the learner. For example, we described the inductive bias of the \\nCANDIDATE-ELIMINATION \\nalgorithm as the assumption that the target concept c is \\nincluded in the hypothesis space H. Furthermore, we showed there that the output \\nof this learning algorithm follows deductively from its inputs plus this implicit \\ninductive bias assumption. The above Bayesian interpretation provides an alter- \\nnative way to characterize the assumptions implicit in learning algorithms. Here, \\ninstead of modeling the inductive inference method by an equivalent deductive \\nsystem, we model it by an equivalent probabilistic reasoning system based on \\nBayes theorem. And here the implicit assumptions that we attribute to the learner \\nare assumptions of the form \"the prior probabilities over H are given by the \\ndistribution P(h), and the strength of data in rejecting or accepting a hypothesis \\nis given by P(Dlh).\" The definitions of P(h) and P(D(h) given in this section \\ncharacterize the implicit assumptions of the CANDIDATE-ELIMINATION and FIND-S \\nalgorithms. A probabilistic reasoning system based on Bayes theorem will exhibit \\ninput-output behavior equivalent to these algorithms, provided it is given these \\nassumed probability distributions. \\nThe discussion throughout this section corresponds to a special case of \\nBayesian reasoning, because we considered the case where P(D1h) takes on val- \\nues of only 0 and 1, reflecting the deterministic predictions of hypotheses and the \\nassumption of noise-free training data. As we shall see in the next section, we \\ncan also model learning from noisy training data, by allowing P(D1h) to take on \\nvalues other than 0 and 1, and by introducing into P(D1h) additional assumptions \\nabout the probability distributions that govern the noise. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 175}, page_content='6.4 MAXIMUM LIKELIHOOD AND LEAST-SQUARED ERROR \\nHYPOTHESES \\nAs illustrated in the above section, Bayesian analysis can sometimes be used to \\nshow that a particular learning algorithm outputs MAP hypotheses even though it \\nmay not explicitly use Bayes rule or calculate probabilities in any form. \\nIn this section we consider the problem of learning a continuous-valued \\ntarget function-a problem faced by many learning approaches such as neural \\nnetwork learning, linear regression, and polynomial curve fitting. A straightfor- \\nward Bayesian analysis will show that under certain assumptions any learning \\nalgorithm that minimizes the squared error between the output hypothesis pre- \\ndictions and the training data will output a maximum likelihood hypothesis. The \\nsignificance of this result is that it provides a Bayesian justification (under cer- \\ntain assumptions) for many neural network and other curve fitting methods that \\nattempt to minimize the sum of squared errors over the training data. \\nConsider the following problem setting. Learner L considers an instance \\nspace X and a hypothesis space H consisting of some class of real-valued functions \\ndefined over X (i.e., each h in H is a function of the form h : X -+ 8, where \\n8 represents the set of real numbers). The problem faced by L is to learn an \\nunknown target function f : X -+ 8 drawn from H. A set of m training examples \\nis provided, where the target value of each example is corrupted by random \\nnoise drawn according to a Normal probability distribution. More precisely, each \\ntraining example is a pair of the form (xi, di) where di = f (xi) + ei. Here f (xi) is \\nthe noise-free value of the target function and ei is a random variable represent- \\ning the noise. It is assumed that the values of the ei are drawn independently and \\nthat they are distributed according to a Normal distribution with zero mean. The \\ntask of the learner is to output a maximum likelihood hypothesis, or, equivalently, \\na MAP hypothesis assuming all hypotheses are equally probable a priori. \\nA simple example of such a problem is learning a linear function, though our \\nanalysis applies to learning arbitrary real-valued functions. Figure 6.2 illustrates \\nFIGURE 6.2 \\nLearning a real-valued function. The target \\nfunction f corresponds to the solid line. \\nThe training examples (xi, di ) are assumed \\nto have Normally distributed noise ei with \\nzero mean added to the true target value \\nf (xi). The dashed line corresponds to the \\nlinear function that minimizes the sum of \\nsquared errors. Therefore, it is the maximum \\nI likelihood hypothesis ~ML, given these five \\nx training examples. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 176}, page_content='CHAPTER 6 BAYESIAN LEARNING 165 \\na linear target function f depicted by the solid line, and a set of noisy training \\nexamples of this target function. The dashed line corresponds to the hypothesis \\nhML with least-squared training error, hence the maximum likelihood hypothesis. \\nNotice that the maximum likelihood hypothesis is not necessarily identical to the \\ncorrect hypothesis, f, because it is inferred from only a limited sample of noisy \\ntraining data. \\nBefore showing why a hypothesis that minimizes the sum of squared errors \\nin this setting is also a maximum likelihood hypothesis, let us quickly review two \\nbasic concepts from probability theory: probability densities and Normal distribu- \\ntions. First, in order to discuss probabilities over continuous variables such as e, \\nwe must introduce probability densities. The reason, roughly, is that we wish for \\nthe total probability over all possible values of the random variable to sum to one. \\nIn the case of continuous variables we cannot achieve this by assigning a finite \\nprobability to each of the infinite set of possible values for the random variable. \\nInstead, we speak of a probability density for continuous variables such as e and \\nrequire that the integral of this probability density over all possible values be one. \\nIn general we will use lower case p to refer to the probability density function, \\nto distinguish it from a finite probability P (which we will sometimes refer to as \\na probability mass). The probability density p(x0) is the limit as E goes to zero, \\nof times the probability that x will take on a value in the interval [xo, xo + 6). \\nProbability density function: \\nSecond, we stated that the random noise variable e is generated by a Normal \\nprobability distribution. A Normal distribution is a smooth, bell-shaped distribu- \\ntion that can be completely characterized by its mean p and its standard deviation \\na. See Table 5.4 for a precise definition. \\nGiven this background we now return to the main issue: showing that the \\nleast-squared error hypothesis is, in fact, the maximum likelihood hypothesis \\nwithin our problem setting. We will show this by deriving the maximum like- \\nlihood hypothesis starting with our earlier definition Equation (6.3), but using \\nlower case p to refer to the probability density \\nAs before, we assume a fixed set of training instances (xl . . . xm) and there- \\nfore consider the data D to be the corresponding sequence of target values \\nD = (dl . . .dm). Here di = f (xi) + ei. Assuming the training examples are mu- \\ntually independent given h, we can write P(DJh) as the product of the various \\n~(di lh) '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 177}, page_content='Given that the noise ei obeys a Normal distribution with zero mean and unknown \\nvariance a2, each di must also obey a Normal distribution with variance a2 cen- \\ntered around the true target value f (xi) rather than zero. Therefore p(di lh) can \\nbe written as a Normal distribution with variance a2 and mean p = f (xi). Let us \\nwrite the formula for this Normal distribution to describe p(di Ih), beginning with \\nthe general formula for a Normal distribution from Table 5.4 and substituting the \\nappropriate p and a2. Because we are writing the expression for the probability \\nof di given that h is the correct description of the target function f, we will also \\nsubstitute p = f (xi) = h(xi), yielding \\nWe now apply a transformation that is common in maximum likelihood calcula- \\ntions: Rather than maximizing the above complicated expression we shall choose \\nto maximize its (less complicated) logarithm. This is justified because lnp is a \\nmonotonic function of p. Therefore maximizing In p also maximizes p. \\n... 1 1 \\nhML = argmax xln - - -(di - h(~i))~ \\nh€H i=l dG7 202 \\nThe first term in this expression is a constant independent of h, and can therefore \\nbe discarded, yielding \\n1 \\nhMr = argmax C -s(di - h(xi)12 \\nh€H i=l \\nMaximizing this negative quantity is equivalent to minimizing the corresponding \\npositive quantity. \\nFinally, we can again discard constants that are independent of h. \\nThus, Equation (6.6) shows that the maximum likelihood hypothesis ~ML is \\nthe one that minimizes the sum of the squared errors between the observed training \\nvalues di and the hypothesis predictions h(xi). This holds under the assumption \\nthat the observed training values di are generated by adding random noise to '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 178}, page_content=\"CHAPTER 6 BAYESIAN LEARNING 167 \\nthe true target value, where this random noise is drawn independently for each \\nexample from a Normal distribution with zero mean. As the above derivation \\nmakes clear, the squared error term (di -h(~~))~ follows directly from the exponent \\nin the definition of the Normal distribution. Similar derivations can be performed \\nstarting with other assumed noise distributions, producing different results. \\nNotice the structure of the above derivation involves selecting the hypothesis \\nthat maximizes the logarithm of the likelihood \\n(In p(D1h)) in order to determine \\nthe most probable hypothesis. As noted earlier, this yields the same result as max- \\nimizing the likelihood p(D1h). This approach of working with the log likelihood \\nis common to many Bayesian analyses, because it is often more mathematically \\ntractable than working directly with the likelihood. Of course, as noted earlier, \\nthe maximum likelihood hypothesis might not be the MAP hypothesis, but if one \\nassumes uniform prior probabilities over the hypotheses then it is. \\nWhy is it reasonable to choose the Normal distribution to characterize noise? \\nOne reason, it must be admitted, is that it allows for a mathematically straightfor- \\nward analysis. A second reason is that the smooth, bell-shaped distribution is a \\ngood approximation to many types of noise in physical systems. In fact, the Cen- \\ni tral Limit Theorem discussed in Chapter 5 shows that the sum of a sufficiently \\nlarge number of independent, identically distributed random variables itself obeys \\na Normal distribution, regardless of the distributions of the individual variables. \\nThis implies that noise generated by the sum of very many independent, but \\nidentically distributed factors will itself be Normally distributed. Of course, in \\nreality, different components that contribute to noise might not follow identical \\ndistributions, in which case this theorem will not necessarily justify our choice. \\nMinimizing the sum of squared errors is a common approach in many neural \\nnetwork, curve fitting, and other approaches to approximating real-valued func- \\ntions. Chapter 4 describes gradient descent methods that seek the least-squared \\nerror hypothesis in neural network learning. \\nBefore leaving our discussion of the relationship between the maximum \\nlikelihood hypothesis and the least-squared error hypothesis, it is important to \\nnote some limitations of this problem setting. The above analysis considers noise \\nonly in the target value of the training example and does not consider noise in \\nthe attributes describing the instances themselves. For example, if the problem \\nis to learn to predict the weight of someone based on that person's age and \\nheight, then the above analysis assumes noise in measurements of weight, but \\nperfect measurements of age and height. The analysis becomes significantly more \\ncomplex as these simplifying assumptions are removed. \\n6.5 MAXIMUM LIKELIHOOD HYPOTHESES FOR PREDICTING \\nPROBABILITIES \\nIn the problem setting of the previous section we determined that the maximum \\nlikelihood hypothesis is the one that minimizes the sum of squared errors over the \\ntraining examples. In this section we derive an analogous criterion for a second \\nsetting that is common in neural network learning: learning to predict probabilities. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 179}, page_content=\"Consider the setting in which we wish to learn a nondeterministic (prob- \\nabilistic) function f : X -+ {0, 11, which has two discrete output values. For \\nexample, the instance space X might represent medical patients in terms of their \\nsymptoms, and the target function f (x) might be 1 if the patient survives the \\ndisease and \\n0 \\nif not. Alternatively, X might represent loan applicants in terms of \\ntheir past credit history, and f (x) might be 1 if the applicant successfully repays \\ntheir next loan and \\n0 \\nif not. In both of these cases we might well expect f to be \\nprobabilistic. For example, among a collection of patients exhibiting the same set \\nof observable symptoms, we might find that 92% survive, and 8% do not. This \\nunpredictability could arise from our inability to observe all the important distin- \\nguishing features of the patients, or from some genuinely probabilistic mechanism \\nin the evolution of the disease. Whatever the source of the problem, the effect is \\nthat we have a target function f (x) whose output is a probabilistic function of the \\ninput. \\nGiven this problem setting, we might wish to learn a neural network (or other \\nreal-valued function approximator) whose output is the probability that f (x) = 1. \\nIn other words, we seek to learn the target function, f' : X + [O, 11, such that \\nf '(x) = P( f (x) = 1). In the above medical patient example, if x is one of those \\nindistinguishable patients of which 92% survive, then \\nf'(x) = 0.92 whereas the \\nprobabilistic function f (x) will be equal to 1 in 92% of cases and equal to 0 in \\nthe remaining 8%. \\nHow can we learn f' using, say, a neural network? One obvious, brute- \\nforce way would be to first collect the observed frequencies of 1's and 0's for \\neach possible value of x and to then train the neural network to output the target \\nfrequency for each x. As we shall see below, we can instead train a neural network \\ndirectly from the observed training examples of f, yet still derive a maximum \\nlikelihood hypothesis for f '. \\nWhat criterion should we optimize in order to find a maximum likelihood \\nhypothesis for f' in this setting? To answer this question we must first obtain \\nan expression for \\nP(D1h). \\nLet us assume the training data D is of the form \\nD = {(xl, dl) . . . (x,, dm)}, where di is the observed 0 or 1 value for f (xi). \\nRecall that in the maximum likelihood, least-squared error analysis of the \\nprevious section, we made the simplifying assumption that the instances \\n(xl . . . x,) \\nwere fixed. This enabled us to characterize the data by considering only the target \\nvalues \\ndi. \\nAlthough we could make a similar simplifying assumption in this case, \\nlet us avoid it here in order to demonstrate that it has no impact on the final \\noutcome. Thus treating both \\nxi \\nand \\ndi \\nas random variables, and assuming that \\neach training example is drawn independently, we can write \\nP(D1h) \\nas \\nm \\nP(Dlh) = n ,(xi, 41,) (6.7) \\ni=l \\nIt is reasonable to assume, furthermore, that the probability of encountering \\nany particular instance \\nxi \\nis independent of the hypothesis h. For example, the \\nprobability that our training set contains a particular patient xi is independent of \\nour hypothesis about survival rates (though of course the survival d, of the patient \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 180}, page_content='CHAPTER 6 BAYESIAN LEARNING 169 \\ndoes depend strongly on h). When x is independent of h we can rewrite the above \\nexpression (applying the product rule from Table 6.1) as \\nNow what is the probability P(dilh, xi) of observing di = 1 for a single \\ninstance xi, given a world in which hypothesis h holds? Recall that h is our \\nhypothesis regarding the target function, which computes this very probability. \\nTherefore, P(di = 1 1 h, xi) = h(xi), and in general \\nIn order to substitute this into the Equation (6.8) for P(Dlh), let us first \\n\" re-express it in a more mathematically manipulable form, as \\nI\\' \\nIt is easy to verify that the expressions in Equations (6.9) and (6.10) are equivalent. \\nNotice that when di = 1, the second term from Equation (6.10), (1 - h(xi))\\'-\", \\nbecomes equal to 1. Hence P(di = llh,xi) = h(xi), which is equivalent to the \\nfirst case in Equation (6.9). A similar analysis shows that the two equations are \\nalso equivalent when di = 0. \\nWe can use Equation (6.10) to substitute for P(di lh, xi) in Equation (6.8) to \\nobtain \\nNow we write an expression for the maximum likelihood hypothesis \\nThe last term is a constant independent of h, so it can be dropped \\nThe expression on the right side of Equation (6.12) can be seen as a gen- \\neralization of the Binomial distribution described in Table 5.3. The expression in \\nEquation (6.12) describes the probability that flipping each of m distinct coins will \\nproduce the outcome (dl . . .dm), assuming that each coin xi has probability h(xi) \\nof producing a heads. Note the Binomial distribution described in Table 5.3 is '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 181}, page_content='similar, but makes the additional assumption that the coins have identical proba- \\nbilities of turning up heads \\n(i.e., \\nthat \\nh(xi) = h(xj), Vi, j). In both cases we assume \\nthe outcomes of the coin flips are mutually independent-an assumption that fits \\nour current setting. \\nAs in earlier cases, we will find it easier to work with the log of the likeli- \\nhood, yielding \\nEquation (6.13) describes the quantity that must be maximized in order to \\nobtain the maximum likelihood hypothesis in our current problem setting. This \\nresult is analogous to our earlier result showing that minimizing the sum of squared \\nerrors produces the maximum likelihood hypothesis in the earlier problem setting. \\nNote the similarity between Equation (6.13) and the general form of the entropy \\nfunction, -xi pi log pi, discussed in Chapter 3. Because of this similarity, the \\nnegation of the above quantity is sometimes called the cross entropy. \\n6.5.1 Gradient Search to Maximize Likelihood in a Neural Net \\nAbove we showed that maximizing the quantity in Equation (6.13) yields the \\nmaximum likelihood hypothesis. Let us use \\nG(h, \\nD) to denote this quantity. In \\nthis section we derive a weight-training rule for neural network learning that seeks \\nto maximize \\nG(h, \\nD) using gradient ascent. \\nAs discussed in Chapter 4, the gradient of G(h, D) is given by the vector \\nof partial derivatives of \\nG(h, \\nD) with respect to the various network weights that \\ndefine the hypothesis h represented by the learned network (see Chapter 4 for a \\ngeneral discussion of gradient-descent search and for details of the terminology \\nthat we reuse here). In this case, the partial derivative of G(h, D) with respect to \\nweight \\nwjk \\nfrom input k to unit j is \\nTo keep our analysis simple, suppose our neural network is constructed from \\na single layer of sigmoid units. In this case we have \\nwhere \\nxijk \\nis the kth input to unit j for the ith training example, and d(x) is \\nthe derivative of the sigmoid squashing function (again, see Chapter 4). Finally, '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 182}, page_content='CIUPlER 6 BAYESIAN LEARNING 171 \\nsubstituting this expression into Equation (6.14), we obtain a simple expression \\nfor the derivatives that constitute the gradient \\nBecause we seek to maximize rather than minimize \\nP(D(h), \\nwe perform \\ngradient ascent rather than gradient descent search. On each iteration of the search \\nthe weight vector is adjusted in the direction of the gradient, using the \\nweight- \\nupdate rule \\nwhere \\nm \\nAwjk = 7 C(di - hbi)) xijk (6.15) \\ni=l \\nand where 7 is a small positive constant that determines the step size of the \\ni gradient ascent search. \\nIt is interesting to compare this weight-update rule to the weight-update \\nrule used by the \\nBACKPROPAGATION \\nalgorithm to minimize the sum of squared \\nerrors between predicted and observed network outputs. The \\nBACKPROPAGATION \\nupdate rule for output unit weights (see Chapter 4), re-expressed using our current \\nnotation, is \\nwhere \\nNotice this is similar to the rule given in Equation (6.15) except for the extra term \\nh(x,)(l - h(xi)), which is the derivative of the sigmoid function. \\nTo summarize, these two weight update rules converge toward maximum \\nlikelihood hypotheses in two different settings. The rule that minimizes sum of \\nsquared error seeks the maximum likelihood hypothesis under the assumption \\nthat the training data can be modeled by Normally distributed noise added to the \\ntarget function value. The rule that minimizes cross entropy seeks the maximum \\nlikelihood hypothesis under the assumption that the observed boolean value is a \\nprobabilistic function of the input instance. \\n6.6 MINIMUM DESCRIPTION LENGTH PRINCIPLE \\nRecall from Chapter 3 the discussion of Occam\\'s razor, a popular inductive bias \\nthat can be summarized as \"choose the shortest explanation for the observed \\ndata.\" In that chapter we discussed several arguments in the long-standing debate \\nregarding Occam\\'s razor. Here we consider a Bayesian perspective on this issue '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 183}, page_content='and a closely related principle called the Minimum Description Length (MDL) \\nprinciple. \\nThe Minimum Description Length principle is motivated by interpreting the \\ndefinition of hM~p in the light of basic concepts from information theory. Consider \\nagain the now familiar definition of  MAP. \\nhMAP = argmax P(Dlh)P(h) \\nh€H \\nwhich can be equivalently expressed in terms of maximizing the log, \\n MAP = argmax log2 P(D lh) + log, P(h) \\nh€H \\nor alternatively, minimizing the negative of this quantity \\nhMAp = argmin - log, P(D 1 h) - log, P(h) \\nh€H \\nSomewhat surprisingly, Equation (6.16) can be interpreted as a statement \\nthat short hypotheses are preferred, assuming a particular representation scheme \\nfor encoding hypotheses and data. To explain this, let us introduce a basic result \\nfrom information theory: Consider the problem of designing a code to transmit \\nmessages drawn at random, where the probability of encountering message i is \\npi. We are interested here in the most compact code; that is, we are interested in \\nthe code that minimizes the expected number of bits we must transmit in order to \\nencode a message drawn at random. Clearly, to minimize the expected code length \\nwe should assign shorter codes to messages that are more probable. Shannon and \\nWeaver (1949) showed that the optimal code \\n(i.e., \\nthe code that minimizes the \\nexpected message length) assigns - log, pi bitst to encode message i. We will \\nrefer to the number of bits required to encode message i using code C as the \\ndescription length of message i with respect to C, which we denote by Lc(i). \\nLet us interpret Equation (6.16) in light of the above result from coding \\ntheory. \\n0 - log, P(h) is the description length of h under the optimal encoding for \\nthe hypothesis space H. In other words, this is the size of the description \\nof hypothesis h using this optimal representation. In our notation, LC, (h) = \\n- log, P(h), where CH is the optimal code for hypothesis space H. \\n0 -log2 P(D1h) is the description length of the training data D given \\nhypothesis h, under its optimal encoding. In our notation, Lc,,,(Dlh) = \\n- log, P(Dlh), where CD,~ is the optimal code for describing data D assum- \\ning that both the sender and receiver know the hypothesis h. \\nt~otice the expected length for transmitting one message is therefore xi -pi logz pi, the formula \\nfor the entropy (see Chapter 3) of the set of possible messages. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 184}, page_content=\"CHAPTER 6 BAYESIAN LEARNING 173 \\n0 Therefore we can rewrite Equation (6.16) to show that hMAP is the hypothesis \\nh that minimizes the sum given by the description length of the hypothesis \\nplus the description length of the data given the hypothesis. \\nwhere \\nCH \\nand \\nCDlh \\nare the optimal encodings for H and for D given h, \\nrespectively. \\nThe Minimum Description Length (MDL) principle recommends choosing \\nthe hypothesis that minimizes the sum of these two description lengths. Of course \\nto apply this principle in practice we must choose specific encodings or represen- \\ntations appropriate for the given learning task. Assuming we use the codes \\nC1 \\nand \\nCZ to represent the hypothesis and the data given the hypothesis, we can state the \\nMDL principle as \\n1' \\nI Minimum Description Length principle: Choose hMDL where \\nThe above analysis shows that if we choose C1 to be the optimal encoding \\nof hypotheses \\nCH, \\nand if we choose C2 to be the optimal encoding CDlh, then \\n~MDL = A MAP. \\nIntuitively, we can think of the MDL principle as recommending the shortest \\nmethod for re-encoding the training data, where we count both the size of the \\nhypothesis and any additional cost of encoding the data given this hypothesis. \\nLet us consider an example. Suppose we wish to apply the MDL prin- \\nciple to the problem of learning decision trees from some training data. What \\nshould we choose for the representations \\nC1 \\nand C2 of hypotheses and data? \\nFor \\nC1 \\nwe might naturally choose some obvious encoding of decision trees, in \\nwhich the description length grows with the number of nodes in the tree and \\nwith the number of edges. How shall we choose the encoding C2 of the data \\ngiven a particular decision tree hypothesis? To keep things simple, suppose that \\nthe sequence of instances \\n(xl . . .x,) is already known to both the transmitter \\nand receiver, so that we need only transmit the classifications (f (XI) . . . f (x,)). \\n(Note the cost of transmitting the instances themselves is independent of the cor- \\nrect hypothesis, so it does not affect the selection of ~MDL in any case.) Now if \\nthe training classifications (f (xl) . . . f (xm)) are identical to the predictions of the \\nhypothesis, then there is no need to transmit any information about these exam- \\nples (the receiver can compute these values once it has received the hypothesis). \\nThe description length of the classifications given the hypothesis in this case is, \\ntherefore, zero. In the case where some examples are misclassified by h, then \\nfor each misclassification we need to transmit a message that identifies which \\nexample is \\nmisclassified \\n(which can be done using at most \\nlogzm \\nbits) as well \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 185}, page_content='as its correct classification (which can be done using at most log2 k bits, where \\nk is the number of possible classifications). The hypothesis hMDL under the en- \\ncoding~ \\nC1 \\nand \\nC2 \\nis just the one that minimizes the sum of these description \\nlengths. \\nThus the MDL principle provides a way of trading off hypothesis complexity \\nfor the number of errors committed by the hypothesis. It might select a shorter \\nhypothesis that makes a few errors over a longer hypothesis that perfectly classifies \\nthe training data. Viewed in this light, it provides one method for dealing with \\nthe issue of overjitting the data. \\nQuinlan \\nand \\nRivest \\n(1989) describe experiments applying the MDL principle \\nto choose the best size for a decision tree. They report that the MDL-based method \\nproduced learned trees whose accuracy was comparable to that of the standard \\ntree- \\npruning methods discussed in Chapter 3. Mehta et al. (1995) describe an alternative \\nMDL-based approach to decision tree pruning, and describe experiments in which \\nan MDL-based approach produced results comparable to standard tree-pruning \\nmethods. \\nWhat shall we conclude from this analysis of the Minimum Description \\nLength principle? Does this prove once and for all that short hypotheses are best? \\nNo. What we have shown is only that \\nifa \\nrepresentation of hypotheses is chosen so \\nthat the size of hypothesis h is - log2 P(h), and ifa representation for exceptions \\nis chosen so that the encoding length of D given h is equal to -log2 \\nP(Dlh), \\nthen the MDL principle produces MAP hypotheses. However, to show that we \\nhave such a representation we must know all the prior probabilities \\nP(h), \\nas well \\nas the \\nP(D1h). \\nThere is no reason to believe that the MDL hypothesis relative to \\narbitrary encodings C1 and C2 should be preferred. As a practical matter it might \\nsometimes be easier for a human designer to specify a representation that captures \\nknowledge about the relative probabilities of hypotheses than it is to fully specify \\nthe probability of each hypothesis. Descriptions in the literature on the application \\nof MDL to practical learning problems often include arguments providing some \\nform of justification for the encodings chosen for \\nC1 \\nand \\nC2. \\n6.7 BAYES OPTIMAL CLASSIFIER \\nSo far we have considered the question \"what is the most probable hypothesis \\ngiven the training data?\\' In fact, the question that is often of most significance is \\nthe closely related question \"what is the most probable classiJication of the new \\ninstance given the training \\ndata?\\'Although \\nit may seem that this second question \\ncan be answered by simply applying the MAP hypothesis to the new instance, in \\nfact it is possible to do better. \\nTo develop some intuitions consider a hypothesis space containing three \\nhypotheses, \\nhl, \\nh2, \\nand \\nh3. \\nSuppose that the posterior probabilities of these hy- \\npotheses given the training data are .4, .3, and .3 respectively. Thus, hl is the \\nMAP hypothesis. Suppose a new instance x is encountered, which is classified \\npositive by hl, but negative by h2 and h3. Taking all hypotheses into account, \\nthe probability that x is positive is .4 (the probability associated with hi), and '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 186}, page_content='CHAFER 6 BAYESIAN LEARNING 175 \\nthe probability that it is negative is therefore .6. The most probable classification \\n(negative) in this case is different from the classification generated by the MAP \\nhypothesis. \\nIn general, the most probable classification of the new instance is obtained \\nby combining the predictions of all hypotheses, weighted by their posterior prob- \\nabilities. If the possible classification of the new example can take on any value \\nvj from some set V, then the probability P(vjlD) that the correct classification \\nfor the new instance is \\nv;, \\nis just \\nThe optimal classification of the new instance is the value v,, for which \\nP (v; 1 D) is maximum. \\nBayes optimal classification: \\nTo illustrate in terms of the above example, the set of possible classifications \\nof the new instance is V = (@, 81, and \\ntherefore \\nand \\nAny system that classifies new instances according to Equation (6.18) is \\ncalled a Bayes optimal classzjier, or Bayes optimal learner. No other classification \\nmethod using the same hypothesis space and same prior knowledge can outperform \\nthis method on average. This method maximizes the probability that the new \\ninstance is classified correctly, given the available data, hypothesis space, and \\nprior probabilities over the hypotheses. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 187}, page_content=\"For example, in learning boolean concepts using version spaces as in the \\nearlier section, the Bayes optimal classification of a new instance is obtained \\nby taking a weighted vote among all members of the version space, with each \\ncandidate hypothesis weighted by its posterior probability. \\nNote one curious property of the Bayes optimal classifier is that the pre- \\ndictions it makes can correspond to a hypothesis not contained in H! Imagine \\nusing Equation (6.18) to classify every instance in X. The labeling of instances \\ndefined in this way need not correspond to the instance labeling of any single \\nhypothesis h from H. One way to view this situation is to think of the Bayes \\noptimal classifier as effectively considering a hypothesis space H' different from \\nthe space of hypotheses H to which Bayes theorem is being applied. In particu- \\nlar, H' effectively includes hypotheses that perform comparisons between linear \\ncombinations of predictions from multiple hypotheses in H. \\n6.8 GIBBS ALGORITHM \\nAlthough the Bayes optimal classifier obtains the best performance that can be \\nachieved from the given training data, it can be quite costly to apply. The expense \\nis due to the fact that it computes the posterior probability for every hypothesis \\nin H and then combines the predictions of each hypothesis to classify each new \\ninstance. \\nAn alternative, less optimal method is the Gibbs algorithm (see Opper and \\nHaussler \\n1991), \\ndefined as follows: \\n1. Choose a hypothesis h from H at random, according to the posterior prob- \\nability distribution over H. \\n2. Use h to predict the classification of the next instance x. \\nGiven a new instance to classify, the Gibbs algorithm simply applies a \\nhypothesis drawn at random according to the current posterior probability distri- \\nbution. Surprisingly, it can be shown that under certain conditions the expected \\nmisclassification error for the Gibbs algorithm is at most twice the expected error \\nof the Bayes optimal classifier (Haussler et al. 1994). More precisely, the ex- \\npected value is taken over target concepts drawn at random according to the prior \\nprobability distribution assumed by the learner. Under this condition, the expected \\nvalue of the error of the Gibbs algorithm is at worst twice the expected value of \\nthe error of the Bayes optimal classifier. \\nThis result has an interesting implication for the concept learning problem \\ndescribed earlier. In particular, it implies that if the learner assumes a uniform \\nprior over H, and if target concepts are in fact drawn from such a distribution \\nwhen presented to the learner, then classifying the next instance according to \\na hypothesis drawn at random from the current version space (according to a \\nuniform distribution), will have expected error at most twice that of the Bayes \\noptimal \\nclassijier. Again, we have an example where a Bayesian analysis of a \\nnon-Bayesian algorithm yields insight into the performance of that algorithm. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 188}, page_content='CHAPTJZR 6 BAYESIAN LEARNING 177 \\n6.9 NAIVE BAYES CLASSIFIER \\nOne highly practical Bayesian learning method is the naive Bayes learner, often \\ncalled the naive Bayes classijier. In some domains its performance has been shown \\nto be comparable to that of neural network and decision tree learning. This section \\nintroduces the naive Bayes classifier; the next section applies it to the practical \\nproblem of learning to classify natural language text documents. \\nThe naive Bayes classifier applies to learning tasks where each instance x \\nis described by a conjunction of attribute values and where the target function \\nf (x) can take on any value from some finite set V. A set of training examples of \\nthe target function is provided, and a new instance is presented, described by the \\ntuple of attribute values (al, a2.. .a,). The learner is asked to predict the target \\nvalue, or classification, for this new instance. \\nThe Bayesian approach to classifying the new instance is to assign the most \\nprobable target value, \\nVMAP, \\ngiven the attribute values (al, a2 . . . a,) that describe \\nthe instance. \\nVMAP = argmax P(vjlal, a2. . . a,) \\nvj€v \\nWe can use Bayes theorem to rewrite this expression as \\nNow we could attempt to estimate the two terms in Equation (6.19) based on \\nthe training data. It is easy to estimate each of the P(vj) simply by counting the \\nfrequency with which each target value vj occurs in the training data. However, \\nestimating the different P(al, a2.. . a,lvj) terms in this fashion is not feasible \\nunless we have a very, very large set of training data. The problem is that the \\nnumber of these terms is equal to the number of possible instances times the \\nnumber of possible target values. Therefore, we need to see every instance in \\nthe instance space many times in order to obtain reliable estimates. \\nThe naive Bayes classifier is based on the simplifying assumption that the \\nattribute values are conditionally independent given the target value. In other \\nwords, the assumption is that given the target value of the instance, the probability \\nof observing the conjunction al, a2.. .a, is just the product of the probabilities \\nfor the individual attributes: P(a1, a2 . . . a, 1 vj) = ni P(ai lvj). Substituting this \\ninto Equation (6.19), we have the approach used by the naive Bayes classifier. \\nNaive Bayes classifier: \\nVNB = argmax P (vj) n P (ai 1vj) (6.20) \\nujcv \\nwhere VNB denotes the target value output by the naive Bayes classifier. Notice \\nthat in a naive Bayes classifier the number of distinct P(ailvj) terms that must '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 189}, page_content='be estimated from the training data is just the number of distinct attribute values \\ntimes the number of distinct target values-a much smaller number than if we \\nwere to estimate the \\nP(a1, \\na2 . . . an lvj) terms as first contemplated. \\nTo summarize, the naive Bayes learning method involves a learning step in \\nwhich the various \\nP(vj) \\nand \\nP(ai \\nJvj) \\nterms are estimated, based on their frequen- \\ncies over the training data. The set of these estimates corresponds to the learned \\nhypothesis. This hypothesis is then used to classify each new instance by applying \\nthe rule in Equation (6.20). Whenever the naive Bayes assumption of conditional \\nindependence is satisfied, this naive Bayes classification VNB is identical to the \\nMAP classification. \\nOne interesting difference between the naive Bayes learning method and \\nother learning methods we have considered is that there is no explicit search \\nthrough the space of possible hypotheses (in this case, the space of possible \\nhypotheses is the space of possible values that can be assigned to the various \\nP(vj) \\nand \\nP(ailvj) \\nterms). Instead, the hypothesis is formed without searching, simply by \\ncounting the frequency of various data combinations within the training examples. \\n6.9.1 An Illustrative Example \\nLet us apply the naive Bayes classifier to a concept learning problem we consid- \\nered during our discussion of decision tree learning: classifying days according \\nto whether someone will play tennis. Table 3.2 from Chapter 3 provides a set \\nof 14 training examples of the target concept \\nPlayTennis, \\nwhere each day is \\ndescribed by the attributes Outlook, Temperature, Humidity, and Wind. Here we \\nuse the naive Bayes classifier and the training data from this table to classify the \\nfollowing novel instance: \\n(Outlook = sunny, Temperature = cool, Humidity = high, Wind = strong) \\nOur task is to predict the target value (yes or no) of the target concept \\nPlayTennis \\nfor this new instance. Instantiating Equation (6.20) to fit the current \\ntask, the target value VNB is given by \\n= argrnax P(vj) P(0utlook = sunny)v,)P(Temperature = coolIvj) \\nvj~(yes,no] \\nNotice in the final expression that ai has been instantiated using the particular \\nattribute values of the new instance. To calculate VNB we now require 10 proba- \\nbilities that can be estimated from the training data. First, the probabilities of the \\ndifferent target values can easily be estimated based on their frequencies over the \\n14 training examples \\nP(P1ayTennis = yes) = 9/14 = .64 \\nP(P1ayTennis = no) = 5/14 = .36 '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 190}, page_content='CHAETER 6 BAYESIAN LEARNING 179 \\nSimilarly, we can estimate the conditional probabilities. For example, those for \\nWind = strong are \\nP(Wind = stronglPlayTennis = yes) = 319 = .33 \\nP(Wind = strongl PlayTennis = no) = 315 = .60 \\nUsing these probability estimates and similar estimates for the remaining attribute \\nvalues, we calculate VNB according to Equation (6.21) as follows (now omitting \\nattribute names for brevity) \\nThus, the naive Bayes classifier assigns the target value PlayTennis = no to this \\nnew instance, based on the probability estimates learned from the training data. \\nFurthermore, by normalizing the above quantities to sum to one we can calculate \\nthe conditional probability that the target value is no, given the observed attribute \\nvalues. For the current example, this probability is ,02$ym,, = -795. \\n6.9.1.1 ESTIMATING PROBABILITIES \\nUp to this point we have estimated probabilities by the fraction of times the event \\nis observed to occur over the total number of opportunities. For example, in the \\nabove case we estimated P(Wind = strong] Play Tennis = no) by the fraction % \\nwhere n = 5 is the total number of training examples for which PlayTennis = no, \\nand n, = 3 is the number of these for which Wind = strong. \\nWhile this observed fraction provides a good estimate of the probability in \\nmany cases, it provides poor estimates when n, is very small. To see the difficulty, \\nimagine that, in fact, the value of P(Wind = strongl PlayTennis = no) is .08 and \\nthat we have a sample containing only 5 examples for which PlayTennis = no. \\nThen the most probable value for n, is 0. This raises two difficulties. First, $ pro- \\nduces a biased underestimate of the probability. Second, when this probability es- \\ntimate is zero, this probability term will dominate the Bayes classifier if the future \\nquery contains Wind = strong. The reason is that the quantity calculated in Equa- \\ntion (6.20) requires multiplying all the other probability terms by this zero value. \\nTo avoid this difficulty we can adopt a Bayesian approach to estimating the \\nprobability, using the m-estimate defined as follows. \\nm-estimate of probability: \\nHere, n, and n are defined as before, p is our prior estimate of the probability \\nwe wish to determine, and m is a constant called the equivalent sample size, \\nwhich determines how heavily to weight p relative to the observed data. A typical \\nmethod for choosing p in the absence of other information is to assume uniform '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 191}, page_content='priors; that is, if an attribute has k possible values we set p = i. For example, in \\nestimating P(Wind = stronglPlayTennis = no) we note the attribute Wind has \\ntwo possible values, so uniform priors would correspond to choosing p = .5. Note \\nthat if m is zero, the m-estimate is equivalent to the simple fraction 2. If both n \\nand m are nonzero, then the observed fraction 2 and prior p will be combined \\naccording to the weight m. The reason m is called the equivalent sample size is \\nthat Equation (6.22) can be interpreted as augmenting the n actual observations \\nby an additional m virtual samples distributed according to p. \\n6.10 AN EXAMPLE: LEARNING TO CLASSIFY TEXT \\nTo illustrate the practical importance of Bayesian learning methods, consider learn- \\ning problems in which the instances are text documents. For example, we might \\nwish to learn the target concept \"electronic news articles that I find interesting,\" \\nor \"pages on the World Wide Web that discuss machine learning topics.\" In both \\ncases, if a computer could learn the target concept accurately, it could automat- \\nically filter the large volume of online text documents to present only the most \\nrelevant documents to the user. \\nWe present here a general algorithm for learning to classify text, based \\non the naive Bayes classifier. Interestingly, probabilistic approaches such as the \\none described here are among the most effective algorithms currently known for \\nlearning to classify text documents. Examples of such systems are described by \\nLewis \\n(1991), \\nLang \\n(1995), \\nand Joachims (1996). \\nThe naive Bayes algorithm that we shall present applies in the following \\ngeneral setting. Consider an instance space X consisting of all possible text docu- \\nments (i.e., all possible strings of words and punctuation of all possible lengths). \\nWe are given training examples of some unknown target function f (x), which \\ncan take on any value from some finite set V. The task is to learn from these \\ntraining examples to predict the target value for subsequent text documents. For \\nillustration, we will consider the target function classifying documents as interest- \\ning or uninteresting to a particular person, using the target values like and dislike \\nto indicate these two classes. \\nThe two main design issues involved in applying the naive Bayes classifier \\nto such \\nrext \\nclassification problems are first to decide how to represent an arbitrary \\ntext document in terms of attribute values, and second to decide how to estimate \\nthe probabilities required by the naive Bayes classifier. \\nOur approach to representing arbitrary text documents is disturbingly simple: \\nGiven a text document, such as this paragraph, we define an attribute for each word \\nposition in the document and define the value of that attribute to be the English \\nword found in that position. Thus, the current paragraph would be described by \\n11 1 attribute values, corresponding to the 11 1 word positions. The value of the \\nfirst attribute is the word \"our,\" the value of the second attribute is the word \\n\"approach,\" and so on. Notice that long text documents will require a larger \\nnumber of attributes than short documents. As we shall see, this will not cause \\nus any trouble. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 192}, page_content='CHAPTER 6 BAYESIAN LEARNING 181 \\nGiven this representation for text documents, we can now apply the naive \\nBayes classifier. For the sake of concreteness, let us assume we are given a set of \\n700 training documents that a friend has classified as dislike and another 300 she \\nhas classified as \\nlike. \\nWe are now given a new document and asked to classify \\nit. Again, for concreteness let us assume the new text document is the preceding \\nparagraph. In this case, we instantiate Equation (6.20) to calculate the naive Bayes \\nclassification as \\n-a- \\nVns = argmax P(Vj) n ~(ai lvj) \\nvj~{like,dislike} i=l \\n- - argmax P(vj) P(a1 = \"our\"lvj)P(a2 = \"approach\"lvj) \\nv, ~{like,dislike} \\nTo summarize, the naive Bayes classification VNB is the classification that max- \\nimizes the probability of observing the words that were actually found in the \\nI document, subject to the usual naive Bayes independence assumption. The inde- \\nF pendence assumption P(al, . . . alll lvj) = nfL1 P(ai lvj) states in this setting that \\nthe word probabilities for one text position are independent of the words that oc- \\ncur in other positions, given the document classification vj. Note this assumption \\nis clearly incorrect. For example, the probability of observing the word \"learning\" \\nin some position may be greater if the preceding word is \"machine.\" Despite the \\nobvious inaccuracy of this independence assumption, we have little choice but to \\nmake it-without it, the number of probability terms that must be computed is \\nprohibitive. Fortunately, in practice the naive Bayes learner performs remarkably \\nwell in many text classification problems despite the incorrectness of this indepen- \\ndence assumption. Dorningos and \\nPazzani \\n(1996) provide an interesting analysis \\nof this fortunate phenomenon. \\nTo calculate VNB using the above expression, we require estimates for the \\nprobability terms \\nP(vj) \\nand \\nP(ai = wklvj) (here we introduce wk to indicate the kth \\nword in the English vocabulary). The first of these can easily be estimated based \\non the fraction of each class in the training data \\n(P(1ike) = .3 and P(dis1ike) = .7 \\nin the current example). As usual, estimating the class conditional probabilities \\n(e.g., \\nP(al = \"our\"ldis1ike)) is more problematic because we must estimate one \\nsuch probability term for each combination of text position, English word, and \\ntarget value. Unfortunately, there are approximately 50,000 distinct words in the \\nEnglish vocabulary, 2 possible target values, and 11 1 text positions in the current \\nexample, so we must estimate \\n2. \\n11 \\n1 \\n-50,000 = 10 million such terms from the \\ntraining data. \\nFortunately, we can make an additional reasonable assumption that reduces \\nthe \\nnumber of probabilities that must be estimated. In particular, we shall as- \\nsume the probability of encountering a specific word \\nwk \\n(e.g., \\n\"chocolate\") is \\nindependent of the specific word position being considered \\n(e.g., \\na23 \\nversus \\nagg). \\nMore formally, this amounts to assuming that the attributes are independent and \\nidentically distributed, given the target classification; that is, \\nP(ai = wk)vj) = '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 193}, page_content='P(a, = wkJvj) for all i, j, k, m. Therefore, we estimate the entire set of proba- \\nbilities \\nP(a1 = wk lvj), P(a2 = wk lv,) . . . by the single position-independent prob- \\nability \\nP(wklvj), \\nwhich we will use regardless of the word position. The net \\neffect is that we now require only \\n2.50,000 \\ndistinct terms of the form \\nP(wklvj). \\nThis is still a large number, but manageable. Notice in cases where training data \\nis limited, the primary advantage of \\nmaking \\nthis assumption is that it increases \\nthe number of examples available to estimate each of the required probabilities, \\nthereby increasing the reliability of the estimates. \\nTo complete the design of our learning algorithm, we must still choose a \\nmethod for estimating the probability terms. We adopt the \\nm-estimate-Equa- \\ntion \\n(6.22)-with uniform priors and with rn equal to the size of the word vocab- \\nulary. Thus, the estimate for \\nP(wklvj) \\nwill be \\nwhere n is the total number of word positions in all training examples whose \\ntarget value is \\nvj, \\nnk \\nis the number of times word \\nwk \\nis found among these n \\nword positions, and I Vocabulary I is the total number of distinct words (and other \\ntokens) found within the training data. \\nTo summarize, the final algorithm uses a naive Bayes classifier together \\nwith the assumption that the probability of word occurrence is independent of \\nposition within the text. The final algorithm is shown in Table 6.2. Notice the al- \\ngorithm is quite simple. During learning, the procedure \\nLEARN~AIVEBAYES-TEXT \\nexamines all training documents to extract the vocabulary of all words and to- \\nkens that appear in the text, then counts their frequencies among the different \\ntarget classes to obtain the necessary probability estimates. Later, given a new \\ndocument to be classified, the procedure \\nCLASSINSAIVEJ~AYES-TEXT \\nuses these \\nprobability estimates to calculate VNB according to Equation (6.20). Note that \\nany words appearing in the new document that were not observed in the train- \\ning set are simply ignored by CLASSIFYSAIVEBAYES-TEXT. Code for this algo- \\nrithm, as well as training data sets, are available on the World Wide Web at \\nhttp://www.cs.cmu.edu/-tom/book.htrnl. \\n6.10.1 Experimental Results \\nHow effective is the learning algorithm of Table 6.2? In one experiment (see \\nJoachims \\n1996), \\na minor variant of this algorithm was applied to the problem \\nof classifying \\nusenet \\nnews articles. The target classification for an article in this \\ncase was the name of the \\nusenet \\nnewsgroup in which the article appeared. One \\ncan \\nthink \\nof the task as creating a newsgroup posting service that learns to as- \\nsign documents to the appropriate newsgroup. In the experiment described by \\nJoachims \\n(1996), \\n20 electronic newsgroups were considered (listed in Table 6.3). \\nThen 1,000 articles were collected from each newsgroup, forming a data set of \\n20,000 documents. The naive Bayes algorithm was then applied using two-thirds \\nof these 20,000 documents as training examples, and performance was measured '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 194}, page_content='CHAPTER 6 BAYESIAN LEARNING 183 \\nExamples is a set of text documents along with their target values. V is the set of all possible target \\nvalues. This function learns the probability terms \\nP(wk \\nIv,), \\ndescribing the probability that a randomly \\ndrawn word from a document in class \\nvj \\nwill be the English word \\nwk. \\nIt also learns the class prior \\nprobabilities \\nP(vj). \\n1. collect all words, punctwtion, and other tokens that occur in Examples \\na Vocabulary c the set of all distinct words and other tokens occurring in any text document \\nfrom Examples \\n2. calculate the required P(vj) and P(wkJvj) probability terms \\nFor each target value vj in V do \\ndocsj t the subset of documents from Examples for which the target value is vj \\nldocs . I \\nP(uj) + 1ExornLlesl \\na Texti c a single document created by concatenating all members of docsi \\na n +*total number of distinct word positions in ~exc \\n0 for each word wk in Vocabulary \\n0 nk c number of times word wk occurs in Textj \\nP(wk lvj) + n+12LLoryl \\n\" Return the estimated target value for the document Doc. ai denotes the word found in the ith position \\nwithin Doc. \\n0 positions t all word positions in Doc that contain tokens found in Vocabulary \\na Return VNB, where \\nVNB = argmax ~(vj) n P(ai 19) \\nV, EV ieposirions \\nTABLE 6.2 \\nNaive Bayes algorithms for learning and classifying text. In addition to the usual naive Bayes as- \\nsumptions, these algorithms assume the probability of a word occurring is independent of its position \\nwithin the text. \\nover the remaining third. Given 20 possible newsgroups, we would expect random \\nguessing to achieve a classification accuracy of approximately 5%. The accuracy \\nachieved by the program was 89%. The algorithm used in these experiments was \\nexactly the algorithm of Table 6.2, with one exception: Only a subset of the words \\noccurring in the documents were included as the value of the Vocabulary vari- \\nable in the algorithm. In particular, the 100 most frequent words were removed \\n(these include words such as \"the\" and \"of \\'), and any word occurring fewer than \\nthree times was also removed. The resulting vocabulary contained approximately \\n38,500 words. \\nSimilarly impressive results have been achieved by others applying similar \\nstatistical learning approaches to text classification. For example, Lang (1995) \\ndescribes another variant of the naive Bayes algorithm and its application to \\nlearning the target concept \\n\"usenet \\narticles that I find interesting.\" He describes \\nthe \\nNEWSWEEDER \\nsystem-a program for reading \\nnetnews \\nthat allows the user to \\nrate articles as he or she reads them. \\nNEWSWEEDER \\nthen uses these rated articles as '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 195}, page_content='TABLE 6.3 \\nTwenty usenet newsgroups used in the text classification experiment. After training on 667 articles \\nfrom each newsgroup, a naive Bayes classifier achieved an accuracy of 89% predicting to which \\nnewsgroup subsequent articles belonged. Random guessing would produce an accuracy of only 5%. \\ntraining examples to learn to predict which subsequent articles will be of interest \\nto the user, so that it can bring these to the user\\'s attention. Lang (1995) reports \\nexperiments in which \\nNEWSWEEDER \\nused its learned profile of user interests to \\nsuggest the most highly rated new articles each day. By presenting the user with \\nthe top 10% of its automatically rated new articles each day, it created a pool of \\narticles containing three to four times as many interesting articles as the general \\npool of articles read by the user. For example, for one user the fraction of articles \\nrated \"interesting\" was 16% overall, but was 59% among the articles recommended \\nby \\nNEWSWEEDER. \\nSeveral other, non-Bayesian, statistical text learning algorithms are common, \\nmany based on similarity metrics initially developed for information retrieval \\n(e.g., \\nsee Rocchio \\n197 \\n1; \\nSalton \\n199 \\n1). Additional text learning algorithms are described \\nin \\nHearst \\nand Hirsh (1996). \\n6.11 BAYESIAN BELIEF NETWORKS \\nAs discussed in the previous two sections, the naive Bayes classifier makes signif- \\nicant use of the assumption that the values of the attributes a1 . . .a, are condition- \\nally \\nindependent given the target value v. This assumption dramatically reduces \\nthe complexity of learning the target function. When it is met, the naive Bayes \\nclassifier outputs the optimal Bayes classification. However, in many cases this \\nconditional independence assumption is clearly overly restrictive. \\nA Bayesian belief network describes the probability distribution governing a \\nset of variables by specifying a set of conditional independence assumptions along \\nwith a set of conditional probabilities. In contrast to the naive Bayes classifier, \\nwhich assumes that all the variables are conditionally independent given the value \\nof the target variable, Bayesian belief networks allow stating conditional indepen- \\ndence assumptions that apply to subsets of the variables. Thus, Bayesian belief \\nnetworks provide an intermediate approach that is less constraining than the global \\nassumption of conditional independence made by the naive Bayes classifier, but \\nmore tractable than avoiding conditional independence assumptions altogether. \\nBayesian belief networks are an active focus of current research, and a variety of \\nalgorithms have been proposed for learning them and for using them for inference. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 196}, page_content=\"CHAPTER 6 BAYESIAN LEARNING 185 \\nIn this section we introduce the key concepts and the representation of Bayesian \\nbelief networks. More detailed treatments are given by Pearl (1988), Russell and \\nNorvig (1995), Heckerman et al. (1995), and Jensen (1996). \\nIn general, a Bayesian belief network describes the probability distribution \\nover a set of variables. Consider an arbitrary set of random variables Yl . . . Y,, \\nwhere each variable Yi can take on the set of possible values V(Yi). We define \\nthe joint space of the set of variables Y to be the cross product V(Yl) x V(Y2) x \\n. . . V(Y,). In other words, each item in the joint space corresponds to one of the \\npossible assignments of values to the tuple of variables (Yl . . . Y,). The probability \\ndistribution over this joint' space is called the joint probability distribution. The \\njoint probability distribution specifies the probability for each of the possible \\nvariable bindings for the tuple (Yl . . . Y,). A Bayesian belief network describes \\nthe joint probability distribution for a set of variables. \\n6.11.1 Conditional Independence \\ni Let us begin our discussion of Bayesian belief networks by defining precisely \\nthe notion of conditional independence. Let X, Y, and Z be three discrete-valued \\nrandom variables. We say that X is conditionally independent of Y given Z if \\nthe probability distribution governing X is independent of the value of Y given a \\nvalue for 2; that is, if \\nwhere xi E V(X), yj E V(Y), and zk E V(Z). We commonly write the above \\nexpression in abbreviated form as P(XIY, Z) = P(X1Z). This definition of con- \\nditional independence can be extended to sets of variables as well. We say that \\nthe set of variables X1 . . . Xi is conditionally independent of the set of variables \\nYl . . . Ym given the set of variables 21 . . . Z, if \\nP(X1 ... XIJY1 ... Ym, z1 ... Z,) = P(Xl ... X1]Z1 ... Z,) \\nNote the correspondence between this definition and our use of conditional , \\nindependence in the definition of the naive Bayes classifier. The naive Bayes \\nclassifier assumes that the instance attribute A1 is conditionally independent of \\ninstance attribute A2 given the target value V. This allows the naive Bayes clas- \\nsifier to calculate P(Al, A21V) in Equation (6.20) as follows \\nEquation (6.23) is just the general form of the product rule of probability from \\nTable 6.1. Equation (6.24) follows because if A1 is conditionally independent of \\nA2 given V, then by our definition of conditional independence P (A1 IA2, V) = \\nP(A1IV). \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 197}, page_content='S,B S,-B 7S.B 1s.-B \\n-C 0.6 0.9 0.2 \\nCampfire \\nFIGURE 6.3 \\nA Bayesian belief network. The network on the left represents a set of conditional independence \\nassumptions. In particular, each node is asserted to be conditionally independent of its \\nnondescen- \\ndants, given its immediate parents. Associated with each node is a conditional probability table, \\nwhich specifies the conditional distribution for the variable given its immediate parents in the graph. \\nThe conditional probability table for the Campjire node is shown at the right, where Campjire is \\nabbreviated to C, Storm abbreviated to S, and BusTourGroup abbreviated to B. \\n6.11.2 Representation \\nA Bayesian belief network (Bayesian network for short) represents the joint prob- \\nability distribution for a set of variables. For example, the Bayesian network in \\nFigure 6.3 represents the joint probability distribution over the boolean variables \\nStorm, Lightning, Thunder, ForestFire, Campjre, and BusTourGroup. In general, \\na Bayesian network represents the joint probability distribution by specifying a \\nset of conditional independence assumptions (represented by a directed acyclic \\ngraph), together with sets of local conditional probabilities. Each variable in the \\njoint space is represented by a node in the Bayesian network. For each variable two \\ntypes of information are specified. First, the network arcs represent the assertion \\nthat the variable is conditionally independent of its nondescendants in the network \\ngiven its immediate predecessors in the network. We say \\nXjis \\na descendant of \\n, Y if there is a directed path from Y to X. Second, a conditional probability table \\nis given for each variable, describing the probability distribution for that variable \\ngiven the values of its immediate predecessors. The joint probability for any de- \\nsired assignment of values (yl, . . . , y,) to the tuple of network variables (YI . . . Y,) \\ncan be computed by the formula \\nn \\n~(YI,. . . , yd = np(yi~parents(~i)) \\ni=l \\nwhere Parents(Yi) denotes the set of immediate predecessors of Yi in the net- \\nwork. Note the values of P(yiJ Parents(Yi)) are precisely the values stored in the \\nconditional probability table associated with node Yi. \\nTo illustrate, the Bayesian network in Figure 6.3 represents the joint prob- \\nability distribution over the boolean variables Storm, Lightning, Thunder, Forest- '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 198}, page_content='CHmR 6 BAYESIAN LEARNING 187 \\nFire, Campfire, and BusTourGroup. Consider the node Campjire. The network \\nnodes and arcs represent the assertion that CampJire is conditionally indepen- \\ndent of its nondescendants Lightning and Thunder, given its immediate parents \\nStorm and BusTourGroup. This means that once we know the value of the vari- \\nables Storm and BusTourGroup, the variables Lightning and Thunder provide no \\nadditional information about Campfire. The right side of the figure shows the \\nconditional probability table associated with the variable Campfire. The top left \\nentry \\nin this table, for example, expresses the assertion that \\nP(Campfire = TruelStorm = True, BusTourGroup = True) = 0.4 \\nNote this table provides only the conditional probabilities of Campjire given its \\nparent variables Storm and BusTourGroup. The set of local conditional probability \\ntables for all the variables, together with the set of conditional independence as- \\nsumptions described by the network, describe the full joint probability distribution \\nfor the network. \\nOne attractive feature of Bayesian belief networks is that they allow a con- \\nvenient way to represent causal knowledge such as the fact that Lightning causes \\nThunder. In the terminology of conditional independence, we express this by stat- \\ning that Thunder is conditionally independent of other variables in the network, \\ngiven the value of Lightning. Note this conditional independence assumption is \\nimplied by the arcs in the Bayesian network of Figure 6.3. \\n6.11.3 Inference \\nWe might wish to use a Bayesian network to infer the value of some target \\nvariable \\n(e.g., ForestFire) given the observed values of the other variables. Of \\ncourse, given that we are dealing with random variables it will not generally be \\ncorrect to assign the target variable a single determined value. What we really \\nwish to infer is the probability distribution for the target variable, which specifies \\nthe probability that it will take on each of its possible values given the observed \\nvalues of the other variables. This inference step can be straightforward if values \\nfor all of the other variables in the network are known exactly. In the more \\ngeneral case we may wish to infer the probability distribution for some variable \\n(e.g., ForestFire) given observed values for only a subset of the other variables \\n(e.g., Thunder and BusTourGroup may be the only observed values available). In \\ngeneral, a Bayesian network can be used to compute the probability distribution \\nfor any subset of network variables given the values or distributions for any subset \\nof the remaining variables. \\nExact inference of probabilities in general for an arbitrary Bayesian net- \\nwork is known to be NP-hard (Cooper 1990). Numerous methods have been \\nproposed for probabilistic inference in Bayesian networks, including exact infer- \\nence methods and approximate inference methods that sacrifice precision to gain \\nefficiency. For example, Monte \\nCarlo \\nmethods provide approximate solutions by \\nrandomly sampling the distributions of the unobserved variables \\n(Pradham \\nand \\nDagum 1996). In theory, even approximate inference of probabilities in Bayesian '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 199}, page_content='networks can be NP-hard (Dagum and Luby 1993). Fortunately, in practice ap- \\nproximate methods have been shown to be useful in many cases. Discussions of \\ninference methods for Bayesian networks are provided by Russell and Norvig \\n(1995) and by Jensen (1996). \\n6.11.4 Learning Bayesian Belief Networks \\nCan we devise effective algorithms for learning Bayesian belief networks from \\ntraining data? This question is a focus of much current research. Several different \\nsettings for this learning problem can be considered. First, the network structure \\nmight be given in advance, or it might have to be inferred from the training data. \\nSecond, all the network variables might be directly observable in each training \\nexample, or some might be unobservable. \\nIn the case where the network structure is given in advance and the variables \\nare fully observable in the training examples, learning the conditional probability \\ntables is straightforward. We simply estimate the conditional probability table \\nentries just as we would for a naive Bayes classifier. \\nIn the case where the network structure is given but only some of the variable \\nvalues are observable in the training data, the learning problem is more difficult. \\nThis problem is somewhat analogous to learning the weights for the hidden units in \\nan artificial neural network, where the input and output node values are given but \\nthe hidden unit values are left unspecified by the training examples. In fact, Russell \\net al. (1995) propose a similar gradient ascent procedure that learns the entries in \\nthe conditional probability tables. This gradient ascent procedure searches through \\na space of hypotheses that corresponds to the set of all possible entries for the \\nconditional probability tables. The objective function that is maximized during \\ngradient ascent is the probability \\nP(D1h) \\nof the observed training data D given \\nthe hypothesis h. By definition, this corresponds to searching for the maximum \\nlikelihood hypothesis for the table entries. \\n6.11.5 Gradient Ascent Training of Bayesian Networks \\nThe gradient ascent rule given by Russell et al. (1995) maximizes P(D1h) by \\nfollowing the gradient of \\nIn \\nP(D \\nIh) \\nwith respect to the parameters that define the \\nconditional probability tables of the Bayesian network. Let wi;k denote a single \\nentry in one of the conditional probability tables. In particular, let wijk denote \\nthe conditional probability that the network variable Yi will take on the value yi, \\ngiven that its immediate parents Ui take on the values given by uik. For example, \\nif wijk is the top right entry in the conditional probability table in Figure 6.3, then \\nYi is the variable Campjire, Ui is the tuple of its parents (Stomz, BusTourGroup), \\nyij = True, and uik = (False, False). The gradient of In P(D1h) is given by \\nthe derivatives for each of the toijk. As we show below, each of these \\nderivatives can be calculated as '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 200}, page_content=\"CHAPTER 6 BAYESIAN LEARNING 189 \\nFor example, to calculate the derivative of In P(D1h) with respect to the upper- \\nrightmost entry in the table of Figure 6.3 we will have to calculate the quan- \\ntity P(Campf ire = True, Storm = False, BusTourGroup = Falseld) for each \\ntraining example d in D. When these variables are unobservable for the training \\nexample d, this required probability can be calculated from the observed variables \\nin d using standard Bayesian network inference. In fact, these required quantities \\nare easily derived from the calculations performed during most Bayesian network \\ninference, so learning can be performed at little additional cost whenever the \\nBayesian network is used for inference and new evidence is subsequently obtained. \\nBelow we derive Equation (6.25) following Russell et al. (1995). The re- \\nmainder of this section may be skipped on a first reading without loss of continuity. \\nTo simplify notation, in this derivation we will write the abbreviation Ph(D) to \\nrepresent P(DJh). Thus, our problem is to derive the gradient defined by the set \\nof derivatives for all i, j, and k. Assuming the training examples d in the \\ndata set D are drawn independently, we write this derivative as \\nThis last step makes use of the general equality 9 = 1- f(~) ax . W can now \\nintroduce the values of the variables Yi and Ui = Parents(Yi), by summing over \\ntheir possible values yijl and uiu. \\nThis last step follows from the product rule of probability, Table 6.1. Now consider \\nthe rightmost sum in the final expression above. Given that Wijk = Ph(yijl~ik), the \\nonly term in this sum for which & is nonzero is the term for which j' = j and \\ni' = i. Therefore \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 201}, page_content='Applying Bayes theorem to rewrite Ph (dlyij, uik), we have \\nThus, we have derived the gradient given in Equation (6.25). There is one more \\nitem that must be considered before we can state the gradient ascent training \\nprocedure. In particular, we require that as the weights wijk are updated they \\nmust remain valid probabilities in the interval \\n[0,1]. \\nWe also require that the \\nsum xj wijk remains 1 for all i, k. These constraints can be satisfied by updating \\nweights in a two-step process. First we update each wijk by gradient ascent \\nwhere q is a small constant called the learning rate. Second, we renormalize \\nthe weights wijk to assure that the above constraints are satisfied. As discussed \\nby Russell et al., this process will converge to a locally maximum likelihood \\nhypothesis for the conditional probabilities in the Bayesian network. \\nAs in other gradient-based approaches, this algorithm is guaranteed only to \\nfind some local optimum solution. An alternative to gradient ascent is the EM \\nalgorithm discussed in Section 6.12, which also finds locally maximum likelihood \\nsolutions. \\n6.11.6 Learning the Structure of Bayesian Networks \\nLearning Bayesian networks when the network structure is not known in advance \\nis also difficult. Cooper and Herskovits (1992) present a Bayesian scoring metric \\nfor choosing among alternative networks. They also present a heuristic search \\nalgorithm called K2 for learning network structure when the data is fully observ- \\nable. Like most algorithms for learning the structure of Bayesian networks, K2 \\nperforms a greedy search that trades off network complexity for accuracy over the \\ntraining data. In one experiment K2 was given a set of 3,000 training examples \\ngenerated at random from a manually constructed Bayesian network containing \\n37 nodes and 46 arcs. This particular network described potential anesthesia prob- \\nlems in a hospital operating room. In addition to the data, the program was also \\ngiven an initial ordering over the 37 variables that was consistent with the partial '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 202}, page_content='CHAPTER 6 BAYESIAN LEARNING 191 \\nordering of variable dependencies in the actual network. The program succeeded \\nin reconstructing the correct Bayesian network structure almost exactly, with the \\nexception of one incorrectly deleted arc and one incorrectly added arc. \\nConstraint-based approaches to learning Bayesian network structure have \\nalso been developed \\n(e.g., \\nSpirtes \\net al. 1993). These approaches infer indepen- \\ndence and dependence relationships from the data, and then use these relation- \\nships to construct Bayesian networks. Surveys of current approaches to learning \\nBayesian networks are provided by Heckerman (1995) and \\nBuntine \\n(1994). \\n6.12 THE EM ALGORITHM \\nIn many practical learning settings, only a subset of the relevant instance features \\nmight be observable. For example, in training or using the Bayesian belief network \\nof Figure 6.3, we might have data where only a subset of the network variables \\nStorm, Lightning, Thunder, ForestFire, Campfire, and BusTourGroup have been \\nobserved. Many approaches have been proposed to handle the problem of learning \\nin the presence of unobserved variables. As we saw in Chapter 3, if some variable \\n/ is sometimes observed and sometimes not, then we can use the cases for which \\nit has been observed to learn to predict its values when it is not. In this section \\nwe describe the EM algorithm (Dempster et al. \\n1977), \\na widely used approach \\nto learning in the presence of unobserved variables. The EM algorithm can be \\nused even for variables whose value is never directly observed, provided the \\ngeneral form of the probability distribution governing these variables is known. \\nThe EM algorithm has been used to train Bayesian belief networks (see Heckerman \\n1995) as well as radial basis function networks discussed in Section 8.4. The EM \\nalgorithm is also the basis for many unsupervised clustering algorithms \\n(e.g., \\nCheeseman et al. \\n1988), \\nand it is the basis for the widely used Baum-Welch \\nforward-backward algorithm for learning Partially Observable Markov Models \\n(Rabiner 1989). \\n6.12.1 Estimating Means of k Gaussians \\nThe easiest way to introduce the EM algorithm is via an example. Consider a \\nproblem in which the data D is a set of instances generated by a probability \\ndistribution that is a mixture of k distinct Normal distributions. This problem \\nsetting is illustrated in Figure 6.4 for the case where k = 2 and where the instances \\nare the points shown along the x axis. Each instance is generated using a two-step \\nprocess. First, one of the k Normal distributions is selected at random. Second, \\na single random instance xi is generated according to this selected distribution. \\nThis process is repeated to generate a set of data points as shown in the figure. To \\nsimplify our discussion, we consider the special case where the selection of the \\nsingle Normal distribution at each step is based on choosing each with uniform \\nprobability, where each of the k Normal distributions has the same variance a2, and \\nwhere a2 is known. The learning task is to output a hypothesis h = (FI, . . . pk) \\nthat describes the means of each of the k distributions. We would like to find '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 203}, page_content='FIGURE 6.4 \\nInstances generated by a mixture of two Normal distributions with identical variance a. The instances \\nare shown by the points along the x axis. If the means of the Normal distributions are unknown, the \\nEM algorithm can be used to search for their maximum likelihood estimates. \\na maximum likelihood hypothesis for these means; that is, a hypothesis h that \\nmaximizes p(D lh). \\nNote it is easy to calculate the maximum likelihood hypothesis for the mean \\nof a single Normal distribution given the observed data instances XI, x2, . . . , xm \\ndrawn from this single distribution. This problem of finding the mean of a single \\ndistribution is just a special case of the problem discussed in Section 6.4, Equa- \\ntion \\n(6.6), \\nwhere we showed that the maximum likelihood hypothesis is the one \\nthat minimizes the sum of squared errors over the m training instances. Restating \\nEquation (6.6) using our current notation, we have \\nIn this case, the sum of squared errors is minimized by the sample mean \\nOur problem here, however, involves a mixture of k different Normal dis- \\ntributions, and we cannot observe which instances were generated by which dis- \\ntribution. Thus, we have a prototypical example of a problem involving hidden \\nvariables. In the example of Figure 6.4, we can think of the full description of \\neach instance as the triple (xi, zil , ziz), where xi is the observed value of the ith \\ninstance and where zil and zi2 indicate which of the two Normal distributions was \\nused to generate the value xi. In particular, zij has the value 1 if xi was created by \\nthe jth Normal distribution and 0 otherwise. Here xi is the observed variable in \\nthe description of the instance, and zil and zi2 are hidden variables. If the values \\nof zil and zi2 were observed, we could use Equation (6.27) to solve for the means \\np1 and p2. Because they are not, we will instead use the EM algorithm. \\nApplied to our k-means problem the EM algorithm searches for a maximum \\nlikelihood hypothesis by repeatedly re-estimating the expected values of the hid- \\nden variables zij given its current hypothesis (pI . . . pk), then recalculating the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 204}, page_content=\"CHAPTER 6 BAYESIAN LEARNING 193 \\nmaximum likelihood hypothesis using these expected values for the hidden vari- \\nables. We will first describe this instance of the EM algorithm, and later state the \\nEM algorithm in its general form. \\n' \\nApplied to the problem of estimating the two means for Figure 6.4, the \\nEM algorithm first initializes the hypothesis to h = (PI, p2), where p1 and p2 are \\narbitrary initial values. It then iteratively re-estimates h by repeating the following \\ntwo steps until the procedure converges to a stationary value for h. \\nStep 1: Calculate the expected value E[zij] of each hidden variable zi,, assuming \\nthe current hypothesis h = (p1, p2) holds. \\nStep 2: Calculate a new maximum likelihood hypothesis h' = (pi, p;), assuming \\nthe value taken on by each hidden variable zij is its expected value E[zij] \\ncalculated in Step 1. Then replace the hypothesis h = (pl, p2) by the \\nnew hypothesis h' = (pi, pi) and iterate. \\nLet us examine how both of these steps can be implemented in practice. \\n/ Step 1 must calculate the expected value of each zi,. This E[4] is just the prob- \\nability that instance xi was generated by the jth Normal distribution \\nThus the first step is implemented by substituting the current values (pl, p2) and \\nthe observed xi into the above expression. \\nIn the second step we use the E[zij] calculated during Step 1 to derive a \\nnew maximum likelihood hypothesis h' = (pi, pi). AS we will discuss later, the \\nmaximum likelihood hypothesis in this case is given by \\nNote this expression is similar to the sample mean from Equation (6.28) that is \\nused to estimate p for a single Normal distribution. Our new expression is just \\nthe weighted sample mean for pj, with each instance weighted by the expectation \\nE[z,j] that it was generated by the jth Normal distribution. \\nThe above algorithm for estimating the means of a mixture of k Normal \\ndistributions illustrates the essence of the EM approach: The current hypothesis \\nis used to estimate the unobserved variables, and the expected values of these \\nvariables are then used to calculate an improved hypothesis. It can be proved that \\non each iteration through this loop, the EM algorithm increases the likelihood \\nP(Dlh) unless it is at a local maximum. The algorithm thus converges to a local \\nmaximum likelihood hypothesis for (pl, w2). \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 205}, page_content=\"6.12.2 General Statement of EM Algorithm \\nAbove we described an EM algorithm for the problem of estimating means of a \\nmixture of Normal distributions. More generally, the EM algorithm can be applied \\nin many settings where we wish to estimate some set of parameters 8 that describe \\nan underlying probability distribution, given only the observed portion of the full \\ndata produced by this distribution. In the above two-means example the parameters \\nof interest were 8 = (PI, p2), and the full data were the triples (xi, zil, zi2) of \\nwhich only the xi were observed. In general let X = {xl, . . . , x,} denote the \\nobserved data in a set of m independently drawn instances, let Z = {zl, . . . , z,} \\ndenote the unobserved data in these same instances, and let Y = X U Z denote \\nthe full data. Note the unobserved Z can be treated as a random variable whose \\nprobability distribution depends on the unknown parameters 8 and on the observed \\ndata X. Similarly, Y is a random variable because it is defined in terms of the \\nrandom variable Z. In the remainder of this section we describe the general form \\nof the EM algorithm. We use h to denote the current hypothesized values of the \\nparameters 8, and h' to denote the revised hypothesis that is estimated on each \\niteration of the EM algorithm. \\nThe EM algorithm searches for the maximum likelihood hypothesis h' by \\nseeking the h' that maximizes E[ln P(Y (h')]. This expected value is taken over \\nthe probability distribution governing Y, which is determined by the unknown \\nparameters 8. Let us consider exactly what this expression signifies. First, P(Ylhl) \\nis the likelihood of the full data Y given hypothesis h'. It is reasonable that we wish \\nto find a h' that maximizes some function of this quantity. Second, maximizing \\nthe logarithm of this quantity \\nIn P(Ylhl) also maximizes P(Ylhl), as we have \\ndiscussed on several occasions already. Third, we introduce the expected value \\nE[ln P(Ylhl)] because the full data Y is itself a random variable. Given that \\nthe full data Y is a combination of the observed data X and unobserved data \\nZ, we must average over the possible values of the unobserved Z, weighting \\neach according to its probability. In other words we take the expected value \\nE[ln P(Y lh')] over the probability distribution governing the random variable Y. \\nThe distribution governing Y is determined by the completely known values for \\nX, plus the distribution governing Z. \\nWhat is the probability distribution governing Y? In general we will not \\nknow this distribution because it is determined by the parameters 0 that we are \\ntrying to estimate. Therefore, the EM algorithm uses its current hypothesis h in \\nplace of the actual parameters 8 to estimate the distribution governing Y. Let us \\ndefine a function Q(hllh) that gives E[ln P(Y lh')] as a function of h', under the \\nassumption that 8 = h and given the observed portion X of the full data Y. \\nWe write this function Q in the form Q(hllh) to indicate that it is defined in part \\nby the assumption that the current hypothesis h is equal to 8. In its general form, \\nthe EM algorithm repeats the following two steps until convergence: \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 206}, page_content=\"CHAPTER 6 BAYESIAN LEARNING 195 \\nStep 1: Estimation (E) step: Calculate Q(hllh) using the current hypothesis h and \\nthe observed data X to estimate the probability distribution over Y. \\nQ(hf(h) t E[ln P(Ylhl)lh, XI \\nStep 2: Maximization (M) step: Replace hypothesis h by the hypothesis h' that \\nmaximizes this Q function. \\nh t argmax Q (hf 1 h) \\nh' \\nWhen the function Q is continuous, the EM algorithm converges to a sta- \\ntionary point of the likelihood function P(Y(hl). When this likelihood function \\nhas a single maximum, EM will converge to this global maximum likelihood es- \\ntimate for h'. Otherwise, it is guaranteed only to converge to a local maximum. \\nIn this respect, EM shares some of the same limitations as other optimization \\nmethods such as gradient descent, line search, \\nand \\nconjugate gradient discussed \\nin Chapter 4. \\n11 6.12.3 Derivation of the k Means Algorithm \\nTo illustrate the general EM algorithm, let us use it to derive the algorithm given in \\nSection 6.12.1 for estimating the means of a mixture of k Normal distributions. As \\ndiscussed above, the k-means problem is to estimate the parameters 0 = (PI. . . pk) \\nthat define the means of the k Normal distributions. We are given the observed \\ndata X = {(xi)}. The hidden variables Z = {(zil, . . . , zik)} in this case indicate \\nwhich of the k Normal distributions was used to generate xi. \\nTo apply EM we must derive an expression for Q(h(hf) that applies to \\nour k-means problem. First, let us derive an expression for 1np(Y(h1). Note the \\nprobability p(yi (h') of a single instance yi = (xi, Zil, . . . ~ik) of the full data can \\nbe written \\nTo verify this note that only one of the zij can have the value 1, and all others must \\nbe 0. Therefore, this expression gives the probability distribution for xi generated \\nby the selected Normal distribution. Given this probability for a single instance \\np(yi(hl), the logarithm of the probability In P(Y(hl) for all m instances in the \\ndata is \\nm \\nlnP(Ylhf) = lnnp(,lhl) \\ni=l \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 207}, page_content='Finally we must take the expected value of this In P(Ylhl) over the probability \\ndistribution governing Y or, equivalently, over the distribution governing the un- \\nobserved components \\nzij \\nof Y. Note the above expression for In P(Ylhl) is a linear \\nfunction of these \\nzij. \\nIn general, for any function f (z) that is a linear function of \\nz, the following equality holds \\nE[f \\n(z)l = f (Ek.1) \\nThis general fact about linear functions allows us to write \\nTo summarize, the function \\nQ(hllh) \\nfor the k means problem is \\nwhere \\nh\\' = (pi, . . . ,pi) and where E[zij] is calculated based on the current \\nhypothesis h and observed data X. As discussed earlier \\ne-&(x\\'-~)2 \\nE[zij] = - --+ --P\")~ \\n(6.29) \\nEL1 e 2 \\nThus, the first (estimation) step of the EM algorithm defines the Q function \\nbased on the estimated \\nE[zij] \\nterms. The second (maximization) step then finds \\nthe values pi, . . . , pi that maximize this Q function. In the current case \\n1 1 argmax Q(hllh) = argmax - - - \\nh\\' \\nC E[zijI(xi - \\nh1 i=l &2 2u2 j=l \\nThus, the maximum likelihood hypothesis here minimizes a weighted sum of \\nsquared errors, where the contribution of each instance \\nxi \\nto the error that defines \\npj is weighted by E[zij]. The quantity given by Equation (6.30) is minimized by \\nsetting each pi to the weighted sample mean \\nNote that Equations (6.29) and (6.31) define the two steps in the k-means \\nalgorithm described in Section 6.12.1. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 208}, page_content='CHAPTER 6 BAYESIAN LEARNING 197 \\n6.13 SUMMARY AND FURTHER READING \\nThe main points of this chapter include: \\n0 Bayesian methods provide the basis for probabilistic learning methods that \\naccommodate (and require) knowledge about the prior probabilities of alter- \\nnative hypotheses and about the probability of observing various data given \\nthe hypothesis. Bayesian methods allow assigning a posterior probability to \\neach candidate hypothesis, based on these assumed priors and the observed \\ndata. \\n0 Bayesian methods can be used to determine the most probable hypothesis \\ngiven the data-the maximum a posteriori (MAP) hypothesis. This is the \\noptimal hypothesis in the sense that no other hypothesis is more likely. \\n0 The Bayes optimal classifier combines the predictions of all alternative hy- \\npotheses, weighted by their posterior probabilities, to calculate the most \\nprobable classification of each new instance. \\ni \\n0 The naive Bayes classifier is a Bayesian learning method that has been found \\nto be useful in many practical applications. It is called \"naive\" because it in- \\ncorporates the simplifying assumption that attribute values are conditionally \\nindependent, given the classification of the instance. When this assumption \\nis met, the naive Bayes classifier outputs the MAP classification. Even when \\nthis assumption is not met, as in the case of learning to classify text, the \\nnaive Bayes classifier is often quite effective. Bayesian belief networks pro- \\nvide a more expressive representation for sets of conditional independence \\nassumptions among subsets of the attributes. \\n0 The framework of Bayesian reasoning can provide a useful basis for ana- \\nlyzing certain learning methods that do not directly apply Bayes theorem. \\nFor example, under certain conditions it can be shown that minimizing the \\nsquared error when learning a real-valued target function corresponds to \\ncomputing the maximum likelihood hypothesis. \\n0 The Minimum Description Length principle recommends choosing the hy- \\npothesis that minimizes the description length of the hypothesis plus the \\ndescription length of the data given the hypothesis. Bayes theorem and ba- \\nsic results from information theory can be used to provide a rationale for \\nthis principle. \\n0 In many practical learning tasks, some of the relevant instance variables \\nmay be unobservable. The EM algorithm provides a quite general approach \\nto learning in the presence of unobservable variables. This algorithm be- \\ngins with an arbitrary initial hypothesis. It then repeatedly calculates the \\nexpected values of the hidden variables (assuming the current hypothesis \\nis correct), and then recalculates the maximum likelihood hypothesis (as- \\nsuming the hidden variables have the expected values calculated by the first \\nstep). This procedure converges to a local maximum likelihood hypothesis, \\nalong with estimated values for the hidden variables. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 209}, page_content='There are many good introductory texts on probability and statistics, such \\nas Casella and Berger (1990). Several quick-reference books (e.g., Maisel 1971; \\nSpeigel 1991) also provide excellent treatments of the basic notions of probability \\nand statistics relevant to machine learning. \\nMany of the basic notions of Bayesian classifiers and least-squared error \\nclassifiers are discussed by Duda and Hart (1973). Domingos and Pazzani (1996) \\nprovide an analysis of conditions under which naive Bayes will output optimal \\nclassifications, even when its independence assumption is violated (the key here \\nis that there are conditions under which it will output optimal classifications even \\nwhen the associated posterior probability estimates are incorrect). \\nCestnik (1990) provides a discussion of using the m-estimate to estimate \\nprobabilities. \\nExperimental results comparing various Bayesian approaches to decision tree \\nlearning and other algorithms can be found in \\nMichie \\net al. (1994). Chauvin and \\nRumelhart (1995) provide a Bayesian analysis of neural network learning based \\non the BACKPROPAGATION algorithm. \\nA discussion of the Minimum Description Length principle can be found in \\nRissanen (1983, 1989). Quinlan and Rivest (1989) describe its use in avoiding \\noverfitting \\nin decision trees. \\nEXERCISES \\n6.1. Consider again the example application of Bayes rule in Section 6.2.1. Suppose the \\ndoctor decides to order a second laboratory test for the same patient, and suppose \\nthe second test returns a positive result as well. What are the posterior probabilities \\nof cancer and -cancer following these two tests? Assume that the two tests are \\nindependent. \\n6.2. In the example of Section 6.2.1 we computed the posterior probability of cancer by \\nnormalizing the quantities P (+(cancer) . P (cancer) and P (+I-cancer) . P (-cancer) \\nso that they summed to one, Use Bayes theorem and the theorem of total probability \\n(see Table 6.1) to prove that this method is valid \\n(i.e., \\nthat normalizing in this way \\nyields the correct value for P(cancerl+)). \\n6.3. Consider the concept learning algorithm FindG, which outputs a maximally general \\nconsistent hypothesis \\n(e.g., \\nsome maximally general member of the version space). \\n(a) Give a distribution for P(h) and P(D1h) under which FindG is guaranteed to \\noutput a MAP hypothesis. \\n(6) Give a distribution for P(h) and P(D1h) under which FindG is not guaranteed \\nto output a MAP .hypothesis. \\n(c) Give a distribution for P(h) and P(D1h) under which FindG is guaranteed to \\noutput a ML hypothesis but not a MAP hypothesis. \\n6.4. In the analysis of concept learning in Section 6.3 we assumed that the sequence of \\ninstances (xl . . . x,) was held fixed. Therefore, in deriving an expression for P(D(h) \\nwe needed only consider the probability of observing the sequence of target values \\n(dl.. .dm) for this fixed instance sequence. Consider the more general setting in \\nwhich the instances are not held fixed, but are drawn independently from some \\nprobability distribution defined over the instance space X. The data D must now \\nbe described as the set of ordered pairs {(xi, di)}, and P(D1h) must now reflect the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 210}, page_content='CHAPTER 6 BAYESIAN LEARNING 199 \\nprobability of encountering the specific instance XI, as well as the probability of \\nthe observed target value di. Show that Equation (6.5) holds even under this more \\ngeneral setting. Hint: Consider the analysis of Section 6.5. \\n6.5. Consider the Minimum Description Length principle applied to the hypothesis space \\nH consisting of conjunctions of up to n boolean attributes (e.g., Sunny A Warm). \\nAssume each hypothesis is encoded simply by listing the attributes present in the \\nhypothesis, where the number of bits needed to encode any one of the n boolean at- \\ntributes is log, n. Suppose the encoding of an example given the hypothesis uses zero \\nbits if the example is consistent with the hypothesis and uses log, m bits otherwise \\n(to indicate which of the m examples was misclassified-the correct classification \\ncan be inferred to be the opposite of that predicted by the hypothesis). \\n(a) Write down the expression for the quantity to be minimized according to the \\nMinimum Description Length principle. \\n(b) Is it possible to construct a set of training data such that a consistent hypothesis \\nexists, but MDL chooses a less consistent hypothesis? If so, give such a training \\nset. If not, explain why not. \\n(c) Give probability distributions for P(h) and P(D1h) such that the above MDL \\nalgorithm outputs MAP hypotheses. \\n6.6. Draw the Bayesian belief network that represents the conditional independence as- \\nsumptions of the naive Bayes classifier for the PlayTennis problem of Section 6.9.1. \\nGive the conditional probability table associated with the node Wind. \\nREFERENCES \\nBuntine W. L. (1994). Operations for learning with graphical models. Journal of Art$cial Intelligence \\nResearch, 2, 159-225. http://www.cs.washington.edu/research/jair/hom.html. \\nCasella, G., & Berger, R. L. (1990). Statistical inference. Pacific Grove, CA: Wadsworth & \\nBrooks/Cole. \\nCestnik, B. (1990). Estimating probabilities: A crucial task in machine learning. Proceedings of the \\nNinth European Conference on \\nAm&5al \\nIntelligence (pp. 147-149). London: Pitman. \\nChauvin, Y., & Rumelhart, D. (1995). Backpropagation: Theory, architectures, and applications, \\n(edited collection). Hillsdale, NJ: Lawrence Erlbaum Assoc. \\nCheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. (1988). AUTOCLASS: A \\nbayesian classification system. Proceedings of AAAI I988 (pp. 607-611). \\nCooper, G. (1990). Computational complexity of probabilistic inference using Bayesian belief net- \\nworks (research note). Art@cial Intelligence, 42, 393-405. \\nCooper, G., & Herskovits, E. (1992). A Bayesian method for the induction of probabilistic networks \\nfrom data. Machine Learning, 9, 309-347. \\nDagum, \\nP., & Luby, M. (1993). Approximating probabilistic reasoning in Bayesian belief networks \\nis NP-hard. Art$cial Intelligence, 60(1), 141-153. \\nDempster, \\nA. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data \\nvia the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1), 1-38. \\nDomingos, \\nP., & Pazzani, M. (1996). Beyond independence: Conditions for the optimality of the sim- \\nple Bayesian classifier. Proceedings of the 13th International Conference on Machine Learning \\n@p. 105-112). \\nDuda, \\nR. O., & Hart, P. E. (1973). Pattern class$cation and scene analysis. New York: John Wiley \\n& Sons. \\nHearst, \\nM., & Hirsh, H. (Eds.) (1996). Papers from the AAAI Spring Symposium on Machine \\nLearning in Information Access, Stanford, March 25-27. http://www.parc.xerox.com/ist~ \\nprojects/mlia/ '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 211}, page_content=\"200 MACHINE LEARNING \\nHeckerman, D., Geiger, D., & Chickering, D. (1995) Learning Bayesian networks: The combination \\nof knowledge and statistical data. Machine Learning, 20, 197. Kluwer Academic Publishers. \\nJensen, F. V. (1996). An introduction to Bayesian networks. New York: Springer Verlag. \\nJoachims, T. (1996). A probabilistic analysis of the Rocchio algorithm with TFIDF for text catego- \\nrization, (Computer Science Technical Report CMU-CS-96-118). Carnegie Mellon University. \\nLang, \\nK. (1995). Newsweeder: Learning to filter \\nnetnews. \\nIn Prieditis and Russell (Eds.), Proceedings \\nof the 12th International Conference on Machine Learning (pp. 331-339). San Francisco: \\nMorgan Kaufmann Publishers. \\nLewis, D. (1991). Representation and learning in information retrieval, (Ph.D. thesis), (COINS Tech- \\nnical Report 91-93). Dept. of Computer and Information Science, University of Massachusetts. \\nMadigan, D., & Rafferty, A. (1994). ~odel selection and accounting for model uncertainty in graphi- \\ncal models using Occam's window. Journal of the American Statistical Association, 89, 1535- \\n1546. \\nMaisel, L. (1971). Probability, statistics, and random processes. Simon and Schuster Tech Outlines. \\nNew York: Simon and Schuster. \\nMehta, M., Rissanen, J., & Agrawal, R. (1995). MDL-based decision tree pruning. In U. M. Fayyard \\nand R. Uthurusamy (Eds.), Proceedings of the First International Conference on Knowledge \\nDiscovery and Data Mining. Menlo Park, CA: AAAI Press. \\nMichie, D., Spiegelhalter, D. J., & Taylor, C. C. (1994). Machine learning, neural and statistical \\nclassification, (edited collection). New York: Ellis Horwood. \\nOpper, M., & Haussler, D. (1991). Generalization performance of Bayes optimal prediction algorithm \\nfor learning a perceptron. Physical Review Letters, 66, 2677-2681. \\nPearl, \\nJ. \\n(1988). Probabilistic reasoning in intelligent systems: Networks of plausible inference. San \\nMateo, CA: Morgan-Kaufmann. \\nPradham, M., & Dagum, P. (1996). Optimal Monte Carlo estimation of belief network inference. In \\nProceedings of the Conference on Uncertainty in Artijicial Intelligence (pp. 44-53). \\nQuinlan, \\nJ. \\nR., & Rivest, R. (1989). Inferring decision trees using the minimum description length \\nprinciple. Information and Computation, 80, 227-248. \\nRabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech \\nrecognition. Proceedings of the IEEE, 77(2), 257-286. \\nRissanen, J. (1983). A universal prior for integers and estimation by minimum description length. \\nThe Annals of Statistics, 11(2), 41-31. \\nRissanen, \\nJ., \\n(1989). Stochastic complexity in statistical inquiry. New Jersey: World Scientific Pub. \\nRissanen, \\nJ. \\n(1991). Information theory and neural nets. IBM Research Report RJ 8438 (76446), \\nIBM \\nThomas J. Watson Research Center, Yorktown Heights, NY. \\nRocchio, J. (1971). Relevance feedback in information retrieval. In The SMART retrieval system: \\nExperiments in automatic document processing, (Chap. 14, pp. 313-323). Englewood Cliffs, \\nNJ: Prentice-Hall. \\nRussell, S., & Nomig, P. (1995). Artificial intelligence: A modem approach. Englewood Cliffs, NJ: \\nPrentice-Hall. \\nRussell, S., Binder, \\nJ., \\nKoller, D., & Kanazawa, K. (1995). Local learning in probabilistic networks \\nwith hidden variables. Proceedings of the 14th International Joint Conference on Artificial \\nIntelligence, Montreal. San Francisco: Morgan Kaufmann. \\nSalton, \\nG. (1991). Developments in automatic text retrieval. Science, 253, 974-979. \\nShannon, C. E., & Weaver, W. (1949). The mathematical theory of communication. Urbana: Univer- \\nsity of Illinois Press. \\nSpeigel, \\nM. R. (1991). Theory and problems of probability and statistics. Schaum's Outline Series. \\nNew York: \\nMcGraw \\nHill. \\nSpirtes, \\nP., Glymour, C., & Scheines, R. (1993). Causation, prediction, and search. New York: \\nSpringer Verlag. http://hss.cmu.edu/htmUdepartments/philosophy~~D.BOO~ook.h~ \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 212}, page_content='CHAPTER \\nCOMPUTATIONAL \\nLEARNING \\nTHEORY \\nThis chapter presents a theoretical characterization of the difficulty of several types \\nof machine learning problems and the capabilities of several types of machine learn- \\ning algorithms. This theory seeks to answer questions such as \"Under what condi- \\ntions is successful learning possible and impossible?\" and \"Under what conditions \\nis a particular learning algorithm assured of learning successfully?\\' Two specific \\nframeworks for analyzing learning algorithms are considered. Within the probably \\napproximately correct (PAC) framework, we identify classes of hypotheses that can \\nand cannot be learned from a polynomial number of training examples and we de- \\nfine a natural measure of complexity for hypothesis spaces that allows bounding \\nthe number of training examples required for inductive learning. Within the mistake \\nbound framework, we examine the number of training errors that will be made by \\na learner before it determines the correct hypothesis. \\n7.1 INTRODUCTION \\nWhen studying machine learning it is natural to wonder what general laws may \\ngovern machine (and nonmachine) learners. Is it possible to identify classes of \\nlearning problems that are inherently difficult or easy, independent of the learning \\nalgorithm? Can one characterize the number of training examples necessary or \\nsufficient to assure successful learning? How is this number affected if the learner \\nis allowed to pose queries to the trainer, versus observing a random sample of \\ntraining examples? Can one characterize the number of mistakes that a learner '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 213}, page_content='202 MACHINE LEARNING \\nwill make before learning the target function? Can one characterize the inherent \\ncomputational complexity of classes of learning problems? \\nAlthough general answers to all these questions are not yet known, frag- \\nments of a computational theory of learning have begun to emerge. This chapter \\npresents key results from this theory, providing answers to these questions within \\nparticular problem settings. We focus here on the problem of inductively learning \\nan unknown target function, given only training examples of this target func- \\ntion and a space of candidate hypotheses. Within this setting, we will be chiefly \\nconcerned with questions such as how many training examples are sufficient to \\nsuccessfully learn the target function, and how many mistakes will the learner \\nmake before succeeding. As we shall see, it is possible to set quantitative bounds \\non these measures, depending on attributes of the learning problem such as: \\n0 the size or complexity of the hypothesis space considered by the learner \\n0 the accuracy to which the target concept must be approximated \\n0 the probability that the learner will output a successful hypothesis \\n0 the manner in which training examples are presented to the learner \\nFor the most part, we will focus not on individual learning algorithms, but \\nrather on broad classes of learning algorithms characterized by the hypothesis \\nspaces they consider, the presentation of training examples, etc. Our goal is to \\nanswer questions such as: \\n0 Sample complexity. How many training examples are needed for a learner \\nto converge (with high probability) to a successful hypothesis? \\n0 Computational complexity. How much computational effort is needed for a \\nlearner to converge (with high probability) to a successful hypothesis? \\n0 Mistake bound. How many training examples will the learner misclassify \\nbefore converging to a successful hypothesis? \\nNote there are many specific settings in which we could pursue such ques- \\ntions. For example, there are various ways to specify what it means for the learner \\nto be \"successful.\" We might specify that to succeed, the learner must output a \\nhypothesis identical to the target concept. Alternatively, we might simply require \\nthat it output a hypothesis that agrees with the target concept most of the time, or \\nthat it usually output such a hypothesis. Similarly, we must specify how training \\nexamples are to be obtained by the learner. We might specify that training ex- \\namples are presented by a helpful teacher, or obtained by the learner performing \\nexperiments, or simply generated at random according to some process outside \\nthe learner\\'s control. As we might expect, the answers to the above questions \\ndepend on the particular setting, or learning model, we have in mind. \\nThe remainder of this chapter is organized as follows. Section 7.2 introduces \\nthe probably approximately correct (PAC) learning setting. Section 7.3 then an- \\nalyzes the sample complexity and computational complexity for several learning '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 214}, page_content='CHAPTER 7 COMPUTATIONAL LEARNING THEORY 203 \\nproblems within this PAC setting. Section 7.4 introduces an important measure \\nof hypothesis space complexity called the VC-dimension and extends our PAC \\nanalysis to problems in which the hypothesis space is infinite. Section 7.5 intro- \\nduces the mistake-bound model and provides a bound on the number of mistakes \\nmade by several learning algorithms discussed in earlier chapters. Finally, we in- \\ntroduce the WEIGHTED-MAJORITY algorithm, a practical algorithm for combining \\nthe predictions of multiple competing learning algorithms, along with a theoretical \\nmistake bound for this algorithm. \\n7.2 PROBABLY LEARNING AN APPROXIMATELY CORRECT \\nHYPOTHESIS \\nIn this section we consider a particular setting for the learning problem, called the \\nprobably \\napproximately \\ncorrect (PAC) learning model. We begin by specifying \\nthe problem setting that defines the PAC learning model, then consider the ques- \\ntions of how many training examples and how much computation are required \\nin order to learn various classes of target functions within this PAC model. For \\ni the sake of simplicity, we restrict the discussion to the case of learning boolean- \\nvalued concepts from noise-free training data. However, many of the results can \\nbe extended to the more general scenario of learning real-valued target functions \\n(see, for example, Natarajan \\n1991), \\nand some can be extended to learning from \\ncertain types of noisy data (see, for example, Laird 1988; \\nKearns \\nand \\nVazirani \\n1994). \\n7.2.1 The Problem Setting \\nAs in earlier chapters, let X refer to the set of all possible instances over which \\ntarget functions may be defined. For example, X might represent the set of all \\npeople, each described by the attributes age \\n(e.g., \\nyoung or old) and height (short \\nor tall). Let C refer to some set of target concepts that our learner might be called \\nupon to learn. Each target concept c in C corresponds to some subset of X, or \\nequivalently to some boolean-valued function c : X + {0, 1). For example, one \\ntarget concept c in C might be the concept \"people who are skiers.\" If x is a \\npositive example of c, then we will write c(x) = 1; if x is a negative example, \\nc(x) = 0. \\nWe assume instances are generated at random from X according to some \\nprobability distribution D. For example, 2) might be the distribution of instances \\ngenerated by observing people who walk out of the largest sports store in Switzer- \\nland. In general, D may be any distribution, and it will not generally be known \\nto the learner. All that we require of D is that it be stationary; that is, that the \\ndistribution not change over time. Training examples are generated by drawing \\nan \\ninstance x at random according to D, then presenting x along with its target \\nvalue, c(x), to the learner. \\nThe learner L considers some set H of possible hypotheses when attempting \\nto learn the target concept. For example, H might be the set of all hypotheses '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 215}, page_content=\"describable by conjunctions of the attributes age and height. After observing \\na sequence of training examples of the target concept c, L must output some \\nhypothesis h from H, which is its estimate of c. To be fair, we evaluate the \\nsuccess of L by the performance of h over new instances drawn randomly from \\nX according to D, the same probability distribution used to generate the training \\ndata. \\nWithin this setting, we are interested in characterizing the performance of \\nvarious learners L using various hypothesis spaces H, when learning individual \\ntarget concepts drawn from various classes C. Because we demand that L be \\ngeneral enough to learn any target concept from C regardless of the distribution \\nof training examples, we will often be interested in worst-case analyses over all \\npossible target concepts from C and all possible instance distributions D. \\n7.2.2 Error of a Hypothesis \\nBecause we are interested in how closely the learner's output hypothesis h ap- \\nproximates the actual target concept c, let us begin by defining the true error \\nof a hypothesis h with respect to target concept c and instance distribution D. \\nInformally, the true error of h is just the error rate we expect when applying h \\nto future instances drawn according to the probability distribution 27. In fact, we \\nalready defined the true error of h in Chapter 5. For convenience, we restate the \\ndefinition here using c to represent the boolean target function. \\nDefinition: The true error (denoted errorv(h)) of hypothesis h with respect to target \\nconcept c and distribution D is the probability that h will misclassify an instance \\ndrawn at random according to D. \\nHere the notation Pr indicates that the probability is taken over the instance \\nx€D \\ndistribution V. \\nFigure 7.1 shows this definition of error in graphical form. The concepts c \\nand h are depicted by the sets of instances within X that they label as positive. The \\nerror of h with respect to c is the probability that a randomly drawn instance will \\nfall into the region where h and c disagree (i.e., their set difference). Note we have \\nchosen to define error over the entire distribution of instances-not simply over \\nthe training examples-because this is the true error we expect to encounter when \\nactually using the learned hypothesis h on subsequent instances drawn from D. \\nNote that error depends strongly on the unknown probability distribution \\n2). For example, if D is a uniform probability distribution that assigns the same \\nprobability to every instance in X, then the error for the hypothesis in Figure 7.1 \\nwill be the fraction of the total instance space that falls into the region where h \\nand c disagree. However, the same h and c will have a much higher error if D \\nhappens to assign very high probability to instances for which h and c disagree. \\nIn the extreme, if V happens to assign zero probability to the instances for which \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 216}, page_content='Instance space X \\nC \\nWhere c \\nand h disagree \\nFIGURE 7.1 \\nThe error of hypothesis h with respect to target concept c. The error of h with respect to c is the \\nprobability that a randomly drawn instance will fall into the region where h and c disagree on its \\nclassification. The + and - points indicate positive and negative training examples. Note h has a \\nnonzero error with respect to c despite the fact that h and c agree on all five training examples \\nobserved \\nthus \\nfar. \\nh(x) = ~(x), then the error for the h in Figure 7.1 will be 1, despite the fact the \\nh and c agree on a very large number of (zero probability) instances. \\nFinally, note that the error of h with respect to c is not directly observable to \\nthe learner. L can only observe the performance of h over the training examples, \\nand it must choose its output hypothesis on this basis only. We will use the term \\ntraining error to refer to the fraction of training examples misclassified by h, in \\ncontrast to the true error defined above. Much of our analysis of the complexity of \\nlearning centers around the question \"how probable is it that the observed training \\nerror for h gives a misleading estimate of the true errorv(h)?\" \\nNotice the close relationship between this question and the questions con- \\nsidered in Chapter 5. Recall that in Chapter 5 we defined the sample error of h \\nwith respect to a set S of examples to be the fraction of S rnisclassified by h. The \\ntraining error defined above is just the sample error when S is the set of training \\nexamples. In Chapter 5 we determined the probability that the sample error will \\nprovide a misleading estimate of the true error, under the assumption that the data \\nsample S is drawn independent of h. However, when S is the set of training data, \\nthe learned hypothesis h depends very much on S! Therefore, in this chapter we \\nprovide an analysis that addresses this important special case. \\n7.2.3 PAC Learnability \\nOur aim is to characterize classes of target concepts that can be reliably learned \\nfrom a reasonable number of randomly drawn training examples and a reasonable \\namount of computation. \\nWhat \\nkinds \\nof \\nstatements \\nabout learnability should we guess hold true? \\nWe might try to characterize the number of training examples needed to learn '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 217}, page_content='a hypothesis h for which errorD(h) = 0. Unfortunately, it turns out this is fu- \\ntile in the setting we are considering, for two reasons. First, unless we provide \\ntraining examples corresponding to every possible instance in X (an unrealistic \\nassumption), there may be multiple hypotheses consistent with the provided train- \\ning examples, and the learner cannot be certain to pick the one corresponding \\nto the target concept. Second, given that the training examples are drawn ran- \\ndomly, there will always be some nonzero probability that the training examples \\nencountered by the learner will be misleading. (For example, although we might \\nfrequently see skiers of different heights, on any given day there is some small \\nchance that all observed training examples will happen to be 2 meters tall.) \\nTo accommodate these two difficulties, we weaken our demands on the \\nlearner in two ways. First, we will not require that the learner output a zero error \\nhypothesis-we will require only that its error be bounded by some constant, \\nc, \\nthat can be made arbitrarily small. Second, we will not require that the learner \\nsucceed for every sequence of randomly drawn training examples-we will require \\nonly that its probability of failure be bounded by some constant, 6, that can be \\nmade arbitrarily small. In short, we require only that the learner probably learn a \\nhypothesis that is approximately correct-hence the term probably approximately \\ncorrect learning, or PAC learning for short. \\nConsider some class C of possible target concepts and a learner L using \\nhypothesis space H. Loosely speaking, we will say that the concept class C \\nis PAC-learnable by L using H if, for any target concept c in C, L will with \\nprobability (1 - 6) output a hypothesis h with errorv(h) < c, after observing a \\nreasonable number of training examples and performing a reasonable amount of \\ncomputation. More precisely, \\nDefinition: Consider a concept class C defined over a set of instances X of length \\nn and a learner L using hypothesis space H. C is PAC-learnable by L using H \\nif for all c E C, distributions D over X, E such that 0 < 6 < 112, and 6 such that \\n0 < 6 < 112, learner L will with probability at least (1 - 6) output a hypothesis \\nh E H such that errorv(h) 5 E, in time that is polynomial in 116, 116, n, and \\nsize(c). \\nOur definition requires two things from L. First, L must, with arbitrarily high \\nprobability (1 - 6), output a hypothesis having arbitrarily low error (6). Second, it \\nmust do so efficiently-in time that grows at most polynomially with \\n1/c \\nand \\n116, \\nwhich define the strength of our demands on the output hypothesis, and with n and \\nsize(c) \\nthat define the inherent complexity of the underlying instance space X and \\nconcept class C. Here, n is the size of instances in X. For example, if instances in \\nX are conjunctions of k boolean features, then n = k. The second space parameter, \\nsize(c), \\nis the encoding length of c in C, assuming some representation for C. \\nFor example, if concepts in C are conjunctions of up to k boolean features, each \\ndescribed by listing the indices of the features in the conjunction, then \\nsize(c) \\nis \\nthe number of boolean features actually used to describe c. \\nOur definition of PAC learning may at first appear to be concerned only \\nwith the computational resources required for learning, whereas in practice we are '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 218}, page_content=\"usually more concerned with the number of training examples required. However, \\nthe two are very closely related: If L requires some minimum processing time \\nper training example, then for C to be PAC-learnable by L, L must learn from a \\npolynomial number of training examples. In fact, a typical approach to showing \\nthat some class C of target concepts is PAC-learnable, is to first show that each \\ntarget concept in C can be learned from a polynomial number of training examples \\nand then show that the processing time per example is also polynomially bounded. \\nBefore moving on, we should point out a restrictive assumption implicit \\nin our definition of PAC-learnable. This definition implicitly assumes that the \\nlearner's hypothesis space H contains a hypothesis with arbitrarily small error for \\nevery target concept in C. This follows from the requirement in the above defini- \\ntion that the learner succeed when the error bound 6 is arbitrarily close to zero. Of \\ncourse this is difficult to assure if one does not know C in advance (what is C for \\na program that must learn to recognize faces from images?), unless H is taken to \\nbe the power set of X. As pointed out in Chapter 2, such an unbiased H will not \\nsupport accurate generalization from a reasonable number of training examples. \\nNevertheless, the results based on the PAC learning model provide useful insights \\n/ regarding the relative complexity of different learning problems and regarding the \\nrate at which generalization accuracy improves with additional training examples. \\nFurthermore, in Section 7.3.1 we will lift this restrictive assumption, to consider \\nthe case in which the learner makes no prior assumption about the form of the \\ntarget concept. \\n7.3 SAMPLE COMPLEXITY FOR FINITE HYPOTHESIS SPACES \\nAs noted above, PAC-learnability is largely determined by the number of training \\nexamples required by the learner. The growth in the number of required training \\nexamples with problem size, called the sample complexity of the learning problem, \\nis the characteristic that is usually of greatest interest. The reason is that in most \\npractical settings the factor that most limits success of the learner is the limited \\navailability of training data. \\nHere we present a general bound on the sample complexity for a very broad \\nclass of learners, called consistent learners. A learner is consistent if it outputs \\nhypotheses that perfectly fit the training data, whenever possible. It is quite rea- \\nsonable to ask that a learning algorithm be consistent, given that we typically \\nprefer a hypothesis that fits the training data over one that does not. Note that \\nmany of the learning algorithms discussed in earlier chapters, including all the \\nlearning algorithms described in Chapter 2, are consistent learners. \\nCan we derive a bound on the number of training examples required by \\nany consistent learner, independent of the specific algorithm it uses to derive a \\nconsistent hypothesis? The answer is yes. To accomplish this, it is useful to recall \\nthe definition of version space from Chapter 2. There we defined the version space, \\nVSH,D, to be the set of all hypotheses h E H that correctly classify the training \\nexamples D. \\nvs,~ = {h E HI(V(x, 4~)) E D) (h(x) = ~(x))} \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 219}, page_content=\"The significance of the version space here is that every consistent learner outputs \\na hypothesis belonging to the version space, regardless of the instance space X, \\nhypothesis space H, or training data D. The reason is simply that by definition \\nthe version space VSH,D contains every consistent hypothesis in H. Therefore, \\nto bound the number of examples needed by any consistent learner, we need only \\nbound the number of examples needed to assure that the version space contains no \\nunacceptable hypotheses. The following definition, after Haussler (1988), states \\nthis condition precisely. \\nDefinition: Consider a hypothesis space H, target concept c, instance distribution \\nV, and set of training examples D of c. The version space VS,, is said to be \\n€-exhausted with respect to c and V, if every hypothesis h in VSH,* has error less \\nthan 6 with respect to c and V. \\nThis definition is illustrated in Figure 7.2. The version space is €-exhausted \\njust in the case that all the hypotheses consistent with the observed training ex- \\namples \\n(i.e., \\nthose with zero training error) happen to have true error less than \\nE. Of course from the learner's viewpoint all that can be known is that these \\nhypotheses fit the training data equally well-they all have zero training error. \\nOnly an observer who knew the identity of the target concept could determine \\nwith certainty whether the version space is \\n+exhausted. \\nSurprisingly, a proba- \\nbilistic argument allows us to bound the probability that the version space will \\nbe \\n€-exhausted \\nafter a given number of training examples, even without knowing \\nthe identity of the target concept or the distribution from which training examples \\nHypothesis space H \\nm \\nerror =.3 \\nr =.4 \\nFIGURE 7.2 \\nExhausting the version space. The version space VSH,D is the subset of hypotheses h E H, which \\nhave zero training error (denoted by r = 0 in the figure). Of course the true errorv(h) (denoted by \\nerror in the figure) may be nonzero, even for hypotheses that commit zero errors over the training \\ndata. The version space is said to be \\n€-exhausted \\nwhen all hypotheses h remaining in VSH,~ have \\nerrorw(h) < E. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 220}, page_content='are drawn. Haussler (1988) provides such a bound, in the form of the following \\ntheorem. \\nTheorem 7.1. €-exhausting the version space. If the hypothesis space H is finite, \\nand D is a sequence of rn 1 independent randomly drawn examples of some target \\nconcept c, then for any 0 5 E 5 1, the probability that the version space VSH,~ is \\nnot \\n€-exhausted \\n(with respect to c) is less than or equal to \\nProof. Let hl, h2, . . . hk be all the hypotheses in H that have true error greater than E \\nwith respect to c. We fail to €-exhaust the version space if and only if at least one of \\nthese k hypotheses happens to be consistent with all rn independent random training \\nexamples. The probability that any single hypothesis having true error greater than E \\nwould be consistent with one randomly drawn example is at most (1 - E). Therefore \\nthe probability that this hypothesis will be consistent with rn independently drawn \\nexamples is at most (1 - E)~. Given that we have k hypotheses with error greater \\nthan E, the probability that at least one of these will be consistent with all rn training \\nexamples is at most \\nAnd since k 5 I H 1, this is at most 1 H I(1- 6)\". Finally, we use a general inequality \\nstating that if 0 5 E 5 1 then (1 - E) 5 e-\\'. Thus, \\nwhich proves the theorem. .O \\nWe have just proved an upper bound on the probability that the version space \\nis not \\n€-exhausted, \\nbased on the number of training examples m, the allowed error \\nE, \\nand the size of H. Put another way, this bounds the probability that m training \\nexamples will fail to eliminate all \"bad\" hypotheses \\n(i.e., \\nhypotheses with true \\nerror greater than E), for any consistent learner using hypothesis space H. \\nLet us use this result to determine the number of training examples required \\nto reduce this probability of failure below some desired level 6. \\nRearranging terms to solve for m, we find \\n1 m 2 - (ln 1 HI + ln(l/6)) \\nE \\n(7.2) \\nTo summarize, the inequality shown in Equation (7.2) provides a general \\nbound on the number of training examples sufficient for any consistent learner \\nto successfully learn any target concept in H, for any desired values of 6 and \\nE. This number rn of training examples is sufficient to assure that any consistent \\nhypothesis will be probably (with probability (1 - 6)) approximately (within error \\nE) correct. Notice m grows linearly in 1/~ and logarithmically in 116. It also grows \\nlogarithmically in the size of the hypothesis space H. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 221}, page_content='210 MACHINE LEARNING \\nNote that the above bound can be a substantial overestimate. For example, \\nalthough the probability of failing to exhaust the version space must lie in the \\ninterval [O, 11, the bound given by the theorem grows linearly with IHI. For \\nsufficiently large hypothesis spaces, this bound can easily be greater than one. \\nAs a result, the bound given by the inequality in Equation (7.2) can substantially \\noverestimate the number of training examples required. The weakness of this \\nbound is mainly due to the IHI term, which arises in the proof when summing \\nthe probability that a single hypothesis could be unacceptable, over all possible \\nhypotheses. In fact, a much tighter bound is possible in many cases, as well as a \\nbound that covers infinitely large hypothesis spaces. This will be the subject of \\nSection 7.4. \\n7.3.1 Agnostic Learning and Inconsistent Hypotheses \\nEquation (7.2) is important because it tells us how many training examples suffice \\nto ensure (with probability (1 - 6)) that every hypothesis in H having zero training \\nerror will have a true error of at most E. Unfortunately, if H does not contain \\nthe target concept c, then a zero-error hypothesis cannot always be found. In this \\ncase, the most we might ask of our learner is to output the hypothesis from H \\nthat has the minimum error over the training examples. A learner that makes no \\nassumption that the target concept is representable by H and that simply finds \\nthe hypothesis with minimum training error, is often called an agnostic learner, \\nbecause it makes no prior commitment about whether or not C g H. \\nAlthough Equation (7.2) is based on the assumption that the learner outputs \\na zero-error hypothesis, a similar bound can be found for this more general case \\nin which the learner entertains hypotheses with nonzero training error. To state \\nthis precisely, let D denote the particular set of training examples available to \\nthe learner, in contrast to D, which denotes the probability distribution over the \\nentire set of instances. Let \\nerrorD(h) \\ndenote the training error of hypothesis h. \\nIn particular, \\nerror~(h) \\nis defined as the fraction of the training examples in \\nD that are misclassified by h. Note the errorD(h) over the particular sample of \\ntraining data D may differ from the true error errorv(h) over the entire probability \\ndistribution 2). Now let hb,,, denote the hypothesis from H having lowest training \\nerror over the training examples. How many training examples suffice to ensure \\n(with high probability) that its true error \\nerrorD(hb,,,) \\nwill be no more than \\nE + errorg (hbest)? Notice the question considered in the previous section is just a \\nspecial case of this question, when \\nerrorD(hb,,) \\nhappens to be zero. \\nThis question can be answered (see Exercise 7.3) using an argument analo- \\ngous to the proof of Theorem 7.1. It is useful here to invoke the general Hoeffding \\nbounds (sometimes called the additive Chernoff bounds). The Hoeffding bounds \\ncharacterize the deviation between the true probability of some event and its ob- \\nserved frequency over m independent trials. More precisely, these bounds apply \\nto experiments involving m distinct Bernoulli trials (e.g., m independent flips of a \\ncoin with some probability of turning up heads). This is exactly analogous to the \\nsetting we consider when estimating the error of a hypothesis in Chapter 5: The '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 222}, page_content='CHAPTER 7 COMPUTATIONAL LEARNING THEORY 211 \\nprobability of the coin being heads corresponds to the probability that the hypothe- \\nsis will misclassify a randomly drawn instance. The m independent coin flips corre- \\nspond to the m independently drawn instances. The frequency of heads over the m \\nexamples corresponds to the frequency of misclassifications over the m instances. \\nThe Hoeffding bounds state that if the training error \\nerrOrD(h) \\nis measured \\nover the set D containing m randomly drawn examples, then \\nThis gives us a bound on the probability that an arbitrarily chosen single hypothesis \\nhas a very misleading training error. To assure that the best hypothesis found by \\nL has an error bounded in this way, we must consider the probability that any \\none of the 1 H 1 hypotheses could have a large error \\nPr[(3h E H)(errorv(h) > error~(h) + E)] 5 1 H ~e-~~\\'~ \\nIf we call this probability 6, and ask how many examples m suffice to hold S to \\nsome desired value, we now obtain \\nThis is the generalization of Equation (7.2) to the case in which the learner still \\npicks the best hypothesis h E H, but where the best hypothesis may have nonzero \\ntraining error. Notice that m depends logarithmically on H and on 116, as it did \\nin the more restrictive case of Equation (7.2). However, in this less restrictive \\nsituation m now grows as the square of 116, rather than linearly with 116. \\n7.3.2 Conjunctions of Boolean Literals Are PAC-Learnable \\nNow that we have a bound indicating the number of training examples sufficient \\nto probably approximately learn the target concept, we can use it to determine the \\nsample complexity and PAC-learnability of some specific concept classes. \\nConsider the class C of target concepts described by conjunctions of boolean \\nliterals. A boolean literal is any boolean variable (e.g., Old), or its negation (e.g., \\n-Old). \\nThus, conjunctions of boolean literals include target concepts such as \\n\"Old A -Tallv. Is C PAC-learnable? We can show that the answer is yes by \\nfirst showing that any consistent learner will require only a polynomial number \\nof training examples to learn any c in C, and then suggesting a specific algorithm \\nthat uses polynomial time per training example. \\nConsider any consistent learner L using a hypothesis space H identical to C. \\nWe can use Equation (7.2) to compute the number m of random training examples \\nsufficient to ensure that L will, with probability (1 - S), output a hypothesis with \\nmaximum error E. To accomplish this, we need only determine the size IHI of \\nthe hypothesis space. \\nNow consider the hypothesis space H defined by conjunctions of literals \\nbased on n boolean variables. The size 1HI of this hypothesis space is 3\". To see \\nthis, consider the fact that there are only three possibilities for each variable in '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 223}, page_content='212 MACHINE LEARNING \\nany given hypothesis: Include the variable as a literal in the hypothesis, include \\nits negation as a literal, or ignore it. Given n such variables, there are 3\" distinct \\nhypotheses. \\nSubstituting IH I = 3\" into Equation (7.2) gives the following bound for the \\nsample complexity of learning conjunctions of up to n boolean literals. \\nFor example, if a consistent learner attempts to learn a target concept described \\nby conjunctions of up to 10 boolean literals, and we desire a 95% probability \\nthat it will learn a hypothesis with error less than .l, then it suffices to present m \\nrandomly drawn training examples, where rn = -$ (10 1n 3 + ln(11.05)) = 140. \\nNotice that m grows linearly in the number of literals n, linearly in 116, and \\nlogarithmically in \\n116. \\nWhat about the overall computational effort? That will \\ndepend, of course, on the specific learning algorithm. However, as long as our \\nlearning algorithm requires no more than polynomial computation per training \\nexample, and no more than a polynomial number of training examples, then the \\ntotal computation required will be polynomial as well. \\nIn the case of learning conjunctions of boolean literals, one algorithm that \\nmeets this requirement has already been presented in Chapter 2. It is the FIND-S \\nalgorithm, which incrementally computes the most specific hypothesis consistent \\nwith the training examples. For each new positive training example, this algorithm \\ncomputes the intersection of the literals shared by the current hypothesis and the \\nnew training example, using time linear in n. Therefore, the FIND-S algorithm \\nPAC-learns the concept class of conjunctions of n boolean literals with negations. \\nTheorem 7.2. PAC-learnability of boolean conjunctions. The class C of con- \\njunctions of boolean literals is PAC-learnable by the FIND-S algorithm using H = C. \\nProof. Equation (7.4) shows that the sample complexity for this concept class is \\npolynomial in n, 116, and 116, and independent of size (c). To incrementally process \\neach training example, the FIND-S algorithm requires effort linear in n and indepen- \\ndent of 116, 116, and size(c). Therefore, this concept class is PAC-learnable by the \\nFIND-S algorithm. 0 \\n7.3.3 PAC-Learnability of Other Concept Classes \\nAs we just saw, Equation (7.2) provides a general basis for bounding the sample \\ncomplexity for learning target concepts in some given class C. Above we applied \\nit to the class of conjunctions of boolean literals. It can also be used to show \\nthat many other concept classes have polynomial sample complexity \\n(e.g., \\nsee \\nExercise 7.2). \\n7.3.3.1 UNBIASED LEARNERS \\nNot all concept classes have polynomially bounded sample complexity according \\nto the bound of Equation (7.2). For example, consider the unbiased concept class '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 224}, page_content='C that contains every teachable concept relative to X. The set C of all definable \\ntarget concepts corresponds to the power set of X-the set of all subsets of X- \\nwhich contains ICI = 2IXI concepts. Suppose that instances in X are defined by \\nn boolean features. In this case, there will be 1x1 = 2\" distinct instances, and \\ntherefore \\nICI = 21\\'1 = 2\\' distinct concepts. Of course to learn such an unbiased \\nconcept class, the learner must itself use an unbiased hypothesis space H = C. \\nSubstituting I H I = 22n into Equation (7.2) gives the sample complexity for learning \\nthe unbiased concept class relative to X. \\nThus, this unbiased class of target concepts has exponential sample complexity \\nunder the PAC model, according to Equation (7.2). Although Equations (7.2) \\nand (7.5) are not tight upper bounds, it can in fact be proven that the sample \\ncomplexity for the unbiased concept class is exponential in n. \\nI1 \\nI 7.3.3.2 K-TERM DNF AND K-CNF CONCEPTS \\nI1 \\nIt is also possible to find concept classes that have polynomial sample complexity, \\nbut nevertheless cannot be learned in polynomial time. One interesting example is \\nthe concept class C of k-term disjunctive normal form (k-term DNF) expressions. \\nk-term DNF expressions are of the form \\nTI v T2 v . . - v Tk, where each term 1;: \\nis a conjunction of n boolean attributes and their negations. Assuming H = C, it \\nis easy to show that I HI is at most 3\"k (because there are k terms, each of which \\nmay take on 3\" possible values). Note 3\"k is an overestimate of H, because it is \\ndouble counting the cases where = I;. and where 1;: is more_general-than I;.. \\nStill, we can use this upper bound on I HI to obtain an upper bound on the sample \\ncomplexity, substituting this into Equation (7.2). \\nwhich indicates that the sample complexity of k-term DNF is polynomial in \\n1/~, 116, n, and k. Despite having polynomial sample complexity, the computa- \\ntional complexity is not polynomial, because this learning problem can be shown \\nto be equivalent to other problems that are known to be unsolvable in polynomial \\ntime (unless RP = NP). Thus, although k-term DNF has polynomial sample \\ncomplexity, it does not have polynomial computational complexity for a learner \\nusing H = C. \\nThe surprising fact about k-term DNF is that although it is not \\nPAC- \\nlearnable, there is a strictly larger concept class that is! This is possible because \\nthe \\nlarger concept class has polynomial computation complexity per example and \\nstill has polynomial sample complexity. This larger class is the class of k-CNF \\nexpressions: conjunctions of arbitrary length of the form TI A T2 A. . . A I;., where \\neach is a disjunction of up to k boolean attributes. It is straightforward to show \\nthat k-CNF subsumes k-DNF, because any k-term \\nDNF \\nexpression can easily be '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 225}, page_content='rewritten as a k-CNF expression (but not vice versa). Although k-CNF is more \\nexpressive than k-term DNF, it has both polynomial sample complexity and poly- \\nnomial time complexity. Hence, the concept class k-term DNF is PAC learnable \\nby an efficient algorithm using H = k-CNF. See Kearns and Vazirani (1994) for \\na more detailed discussion. \\n7.4 SAMPLE COMPLEXITY FOR INFINITE HYPOTHESIS SPACES \\nIn the above section we showed that sample complexity for PAC learning grows \\nas the logarithm of the size of the hypothesis space. While Equation (7.2) is quite \\nuseful, there are two drawbacks to characterizing sample complexity in terms of \\nIHI. \\nFirst, it can lead to quite weak bounds (recall that the bound on 6 can be \\nsignificantly greater than 1 for large I H I). Second, in the case of infinite hypothesis \\nspaces we cannot apply Equation (7.2) at all! \\nHere we consider a second measure of the complexity of H, called the \\nVapnik-Chervonenkis dimension of H (VC dimension, or \\nVC(H), \\nfor short). As \\nwe shall see, we can state bounds on sample complexity that use \\nVC(H) \\nrather \\nthan \\nIHI. \\nIn many cases, the sample complexity bounds based on \\nVC(H) \\nwill \\nbe tighter than those from Equation (7.2). In addition, these bounds allow us to \\ncharacterize the sample complexity of many infinite hypothesis spaces, and can \\nbe shown to be fairly tight. \\n7.4.1 Shattering a Set of Instances \\nThe VC dimension measures the complexity of the hypothesis space H, not by the \\nnumber of distinct hypotheses 1 H 1, but instead by the number of distinct instances \\nfrom X that can be completely discriminated using H. \\nTo make this notion more precise, we first define the notion of shattering a \\nset of instances. Consider some subset of instances \\nS E X. For example, Figure 7.3 \\nshows a subset of three instances from X. Each hypothesis h from H imposes some \\ndichotomy on S; that is, h partitions \\nS \\ninto the two subsets {x E Slh(x) = 1) and \\n{x E Slh(x) = 0). Given some instance set S, there are 2ISI possible dichotomies, \\nthough H may be unable to represent some of these. We say that H shatters \\nS \\nif \\nevery possible dichotomy of \\nS \\ncan be represented by some hypothesis from H. \\nDefinition: A set of instances S is shattered by hypothesis space H if and only if \\nfor every \\ndichotomy.of S there exists some hypothesis in H consistent with this \\ndichotomy. \\nFigure 7.3 illustrates a set S of three instances that is shattered by the \\nhypothesis space. Notice that each of the \\n23 \\ndichotomies of these three instances \\nis covered by some hypothesis. \\nNote that if a set of instances is not shattered by a hypothesis space, then \\nthere must be some concept (dichotomy) that can be defined over the instances, \\nbut that cannot be represented by the hypothesis space. The ability of H to shatter '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 226}, page_content='Instance space X \\nFIGURE 73 \\nA set of three instances shattered by eight hypotheses. For every possible dichotomy of the instances, \\nthere exists a corresponding hypothesis. \\na set .of instances is thus a measure of its capacity to represent target concepts \\ndefined over these instances. \\n7.4.2 The Vapnik-Chervonenkis Dimension \\nThe ability to shatter a set of instances is closely related to the inductive bias of \\na hypothesis space. Recall from Chapter 2 that an unbiased hypothesis space is \\none capable of representing every possible concept (dichotomy) definable over the \\ninstance space X. Put briefly, an unbiased hypothesis space H is one that shatters \\nthe instance space X. What if H cannot shatter X, but can shatter some large \\nsubset S of X? Intuitively, it seems reasonable to say that the larger the subset \\nof X that can be shattered, the more expressive H. The VC dimension of H is \\nprecisely this measure. \\nDefinition: The Vapnik-Chervonenkis dimension, VC(H), of hypothesis space H \\ndefined over instance space X is the size of the largest finite subset of X shattered \\nby H. If arbitrarily large finite sets of X can be shattered by H, then VC(H) = oo. \\nNote that for any finite H, VC(H) 5 log2 IHI. To see this, suppose that \\nVC(H) = d. Then H will require 2d distinct hypotheses to shatter d instances. \\nHence, 2d 5 IHI, andd = VC(H) slog2(H(. \\n7.4.2.1 ILLUSTRATIW EXAMPLES \\nIn order to develop an intuitive feeling for VC(H), consider a few example hy- \\npothesis spaces. To get started, suppose the instance space X is the set of real \\nnumbers X = 8 (e.g., describing the height of people), and H the set of inter- \\nvals on the real number line. In other words, H is the set of hypotheses of the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 227}, page_content='form a < x < b, where a and b may be any real constants. What is VC(H)? \\nTo answer this question, we must find the largest subset of X that can be shat- \\ntered by H. Consider a particular subset containing two distinct instances, say \\nS = {3.1,5.7}. Can S be shattered by H? Yes. For example, the four hypotheses \\n(1 < x < 2), (1 < x < 4), (4 < x < 7), and (1 < x < 7) will do. Together, they \\nrepresent each of the four dichotomies over S, covering neither instance, either \\none of the instances, and both of the instances, respectively. Since we have found \\na set of size two that can be shattered by H, we know the VC dimension of H \\nis at least two. Is there a set of size three that can be shattered? Consider a set \\nS = (xo, xl, x2} containing three arbitrary instances. Without loss of generality, \\nassume xo < xl < x2. Clearly this set cannot be shattered, because the dichotomy \\nthat includes xo and x2, but not XI, cannot be represented by a single closed inter- \\nval. Therefore, no subset S of size three can be shattered, and VC(H) = 2. Note \\nhere that H is infinite, but \\nVC(H) \\nfinite. \\nNext consider the set X of instances corresponding to points on the x, y plane \\n(see Figure 7.4). Let H be the set of all linear decision surfaces in the plane. In \\nother words, H is the hypothesis space corresponding to a single perceptron unit \\nwith two inputs (see Chapter 4 for a general discussion of perceptrons). What \\nis the VC dimension of this H? It is easy to see that any two distinct points in \\nthe plane can be shattered by H, because we can find four linear surfaces that \\ninclude neither, either, or both points. What about sets of three points? As long as \\nthe points are not colinear, we will be able to find \\n23 \\nlinear surfaces that shatter \\nthem. Of course three colinear points cannot be shattered (for the same reason that \\nthe three points on the real line could not be shattered in the previous example). \\nWhat is \\nVC(H) \\nin this case-two or three? It is at least three. The definition of \\nVC dimension indicates that if we find any set of instances of size d that can \\nbe shattered, then \\nVC(H) 2 d. To show that VC(H) < d, we must show that \\nno set of size d can be shattered. In this example, no sets of size four can be \\nshattered, so \\nVC(H) = 3. More generally, it can be shown that the VC dimension \\nof linear decision surfaces in an r dimensional space (i.e., the VC dimension of a \\nperceptron with r inputs) is r + 1. \\nAs one final example, suppose each instance in X is described by the con- \\njunction of exactly three boolean literals, and suppose that each hypothesis in H is \\ndescribed by the conjunction of up to three boolean literals. What is \\nVC(H)? \\nWe \\nFIGURE 7.4 \\nThe VC dimension for linear decision surfaces in the x, y plane is 3. (a) A set of three points that \\ncan be shattered using linear decision surfaces. (b) A set of three that cannot be shattered. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 228}, page_content='can show that it is at least 3, as follows. Represent each instance by a 3-bit string \\ncorresponding to the values of each of its three literals \\n11, \\n12, \\nand \\n13. \\nConsider the \\nfollowing set of three instances: \\nThis set of three instances can be shattered by H, because a hypothesis \\ncan be constructed for any desired dichotomy as follows: If the dichotomy is to \\nexclude instancei, add the literal \\n-li \\nto the hypothesis. For example, suppose we \\nwish to include \\ninstance2, \\nbut exclude instance1 and \\ninstance3. \\nThen we use the \\nhypothesis \\n-Il A -I3. This argument easily extends from three features to n. Thus, \\nthe VC dimension for conjunctions of n boolean literals is at least n. In fact, it is \\nexactly n, though showing this is more difficult, because it requires demonstrating \\nthat no set of n + 1 instances can be shattered. \\ni 7.4.3 Sample Complexity and the VC Dimension \\nEarlier we considered the question \"How many randomly drawn training examples \\nsuffice to probably approximately learn any target concept in \\nC?\\' \\n(i.e., \\nhow many \\nexamples suffice to \\n€-exhaust \\nthe version space with probability (1 - a)?). Using \\nVC(H) \\nas a measure for the complexity of H, it is possible to derive an alternative \\nanswer to this question, analogous to the earlier bound of Equation (7.2). This \\nnew bound (see Blumer et al. 1989) is \\nNote that just as in the bound from Equation \\n(7.2), \\nthe number of required training \\nexamples m grows logarithmically in 118. It now grows log times linear in 116, \\nrather than linearly. Significantly, the \\nIn I HI term in the earlier bound has now \\nbeen replaced by the alternative measure of hypothesis space complexity, \\nVC(H) \\n(recall \\nVC(H) I log2 I H I). \\nEquation (7.7) provides an upper bound on the number of training examples \\nsufficient to probably approximately learn any target concept in C, for any desired \\nt and a. It is also possible to obtain a lower bound, as summarized in the following \\ntheorem (see Ehrenfeucht et al. 1989). \\nTheorem 7.3. Lower bound on sample complexity. Consider any concept class \\nC such that VC(C) 2 2, any learner L, and any 0 < E < $, and 0 < S < &. Then \\nthere exists a distribution 23 and target concept in C such that if L observes fewer \\nexamples than \\nthen with probability at least 6, L outputs a hypothesis h having errorD(h) > E. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 229}, page_content=\"This theorem states that if the number of training examples is too few, then \\nno learner can PAC-learn every target concept in any nontrivial C. Thus, this \\ntheorem provides a lower bound on the number of training examples necessary for \\nsuccessful learning, complementing the earlier upper bound that gives a \\nsuficient \\nnumber. Notice this lower bound is determined by the complexity of the concept \\nclass C, whereas our earlier upper bounds were determined by H. (why?)+ \\nThis lower bound shows that the upper bound of the inequality in Equa- \\ntion (7.7) is fairly tight. Both bounds are logarithmic in \\n116 \\nand linear in VC(H). \\nThe only difference in the order of these two bounds is the extra log(l/c) depen- \\ndence in the upper bound. \\n7.4.4 VC Dimension for Neural Networks \\nGiven the discussion of artificial neural network learning in Chapter 4, it is in- \\nteresting to consider how we might calculate the VC dimension of a network of \\ninterconnected units such as the feedforward networks trained by the BACKPROPA- \\nGATION procedure. This section presents a general result that allows computing the \\nVC dimension of layered acyclic networks, based on the structure of the network \\nand the VC dimension of its individual units. This VC dimension can then be used \\nto bound the number of training examples sufficient to probably approximately \\ncorrectly learn a feedforward network to desired values of \\nc \\nand 6. This section \\nmay be skipped on a first reading without loss of continuity. \\nConsider a network, G, of units, which forms a layered directed acyclic \\ngraph. A directed acyclic graph is one for which the edges have a direction (e.g., \\nthe units have inputs and outputs), and in which there are no directed cycles. \\nA layered graph is one whose nodes can be partitioned into layers such that \\nall directed edges from nodes at layer 1 go to nodes at layer 1 + 1. The layered \\nfeedforward neural networks discussed throughout Chapter 4 are examples of such \\nlayered directed acyclic graphs. \\nIt turns out that we can bound the VC dimension of such networks based on \\ntheir graph structure and the VC dimension of the primitive units from which they \\nare constructed. To formalize this, we must first define a few more \\nterms. \\nLet n \\nbe the number of inputs to the network G, and let us assume that there is just one \\noutput node. Let each internal unit Ni of G (i.e., each node that is not an input) \\nhave at most r inputs and implement a boolean-valued function ci : 8'' + (0, 1) \\nfrom some function class C. For example, if the internal nodes are perceptrons, \\nthen C will be the class of linear threshold functions defined over 8'. \\nWe can now define the G-composition of C to be the class of all functions \\nthat can be implemented by the network G assuming individual units in G take \\non functions from the class C. In brief, the G-composition of C is the hypothesis \\nspace representable by the network G. \\nt~int: If we were to substitute H for C in the lower bound, this would result in a tighter bound on \\nm in the case H > C. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 230}, page_content=\"The following theorem bounds the VC dimension of the G-composition of \\nC, based on the VC dimension of C and the structure of G. \\nTheorem 7.4. VC-dimension of directed acyclic layered networks. (See Kearns \\nand \\nVazirani 1994.) Let G be a layered directed acyclic graph with n input nodes \\nand \\ns 2 2 internal nodes, each having at most r inputs. Let C be a concept class over \\n8Y of VC dimension d, corresponding to the set of functions that can be described \\nby each of the \\ns \\ninternal nodes. Let CG be the G-composition of C, corresponding \\nto the set of functions that can be represented by G. Then VC(CG) 5 2dslog(es), \\nwhere e is the base of the natural logarithm. \\nNote this bound on the VC dimension of the network G grows linearly with \\nthe VC dimension d of its individual units and log times linear in s, the number \\nof threshold units in the network. \\nSuppose we consider acyclic layered networks whose individual nodes are \\nperceptrons. Recall from Chapter 4 that an r input perceptron uses linear decision \\nsurfaces to represent boolean functions over %'. As noted in Section 7.4.2.1, the \\nVC dimension of linear decision surfaces over is r + 1. Therefore, a single \\nperceptron with r inputs has VC dimension r + 1. We can use this fact, together \\nwith the above theorem, to bound the VC dimension of acyclic layered networks \\ncontaining s perceptrons, each with r inputs, as \\nWe can now bound the number m of training examples sufficient to learn \\nperceptrons \\n(with probability at least (1 - 6)) any target concept from C, to within \\nerror E. Substituting the above expression for the network VC dimension into \\nEquation \\n(7.7), \\nwe have \\nAs illustrated by this perceptron network example, the above theorem is \\ninteresting because it provides a general method for bounding the VC dimension \\nof layered, acyclic networks of units, based on the network structure and the VC \\ndimension of the individual units. Unfortunately the above result does not directly \\napply to networks trained using BACKPROPAGATION, for \\ntwo \\nreasons. First, this \\nresult applies to networks of perceptrons rather than networks of sigmoid units \\nto which the BACKPROPAGATION algorithm applies. Nevertheless, notice that the \\nVC \\ndimension of sigmoid units will be at least as great as that of perceptrons, \\nbecause a sigmoid unit can approximate a perceptron to arbitrary accuracy by \\nusing sufficiently large weights. Therefore, the above bound on m will be at least \\nas large for acyclic layered networks of sigmoid units. The second shortcoming \\nof the above result is that it fails to account for the fact that \\nBACKPROPAGATION \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 231}, page_content='220 MACHINE LEARNING \\ntrains a network by beginning with near-zero weights, then iteratively modifying \\nthese weights until an acceptable hypothesis is found. Thus, \\nBACKPROPAGATION \\nwith a cross-validation stopping criterion exhibits an inductive bias in favor of \\nnetworks with small weights. This inductive bias, which reduces the effective VC \\ndimension, is not captured by the above analysis. \\n7.5 THE MISTAKE BOUND MODEL OF LEARNING \\nWhile we have focused thus far on the PAC learning model, computational learn- \\ning theory considers a variety of different settings and questions. Different learning \\nsettings that have been studied vary by how the training examples are generated \\n(e.g., \\npassive observation of random examples, active querying by the learner), \\nnoise in the data \\n(e.g., \\nnoisy or error-free), the definition of success \\n(e.g., \\nthe \\ntarget concept must be learned exactly, or only probably and approximately), as- \\nsumptions made by the learner \\n(e.g., \\nregarding the distribution of instances and \\nwhether C G H), and the measure according to which the learner is evaluated \\n(e.g., \\nnumber of training examples, number of mistakes, total time). \\nIn this section we consider the mistake bound model of learning, in which \\nthe learner is evaluated by the total number of mistakes it makes before it con- \\nverges to the correct hypothesis. As in the PAC setting, we assume the learner \\nreceives a sequence of training examples. However, here we demand that upon \\nreceiving each example x, the learner must predict the target value \\nc(x), \\nbefore \\nit is shown the correct target value by the trainer. The question considered is \\n\"How many mistakes will the learner make in its predictions before it learns the \\ntarget concept?\\' This question is significant in practical settings where learning \\nmust be done while the system is in actual use, rather than during some off-line \\ntraining stage. For example, if the system is to learn to predict which credit card \\npurchases should be approved and which are fraudulent, based on data collected \\nduring use, then we are interested in minimizing the total number of mistakes it \\nwill make before converging to the correct target function. Here the total num- \\nber of mistakes can be even more important than the total number of training \\nexamples. \\nThis mistake bound learning problem may be studied in various specific \\nsettings. For example, we might count the number of mistakes made before PAC \\nlearning the target concept. In the examples below, we consider instead the number \\nof mistakes made before learning the target concept exactly. Learning the target \\nconcept exactly means converging to a hypothesis such that \\n(Vx)h(x) = c(x). \\n7.5.1 Mistake Bound for the FIND-S Algorithm \\nTo illustrate, consider again the hypothesis space H consisting of conjunctions of \\nup to \\nn \\nboolean literals 11 . . .1, and their negations (e.g., Rich A -Handsome). \\nRecall the FIND-S algorithm from Chapter 2, which incrementally computes the \\nmaximally specific hypothesis consistent with the training examples. A straight- \\nforward implementation of FIND-S for the hypothesis space H is as follows: '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 232}, page_content='CHAPTER 7 COMPUTATIONAL LEARNING THEORY 221 \\nFIND-S: \\n0 Initialize h to the most specific hypothesis l1 A -II A 12 A -12.. .1, A -1, \\n0 For each positive training instance x \\n0 Remove from h any literal that is not satisfied by x \\n0 Output hypothesis h. \\nFIND-S converges in the limit to a hypothesis that makes no errors, provided \\nC H and provided the training data is noise-free. FIND-S begins with the most \\nspecific hypothesis (which classifies every instance a negative example), then \\nincrementally generalizes this hypothesis as needed to cover observed positive \\ntraining examples. For the hypothesis representation used here, this generalization \\nstep consists of deleting unsatisfied literals. \\nCan we prove a bound on the total number of mistakes that FIND-S will make \\nbefore exactly learning the target concept c? The answer is yes. To see this, note \\nfirst that if c E H, then FIND-S can never mistakenly classify a negative example as \\npositive. The reason is that its current hypothesis h is always at least as specific as \\n1 the target concept e. Therefore, to calculate the number of mistakes it will make, \\nwe need only count the number of mistakes it will make misclassifying truly \\npositive examples as negative. How many such mistakes can occur before FIND-S \\nlearns c exactly? Consider the first positive example encountered by FIND-S. The \\nlearner will certainly make a mistake classifying this example, because its initial \\nhypothesis labels every instance negative. However, the result will be that half \\nof the 2n terms in its initial hypothesis will be eliminated, leaving only n terms. \\nFor each subsequent positive example that is mistakenly classified by the current \\nhypothesis, at least one more of the remaining n terms must be eliminated from \\nthe hypothesis. Therefore, the total number of mistakes can be at most n + 1. This \\nnumber of mistakes will be required in the worst case, corresponding to learning \\nthe most general possible target concept \\n(Vx)c(x) = 1 and corresponding to a \\nworst case sequence of instances that removes only one literal per mistake. \\n7.5.2 Mistake Bound for the HALVING Algorithm \\nAs a second example, consider an algorithm that learns by maintaining a descrip- \\ntion of the version space, incrementally refining the version space as each new \\ntraining example is encountered. The CANDIDATE-ELIMINATION algorithm and the \\nLIST-THEN-ELIMINATE algorithm from Chapter 2 are examples of such algorithms. \\nIn this section we derive a worst-case bound on the number of mistakes that will \\nbe made by such a learner, for any finite hypothesis space H, assuming again that \\nthe target concept must be learned exactly. \\nTo analyze the number of mistakes made while learning we must first specify \\nprecisely how the learner will make predictions given a new instance x. Let us \\nassume this prediction is made by taking a majority vote among the hypotheses in \\nthe current version space. If the majority of version space hypotheses classify the \\nnew instance as positive, then this prediction is output by the learner. Otherwise \\na negative prediction is output. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 233}, page_content='222 MACHINE LEARNING \\nThis combination of learning the version space, together with using a ma- \\njority vote to make subsequent predictions, is often called the HALVING algorithm. \\nWhat is the maximum number of mistakes that can be made by the HALVING \\nalgorithm, for an arbitrary finite H, before it exactly learns the target concept? \\nNotice that learning the target concept \"exactly\" corresponds to reaching a state \\nwhere the version space contains only a single hypothesis (as usual, we assume \\nthe target concept \\nc \\nis in H). \\nTo derive the mistake bound, note that the only time the HALVING algorithm \\ncan make a mistake is when the majority of hypotheses in its current version space \\nincorrectly classify the new example. In this case, once the correct classification is \\nrevealed to the learner, the version space will be reduced to at most half its current \\nsize \\n(i.e., \\nonly those hypotheses that voted with the minority will be retained). \\nGiven that each mistake reduces the size of the version space by at least half, \\nand given that the initial version space contains only I H I members, the maximum \\nnumber of mistakes possible before the version space contains just one member \\nis \\nlog2 I H I. In fact one can show the bound is Llog, I H (1. Consider, for example, \\nthe case in which \\nIHI = 7. The first mistake must reduce IHI to at most 3, and \\nthe second mistake will then reduce it to 1. \\nNote that [log2 IH(1 is a worst-case bound, and that it is possible for the \\nHALVING algorithm to learn the target concept exactly without making any mis- \\ntakes at all! This can occur because even when the majority vote is correct, the \\nalgorithm will remove the incorrect, minority hypotheses. If this occurs over the \\nentire training sequence, then the version space may be reduced to a single member \\nwhile making no mistakes along the way. \\nOne interesting extension to the HALVING algorithm is to allow the hy- \\npotheses to vote with different weights. Chapter 6 describes the Bayes optimal \\nclassifier, which takes such a weighted vote among hypotheses. In the Bayes op- \\ntimal classifier, the weight assigned to each hypothesis is the estimated posterior \\nprobability that it describes the target concept, given the training data. Later in \\nthis section we describe a different algorithm based on weighted voting, called \\nthe WEIGHTED-MAJORITY algorithm. \\n7.5.3 Optimal Mistake Bounds \\nThe above analyses give worst-case mistake bounds for two specific algorithms: \\nFIND-S and \\nCANDIDATE-ELIMINATION. \\nIt \\nis \\ninteresting to ask what is the optimal \\nmistake bound for an arbitrary concept class C, assuming H = C. By optimal \\nmistake bound we mean the lowest worst-case mistake bound over all possible \\nlearning algorithms. To be more precise, for any learning algorithm A and any \\ntarget concept c, let \\nMA(c) \\ndenote the maximum over all possible sequences of \\ntraining examples of the number of mistakes made by A to exactly learn c. Now \\nfor any nonempty concept class C, let \\nMA(C) - max,,~ MA(c). Note that above \\nwe showed \\nMFindPS(C) = n + 1 when C is the concept class described by up \\nto n boolean literals. We also showed MHalving(C) 5 log2((CI) for any concept \\nclass C. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 234}, page_content='We define the optimal mistake bound for a concept class C below. \\nDefinition: Let C be an arbitrary nonempty concept class. The optimal mistake \\nbound for C, denoted Opt (C), is the minimum over all possible learning algorithms \\nA of MA(C). \\nOpt (C) = min \\nAdearning algorithms MA (a \\nSpeaking informally, this definition states that Opt(C) is the number of \\nmistakes made for the hardest target concept in C, using the hardest training \\nsequence, by the best algorithm. Littlestone (1987) shows that for any concept \\nclass C, there is an interesting relationship among the optimal mistake bound for \\nC, the bound of the HALVING algorithm, and the VC dimension of C, namely \\nFurthermore, there exist concept classes for which the four quantities above \\nare exactly equal. One such concept class is the powerset Cp of any finite set \\nof instances X. In this case, VC(Cp) = 1x1 = log2(1CpJ), so all four quantities \\nmust be equal. Littlestone (1987) provides examples of other concept classes for \\nwhich VC(C) is strictly less than Opt (C) and for which Opt (C) is strictly less \\nthan M~aIvin~(C) \\n7.5.4 WEIGHTED-MAJORITY Algorithm \\nIn this section we consider a generalization of the HALVING algorithm called \\nthe WEIGHTED-MAJORITY algorithm. The WEIGHTED-MAJORITY algorithm makes \\npredictions by taking a weighted vote among a pool of prediction algorithms and \\nlearns by altering the weight associated with each prediction algorithm. These \\nprediction algorithms can be taken to be the alternative hypotheses in H, or they \\ncan be taken to be alternative learning algorithms that themselves vary over time. \\nAll that we require of a prediction algorithm is that it predict the value of the target \\nconcept, given an instance. One interesting property of the WEIGHTED-MAJORITY \\nalgorithm is that it is able to accommodate inconsistent training data. This is \\nbecause it does not eliminate a hypothesis that is found to be inconsistent with \\nsome training example, but rather reduces its weight. A second interesting property \\nis \\nthat we can bound the number of mistakes made by WEIGHTED-MAJORITY in \\nterms of the number of mistakes committed by the best of the pool of prediction \\nalgorithms. \\nThe WEIGHTED-MAJORITY algorithm begins by assigning a weight of 1 to \\neach \\nprediction algorithm, then considers the training examples. Whenever a pre- \\ndiction algorithm misclassifies a new training example its weight is decreased by \\nmultiplying it by some number B, where 0 5 B < 1. The exact definition of the \\nWEIGHTED-MAJORITY algorithm is given in Table 7.1. \\nNotice if f? = 0 then WEIGHTED-MAJORITY is identical to the HALVING al- \\ngorithm. On the other hand, if we choose some other value for p, no prediction '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 235}, page_content='ai denotes the if* prediction algorithm in the pool A of algorithms. wi denotes the weight associated \\nwith ai. \\nFor all i initialize wi c 1 \\nFor each training example (x, c(x)) \\nc Initialize qo and ql to 0 \\nam For each prediction algorithm ai \\nc If ai(x) =O then qo t q0 +wi \\nIf ai(x) = 1 then ql c ql + wi \\nIf ql > qo then predict c(x) = 1 \\nIf qo > q1 then predict c(x) = 0 \\nIf ql = qo then predict 0 or 1 at random for c(x) \\nFor each prediction algorithm ai in A do \\nIf ai(x) # c(x) then wi +- Buri \\nTABLE 7.1 \\nWEIGHTED-MAJORITY algorithm. \\nalgorithm will ever be eliminated completely. If an algorithm misclassifies a train- \\ning example, it will simply receive a smaller vote in the future. \\nWe now show that the number of mistakes committed by the WEIGHTED- \\nMAJORITY algorithm can be bounded in terms of the number of mistakes made by \\nthe best prediction algorithm in the voting pool. \\nTheorem 7.5. Relative mistake bound for WEIGHTED-MAJORITY. Let D be any \\nsequence of training examples, let A be any set of n prediction algorithms, and let \\nk be the minimum number of mistakes made by any algorithm in A for the training \\nsequence D. Then the number of mistakes over D made by the WEIGHTED-MAJORITY \\nalgorithm using /3 = 4 is at most \\n2.4(k + log, n) \\nProof. We prove the theorem by comparing the final weight of the best prediction \\nalgorithm to the sum of weights over all algorithms. Let aj denote an algorithm from \\nA that commits the optimal number k of mistakes. The final weight wj associated \\nwith aj will be because its initial weight is 1 and it is multiplied by 3 for each \\nmistake. Now consider the sum W = x:=, wi of the weights associated with all n \\nalgorithms in A. W is initially n. For each mistake made by WEIGHTED-MAJORITY, \\nW is reduced to at most :w. This is the case because the algorithms voting in the \\nweighted majority must hold at least half of the total weight W, and this portion \\nof W will be reduced by a factor of 4. Let M denote the total number of mistakes \\ncommitted by WEIGHTED-MAJORITY for the training sequence D. Then the final total \\nweight W is at most n(:lM. Because the final weight wj cannot be greater than the \\nfinal total weight, we have '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 236}, page_content=\"Rearranging terms yields \\nM 5 (k + log' n, 2.4(k + log, n) \\n-1% (a) - \\nwhich proves the theorem. \\nTo summarize, the above theorem states that the number of mistakes made \\nby the WEIGHTED-MAJORITY algorithm will never be greater than a constant factor \\ntimes the number of mistakes made by the best member of the pool, plus a term \\nthat grows only logarithmically in the size of the pool. \\nThis theorem is generalized by Littlestone and Warmuth \\n(1991), \\nwho show \\nthat for an arbitrary 0 5 j3 < 1 the above bound is \\n/ 7.6 SUMMARY AND FURTHER READING \\nThe main points of this chapter include: \\n0 The probably approximately correct (PAC) model considers algorithms that \\nlearn target concepts from some concept class C, using training examples \\ndrawn at random according to an unknown, but fixed, probability distribu- \\ntion. It requires that the learner probably (with probability at least \\n[l - 61) \\nlearn a hypothesis that is approximately (within error \\nE) \\ncorrect, given com- \\nputational effort and training examples that grow only polynornially with \\nI/€, 1/6, the size of the instances, and the size of the target concept. \\n0 Within the setting of the PAC learning model, any consistent learner using \\na finite hypothesis space H where C H will, with probability (1 - S), \\noutput a hypothesis within error E of the target concept, after observing m \\nrandomly drawn training examples, as long as \\nThis gives a bound on the number of training examples sufficient for suc- \\ncessful learning under the PAC model. \\nOne constraining assumption of the PAC learning model is that the learner \\nknows in advance some restricted concept class C that contains the target \\nconcept to be learned. In contrast, the agnostic learning model considers the \\nmore general setting in which the learner makes no assumption about the \\nclass from which the target concept is drawn. Instead, the learner outputs \\nthe hypothesis from H that has the least error (possibly nonzero) over the \\ntraining data. Under this less restrictive agnostic learning model, the learner \\nis assured with probability (1 -6) to output a hypothesis within error E of the \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 237}, page_content=\"best possible hypothesis in H, after observing rn randomly drawn training \\nexamples, provided \\na The number of training examples required for successful learning is strongly \\ninfluenced by the complexity of the hypothesis space considered by the \\nlearner. One useful measure of the complexity of a hypothesis space H \\nis its \\nVapnik-Chervonenkis \\ndimension, \\nVC(H). \\nVC(H) \\nis the size of the \\nlargest subset of instances that can be shattered (split in all possible ways) \\nby H. \\na An alternative upper bound on the number of training examples sufficient \\nfor successful learning under the PAC model, stated in terms of \\nVC(H) \\nis \\nA lower bound is \\na An alternative learning model, called the mistake bound model, is used to \\nanalyze the number of training examples a learner will misclassify before \\nit exactly learns the target concept. For example, the HALVING algorithm \\nwill make at most \\nLlog, 1 H 1 J mistakes before exactly learning any target \\nconcept drawn from H. For an arbitrary concept class C, the best worst- \\ncase algorithm will make Opt (C) mistakes, where \\nVC(C> 5 Opt(C) I log,(lCI) \\na The WEIGHTED-MAJORITY algorithm combines the weighted votes of multiple \\nprediction algorithms to classify new instances. It learns weights for each of \\nthese prediction algorithms based on errors made over a sequence of exam- \\nples. Interestingly, the number of mistakes made by WEIGHTED-MAJORITY can \\nbe bounded in terms of the number of mistakes made by the best prediction \\nalgorithm in the pool. \\nMuch early work on computational learning theory dealt with the question \\nof whether the learner could identify the target concept in the limit, given an \\nindefinitely long sequence of training examples. The identification in the limit \\nmodel was introduced by Gold (1967). A good overview of results in this area is \\n(Angluin 1992). Vapnik (1982) examines in detail the problem of uniform con- \\nvergence, and the closely related PAC-learning model was introduced by Valiant \\n(1984). The discussion in this chapter of \\n€-exhausting \\nthe version space is based \\non Haussler's (1988) exposition. A useful collection of results under the PAC \\nmodel can be found in Blumer et al. (1989). Kearns and \\nVazirani \\n(1994) pro- \\nvide an excellent exposition of many results from computational learning theory. \\nEarlier texts in this area include Anthony and Biggs (1992) and Natarajan (1991). \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 238}, page_content='Current research on computational learning theory covers a broad range of \\nlearning models and learning algorithms. Much of this research can be found \\nin the proceedings of the annual conference on Computational Learning Theory \\n(COLT). Several special issues of the journal Machine Learning have also been \\ndevoted to this topic. \\nEXERCISES \\n7.1. Consider training a two-input perceptron. Give an upper bound on the number of \\ntraining examples sufficient to assure with 90% confidence that the learned percep- \\ntron will have true error of at most 5%. Does this bound seem realistic? \\n7.2. Consider the class C of concepts of the form (a 4 x 5 b)~(c 5 y 5 d), where a, b, c, \\nand d are integers in the interval (0,99). Note each concept in this class corresponds \\nto a rectangle with integer-valued boundaries on a portion of the x, y plane. Hint: \\nGiven a region in the plane bounded by the points (0,O) and (n - 1, n - I), the \\nnumber of distinct rectangles with integer-valued boundaries within this region is \\n(\"M)2. \\n2 \\ni (a) Give an upper bound on the number of randomly drawn training examples \\nsufficient to assure that for any target concept c in C, any consistent learner \\nusing H = C will, with probability 95%, output a hypothesis with error at \\nmost .15. \\n(b) Now suppose the rectangle boundaries a, b, c, and d take on real values instead \\nof integer values. Update your answer to the first part of this question. \\n7.3. In this chapter we derived an expression for the number of training examples suf- \\nficient to ensure that every hypothesis will have true error no worse than 6 plus \\nits observed training error errorD(h). In particular, we used Hoeffding bounds to \\nderive Equation (7.3). Derive an alternative expression for the number of training \\nexamples sufficient to ensure that every hypothesis will have true error no worse \\nthan (1 + y)errorD(h). You can use the general Chernoff bounds to derive such a \\nresult. \\nChernoff bounds: Suppose XI,. . . , Xm are the outcomes of rn independent \\ncoin flips (Bernoulli trials), where the probability of heads on any single trial is \\nPr[Xi = 11 = p and the probability of tails is Pr[Xi = 01 = 1 - p. Define S = \\nXI + X2 + -. - + Xm to be the sum of the outcomes of these m trials. The expected \\nvalue of S/m is E[S/m] = p. The Chernoff bounds govern the probability that S/m \\nwill differ from p by some factor 0 5 y 5 1. \\n7.4. Consider a learning problem in which X = % is the set of real numbers, and C = H \\nis the set of intervals over the reals, H = {(a < x < b) I a, b E E}. What is the \\nprobability that a hypothesis consistent with m examples of this target concept will \\nhave error at least E? Solve this using the VC dimension. Can you find a second \\nway to solve this, based on first principles and ignoring the VC dimension? '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 239}, page_content='7.5. Consider the space of instances X corresponding to all points in the x, y plane. Give \\nthe VC dimension of the following hypothesis spaces: \\n(a) H, = the set of all rectangles in the x, y plane. That is, H = {((a < x < b)~(c < \\nY -= d))la, b, c, d E W. \\n(b) H, = circles in the x, y plane. Points inside the circle are classified as positive \\nexamples \\n(c) H, =triangles in the x, y plane. Points inside the triangle are classified as positive \\nexamples \\n7.6. Write a consistent learner for Hr from Exercise 7.5. Generate a variety of target \\nconcept rectangles at random, corresponding to different rectangles in the plane. \\nGenerate random examples of each of these target concepts, based on a uniform \\ndistribution of instances within the rectangle from \\n(0,O) \\nto (100, 100). Plot the \\ngeneralization error as a function of the number of training examples, m. On the \\nsame graph, plot the theoretical relationship between 6 and m, for 6 = .95. Does \\ntheory fit experiment? \\n7.7. Consider the hypothesis class Hrd2 of \"regular, depth-2 decision trees\" over n \\nBoolean variables. A \"regular, depth-2 decision tree\" is a depth-2 decision tree (a \\ntree with four leaves, all distance 2 from the root) in which the left and right child \\nof the root are required to contain the same variable. For instance, the following \\ntree is in HrdZ. \\nx3 \\n/ \\\\ \\nxl xl \\n/\\\\ /\\\\ \\n+ - - + \\n(a) As a function of n, how many syntactically distinct trees are there in HrdZ? \\n(b) Give an upper bound for the number of examples needed in the PAC model to \\nlearn Hrd2 with error 6 and confidence 6. \\n(c) Consider the following WEIGHTED-MAJORITY algorithm, for the class Hrd2. YOU \\nbegin with all hypotheses in Hrd2 assigned an initial weight equal to 1. Every \\ntime you see a new example, you predict based on a weighted majority vote over \\nall hypotheses in Hrd2. Then, instead of eliminating the inconsistent trees, you cut \\ndown their weight by a factor of 2. How many mistakes will this procedure make \\nat most, as a function of n and the number of mistakes of the best tree in Hrd2? \\n7.8. This question considers the relationship between the PAC analysis considered in this \\nchapter and the evaluation of hypotheses discussed in Chapter 5. Consider a learning \\ntask in which instances are described by n boolean variables (e.g., xl A&  AX^ . . . f,) \\nand are drawn according to a fixed but unknown probability distribution V. The \\ntarget concept is known to be describable by a conjunction of boolean attributes and \\ntheir negations \\n(e.g., xz A&), and the learning algorithm uses this concept class as its \\nhypothesis space H. A consistent learner is provided a set of 100 training examples \\ndrawn according to V. It outputs a hypothesis h from H that is consistent with all \\n100 examples \\n(i.e., \\nthe error of h over these training examples is zero). \\n(a) We are interested in the true error of h, that is, the probability that it will \\nmisclassify future instances drawn randomly according to V. Based on the above \\ninformation, can you give an interval into which this true error will fall with \\nat least 95% probability? If so, state it and justify it briefly. If not, explain the \\ndifficulty. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 240}, page_content=\"(b) You now draw a new set of 100 instances, drawn independently according to the \\nsame distribution D. You find that h misclassifies 30 of these 100 new examples. \\nCan you give an interval into which this true error will fall with approximately \\n95% probability? (Ignore the performance over the earlier training data for this \\npart.) If so, state it and justify it briefly. If not, explain the difficulty. \\n(c) It may seem a bit odd that h misclassifies 30% of the new examples even though \\nit perfectly classified the training examples. Is this event more likely for large \\nn or small n? Justify your answer in a sentence. \\nREFERENCES \\nAngluin, D. (1992). Computational learning theory: Survey and selected bibliography. Proceedings \\nof the Twenty-Fourth Annual ACM Symposium on Theory of Computing (pp. 351-369). ACM \\nPress. \\nAngluin, D., \\nFrazier, \\nM., & Pitt, L. (1992). Learning conjunctions of horn clauses. Machine Learning, \\n9, 147-164. \\nAnthony, M., & Biggs, N. (1992). Computational learning theory: An introduction. Cambridge, \\nEngland: Cambridge University Press. \\nBlumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. (1989). Learnability and the Vapnik- \\nChemonenkis \\ndimension. Journal of the ACM, 36(4) (October), 929-965. \\nEhrenfeucht, A., Haussler, D., Kearns, M., & Valiant, L. (1989). A general lower bound on the \\nnumber of examples needed for learning. Informution and computation, 82, 247-261. \\nGold, E. M. (1967). Language identification in the limit. Information and Control, 10, 447-474. \\nGoldman, S. (Ed.). (1995). Special issue on computational learning theory. Machine Learning, \\n18(2/3), February. \\nHaussler, D. (1988). Quantifying inductive bias: A1 learning algorithms and Valiant's learning frame- \\nwork. ArtGcial Intelligence, 36, 177-221. \\nKearns, M. J., & Vazirani, U. V. (1994). An introduction to computational learning theory. Cambridge, \\nMA: \\nMIT \\nPress. \\nLaird, P. (1988). Learning from good and bad data. Dordrecht: Kluwer Academic Publishers. \\nLi, M., & Valiant, L. G. (Eds.). (1994). Special issue on computational learning theory. Machine \\nLearning, 14(1). \\nLittlestone, N. (1987). Learning quickly when irrelevant attributes abound: A new linear-threshold \\nalgorithm. Machine Learning, 2, 285-318. \\nLittlestone, N., & Warmuth, M. (1991). The weighted majority algorithm (Technical report UCSC- \\nCRL-91-28). Univ. of California Santa Cruz, Computer Engineering and Information Sciences \\nDept., Santa \\nCruz, \\nCA. \\nLittlestone, N., & Warmuth, M. (1994). The weighted majority algorithm. Information and Compu- \\ntation (log), 212-261. \\nPltt, L. (Ed.). (1990). Special issue on computational learning theory. Machine Learning, 5(2). \\nNatarajan, B. K. (1991). Machine learning: A theoretical approach. San Mateo, CA: Morgan Kauf- \\nmann. \\nValiant, L. (1984). A theory of the learnable. Communications of the ACM, 27(1 I), 1134-1 142. \\nVapnik, V. N. (1982). Estimation of dependences based on empirical data. New York: Springer- \\nVerlag. \\nVapnik, \\nV. N., & Chervonenkis, A. (1971). On the uniform convergence of relative frequencies of \\nevents to their probabilities. Theory of Probability and Its Applications, 16, 264-280. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 241}, page_content='CHAPTER \\nINSTANCE-BASED \\nLEARNING \\nIn contrast to learning methods that construct a general, explicit description of \\nthe target function when training examples are provided, instance-based learning \\nmethods simply store the training examples. Generalizing beyond these examples \\nis postponed until a new instance must be classified. Each time a new query \\ninstance is encountered, its relationship to the previously stored examples is ex- \\namined in order to assign a target function value for the new instance. \\nInstance- \\nbased learning includes nearest neighbor and locally weighted regression meth- \\nods that assume instances can be represented as points in a Euclidean space. It \\nalso includes case-based reasoning methods that use more complex, symbolic rep- \\nresentations for instances. Instance-based methods are sometimes referred to as \\n\"lazy\" learning methods because they delay processing until a new instance must \\nbe classified. A key advantage of this kind of delayed, or lazy, learning is \\nthat instead of estimating the target function once for the entire instance space, \\nthese methods can estimate it locally and differently for each new instance to be \\nclassified. \\n8.1 INTRODUCTION \\nInstance-based learning methods such as nearest neighbor and locally weighted re- \\ngression are conceptually straightforward approaches to approximating real-valued \\nor discrete-valued target functions. Learning in these algorithms consists of simply \\nstoring the presented training data. When a new query instance is encountered, a \\nset of similar related instances is retrieved from memory and used to classify the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 242}, page_content='CHAPTER 8 INSTANCE-BASED LEARNING 231 \\nnew query instance. One key difference between these approaches and the meth- \\nods discussed in other chapters is that instance-based approaches can construct \\na different approximation to the target function for each distinct query instance \\nthat must be classified. In fact, many techniques construct only a local approxi- \\nmation to the target function that applies in the neighborhood of the new query \\ninstance, and never construct an approximation designed to perform well over the \\nentire instance space. This has significant advantages when the target function is \\nvery complex, but can still be described by a collection of less complex local \\napproximations. \\nInstance-based methods can also use more complex, symbolic representa- \\ntions for instances. In case-based learning, instances are represented in this fashion \\nand the process for identifying \"neighboring\" instances is elaborated accordingly. \\nCase-based reasoning has been applied to tasks such as storing and reusing past \\nexperience at a help desk, reasoning about legal cases by referring to previous \\ncases, and solving complex scheduling problems by reusing relevant portions of \\npreviously solved problems. \\nOne disadvantage of instance-based approaches is that the cost of classifying \\nnew instances can be high. This is due to the fact that nearly all computation \\ntakes place at classification time rather than when the training examples are first \\nencountered. Therefore, techniques for efficiently indexing training examples are \\na significant practical issue in reducing the computation required at query time. \\nA second disadvantage to many instance-based approaches, especially nearest- \\nneighbor approaches, is that they typically consider all attributes of the instances \\nwhen attempting to retrieve similar training examples from memory. If the target \\nconcept depends on only a few of the many available attributes, then the instances \\nthat are truly most \"similar\" may well be a large distance apart. \\nIn the next section we introduce the k-NEAREST NEIGHBOR learning algo- \\nrithm, including several variants of this widely-used approach. The subsequent \\nsection discusses locally weighted regression, a learning method that constructs \\nlocal approximations to the target function and that can be viewed as a general- \\nization of k-NEAREST NEIGHBOR algorithms. We then describe radial basis function \\nnetworks, which provide an interesting bridge between instance-based and neural \\nnetwork learning algorithms. The next section discusses case-based reasoning, an \\ninstance-based approach that employs symbolic representations and \\nknowledge- \\nbased inference. This section includes an example application of case-based rea- \\nsoning to a problem in engineering design. Finally, we discuss the \\nfundarnen- \\ntal \\ndifferences in capabilities that distinguish lazy learning methods discussed in \\nthis \\nchapter from eager learning methods discussed in the other chapters of this \\nbook. \\n8.2 k-NEAREST NEIGHBOR LEARNING \\nThe most basic instance-based method is the k-NEAREST NEIGHBOR algorithm. This \\nalgorithm assumes all instances correspond to points in the n-dimensional space \\n8\". The nearest neighbors of an instance are defined in terms of the standard '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 243}, page_content='I \\nEuclidean distance. More precisely, let an arbitrary instance x be described by the \\nfeature vector \\nwhere ar (x) denotes the value of the rth attribute of instance x. Then the distance \\nbetween two instances xi and xj is defined to be d(xi, xj), where \\nIn nearest-neighbor learning the target function may be either discrete-valued \\nor real-valued. Let us first consider learning discrete-valued target functions of the \\nform f : W -+ V, where V is the finite set {vl, . . . v,}. The k-NEAREST NEIGHBOR \\nalgorithm for \\napproximatin5 \\na discrete-valued target function is given in Table 8.1. \\nAs shown there, the value f (x,) returned by this algorithm as its estimate of f (x,) \\nis just the most common value of f among the k training examples nearest to \\nx,. If we choose k = 1, then the 1-NEAREST NEIGHBOR algorithm assigns to f(x,) \\nthe value f (xi) where xi is the training instance nearest to x,. For larger values \\nof k, the algorithm assigns the most common value among the k nearest training \\nexamples. \\nFigure 8.1 illustrates the operation of the k-NEAREST NEIGHBOR algorithm for \\nthe case where the instances are points in a two-dimensional space and where the \\ntarget function is boolean valued. The positive and negative training examples are \\nshown by \"+\" and \"-\" respectively. A query point x, is shown as well. Note the \\n1-NEAREST NEIGHBOR algorithm classifies x, as a positive example in this figure, \\nwhereas the 5-NEAREST NEIGHBOR algorithm classifies it as a negative example. \\nWhat is the nature of the hypothesis space H implicitly considered by the \\nk-NEAREST NEIGHBOR algorithm? Note the k-NEAREST NEIGHBOR algorithm never \\nforms an explicit general hypothesis f regarding the target function f. It simply \\ncomputes the classification of each new query instance as needed. Nevertheless, \\nTraining algorithm: \\nFor each training example (x, f (x)), add the example to the list trainingaxamples \\nClassification algorithm: \\nGiven a query instance xq to be classified, \\nLet xl . . .xk denote the k instances from trainingaxamples that are nearest to xq \\nReturn \\nG \\nwhere S(a, b) = 1 if a = b and where 6(a, b) = 0 otherwise. \\nTABLE 8.1 \\nThe k-NEAREST NEIGHBOR algorithm for approximating a discrete-valued function f : 8\" -+ V. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 244}, page_content='CHAPTER 8 INSTANCE-BASED LEARNING 233 \\nFIGURE 8.1 \\nk-NEAREST NEIGHBOR. A set of positive and negative training examples is shown on the left, along \\nwith a query instance x, to be classified. The I-NEAREST NEIGHBOR algorithm classifies x, positive, \\nwhereas 5-NEAREST NEIGHBOR classifies it as negative. \\nOn \\nthe right is the decision surface induced \\nby the 1-NEAREST NEIGHBOR algorithm for a typical set of training examples. The convex polygon \\nsurrounding each training example indicates the region of instance space closest to that point \\n(i.e., \\nthe instances for which the 1-NEAREST NEIGHBOR algorithm will assign the classification belonging \\nto that training example). \\nwe can still ask what the implicit general function is, or what classifications \\nwould be assigned if we were to hold the training examples constant and query \\nthe algorithm with every possible instance in X. The diagram on the right side \\nof Figure 8.1 shows the shape of this decision surface induced by 1-NEAREST \\nNEIGHBOR over the entire instance space. The decision surface is a combination of \\nconvex polyhedra surrounding each of the training examples. For every training \\nexample, the polyhedron indicates the set of query points whose classification \\nwill be completely determined by that training example. Query points outside the \\npolyhedron are closer to some other training example. This kind of diagram is \\noften called the Voronoi diagram of the set of training examples. \\nThe k-NEAREST NEIGHBOR algorithm is easily adapted to approximating \\ncontinuous-valued target functions. To accomplish this, we have the algorithm \\ncalculate the mean value of the k nearest training examples rather than calculate \\ntheir most common value. More precisely, to approximate a real-valued target \\nfunction f : !)In + !)I we replace the final line of the above algorithm by the line \\n8.2.1 Distance-Weighted NEAREST NEIGHBOR Algorithm \\nOne obvious refinement to the k-NEAREST NEIGHBOR algorithm is to weight the con- \\ntribution of each of the k neighbors according to their distance to the query point \\nx,, giving greater weight to closer neighbors. For example, in the algorithm of \\nTable 8.1, which approximates discrete-valued target functions, we might weight \\nthe vote of each neighbor according to the inverse square of its distance from x,. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 245}, page_content=\"This can be accomplished by replacing the final line of the algorithm by \\nwhere \\nTo accommodate the case where the query point x, exactly matches one of the \\ntraining instances xi and the denominator d(x,, xi12 is therefore zero, we assign \\nf(x,) to be f (xi) in this case. If there are several such training examples, we \\nassign the majority classification among them. \\nWe can distance-weight the instances for real-valued target functions in a \\nsimilar fashion, replacing the final line of the algorithm in this case by \\nwhere wi is as defined in Equation (8.3). Note the denominator in Equation (8.4) is \\na constant that normalizes the contributions of the various weights \\n(e.g., \\nit assures \\nthat if f (xi) = c for all training examples, then f(x,) t c as well). \\nNote all of the above variants of the k-NEAREST NEIGHBOR algorithm consider \\nonly the k nearest neighbors to classify the query point. Once we add distance \\nweighting, there is really no harm in allowing all training examples to have an \\ninfluence on the classification of the x,, because very distant examples will have \\nvery little effect on f(x,). The only disadvantage of considering all examples is \\nthat our classifier will run more slowly. If all training examples are considered \\nwhen classifying a new query instance, we call the algorithm a global method. \\nIf only the nearest training examples are considered, we call it a local method. \\nWhen the rule in Equation (8.4) is applied as a global method, using all training \\nexamples, it is known as Shepard's method \\n(Shepard \\n1968). \\n8.2.2 Remarks on k-NEAREST NEIGHBOR Algorithm \\nThe distance-weighted k-NEAREST NEIGHBOR algorithm is a highly effective induc- \\ntive inference method for many practical problems. It is robust to noisy training \\ndata and quite effective when it is provided a sufficiently large set of training \\ndata. Note that by taking the weighted average of the k neighbors nearest to the \\nquery point, it can smooth out the impact of isolated noisy training examples. \\nWhat is the inductive bias of k-NEAREST NEIGHBOR? The basis for classifying \\nnew query points is easily understood based on the diagrams in Figure 8.1. The \\ninductive bias corresponds to an assumption that the classification of an instance \\nx, will be most similar to the classification of other instances that are nearby in \\nEuclidean distance. \\nOne practical issue in applying k-NEAREST NEIGHBOR algorithms is that the \\ndistance between instances is calculated based on all attributes of the instance \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 246}, page_content='(i.e., on all axes in the Euclidean space containing the instances). This lies in \\ncontrast to methods such as rule and decision tree learning systems that select \\nonly a subset of the instance attributes when forming the hypothesis. To see the \\neffect of this policy, consider applying k-NEAREST NEIGHBOR to a problem in which \\neach instance is described by 20 attributes, but where only 2 of these attributes \\nare relevant to determining the classification for the particular target function. In \\nthis case, instances that have identical values for the 2 relevant attributes may \\nnevertheless be distant from one another in the 20-dimensional instance space. \\nAs a result, the similarity metric used by k-NEAREST NEIGHBOR--depending on \\nall 20 attributes-will be misleading. The distance between neighbors will be \\ndominated by the large number of irrelevant attributes. This difficulty, which \\narises when many irrelevant attributes are present, is sometimes referred to as the \\ncurse of dimensionality. Nearest-neighbor approaches are especially sensitive to \\nthis problem. \\nOne interesting approach to overcoming this problem is to weight each \\nattribute differently when calculating the distance between two instances. This \\ncorresponds to stretching the axes in the Euclidean space, shortening the axes that \\ncorrespond to less relevant attributes, and lengthening the axes that correspond \\nto more relevant attributes. The amount by which each axis should be stretched \\ncan be determined automatically using a cross-validation approach. To see how, \\nfirst note that we wish to stretch (multiply) the jth axis by some factor zj, where \\nthe values zl . . . z, are chosen to minimize the true classification error of the \\nlearning algorithm. Second, note that this true error can be estimated using cross- \\nvalidation. Hence, one algorithm is to select a random subset of the available \\ndata to use as training examples, then determine the values of zl . . . z, that lead \\nto the \\nminimum \\nerror in classifying the remaining examples. By repeating this \\nprocess multiple times the estimate for these weighting factors can be made more \\naccurate. This process of stretching the axes in order to optimize the performance \\nof \\nk-NEAREST \\nNEIGHBOR provides a mechanism for suppressing the impact of \\nirrelevant attributes. \\nAn even more drastic alternative is to completely eliminate the least relevant \\nattributes from the instance space. This is equivalent to setting some of the zi \\nscaling factors to zero. Moore and Lee (1994) discuss efficient cross-validation \\nmethods for selecting relevant subsets of the attributes for \\nk-NEAREST \\nNEIGHBOR \\nalgorithms. In particular, they explore methods based on leave-one-out cross- \\nvalidation, in which the set of m training instances is repeatedly divided into a \\ntraining set of size m - 1 and test set of size 1, in all possible ways. This leave-one- \\nout approach is easily implemented in k-NEAREST NEIGHBOR algorithms because \\nno additional training effort is required each time the training set is redefined. \\nNote both of the above approaches can be seen as stretching each axis by some \\nconstant factor. Alternatively, we could stretch each axis by a value that varies over \\nthe instance space. However, as we increase the number of degrees of freedom \\navailable to the algorithm for redefining its distance metric in such a fashion, we \\nalso increase the risk of overfitting. Therefore, the approach of locally stretching \\nthe axes is much less common. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 247}, page_content='One additional practical issue in applying k-NEAREST NEIGHBOR is efficient \\nmemory indexing. Because this algorithm delays all processing until a new query \\nis received, significant computation can be required to process each new query. \\nVarious methods have been developed for indexing the stored training examples so \\nthat the nearest neighbors can be identified more efficiently at some additional cost \\nin memory. One such indexing method is the kd-tree \\n(Bentley \\n1975; \\nFriedman \\net al. \\n1977), \\nin which instances are stored at the leaves of a tree, with nearby \\ninstances stored at the same or nearby nodes. The internal nodes of the tree sort \\nthe new query x, to the relevant leaf by testing selected attributes of x,. \\n8.2.3 A Note on Terminology \\nMuch of the literature on nearest-neighbor methods and weighted local regression \\nuses a terminology that has arisen from the field of statistical pattern recognition. \\nIn reading that literature, it is useful to know the following terms: \\n0 Regression means approximating a real-valued target function. \\nResidual is the error {(x) - f (x) in approximating the target function. \\nKernel function is the function of distance that is used to determine the \\nweight of each training example. In other words, the kernel function is the \\nfunction K such that wi = K(d(xi, x,)). \\n83 LOCALLY WEIGHTED REGRESSION \\nThe nearest-neighbor approaches described in the previous section can be thought \\nof as approximating the target function f (x) at the single query point x = x,. \\nLocally weighted regression is a generalization of this approach. It constructs an \\nexplicit approximation to f over a local region surrounding x,. Locally weighted \\nregression uses nearby or distance-weighted training examples to form this local \\napproximation to f. For example, we might approximate the target function in \\nthe neighborhood surrounding x, using a linear function, a quadratic function, \\na multilayer neural network, or some other functional form. The phrase \"locally \\nweighted regression\" is called local because the function is approximated based a \\nonly on data near the query point, weighted because the contribution of each \\ntraining example is weighted by its distance from the query point, and regression \\nbecause this is the term used widely in the statistical learning community for the \\nproblem of approximating real-valued functions. \\nGiven a new query instance x,, the general approach in locally weighted \\nregression is to construct an approximation f^ that fits the training examples in the \\nneighborhood surrounding x,. This approximation is then used to calculate the \\nvalue f\"(x,), which is output as the estimated target value for the query instance. \\nThe description of f^ may then be deleted, because a different local approximation \\nwill be calculated for each distinct query instance. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 248}, page_content='CHAPTER 8 INSTANCE-BASED LEARNING 237 \\n8.3.1 Locally Weighted Linear Regression \\nLet us consider the case of locally weighted regression in which the target function \\nf is approximated near x, using a linear function of the form \\nAs before, ai(x) denotes the value of the ith attribute of the instance x. \\nRecall that in Chapter 4 we discussed methods such as gradient descent to \\nfind the coefficients wo . . . w, to minimize the error in fitting such linear func- \\ntions to a given set of training examples. In that chapter we were interested in \\na global approximation to the target function. Therefore, we derived methods to \\nchoose weights that minimize the squared error summed over the set D of training \\nexamples \\nwhich led us to the gradient descent training rule \\nwhere q is a constant learning rate, and where the training rule has been re- \\nexpressed from the notation of Chapter 4 to fit our current notation (i.e., t + f (x), \\no -+ f(x), and xj -+ aj(x)). \\nHow shall we modify this procedure to derive a local approximation rather \\nthan a global one? The simple way is to redefine the error criterion E to emphasize \\nfitting the local training examples. Three possible criteria are given below. Note \\nwe write the error E(x,) to emphasize the fact that now the error is being defined \\nas a function of the query point x,. \\n1. Minimize the squared error over just the k nearest neighbors: \\n1 El(xq) = - C (f (x) - f^(xN2 \\nxc k nearest nbrs of xq \\n2. Minimize the squared error over the entire set D of training examples, while \\nweighting the error of each training example by some decreasing function \\nK of its distance from x, : \\n3. Combine 1 and 2: \\nCriterion two is perhaps the most esthetically pleasing because it allows \\nevery training example to have an impact on the classification of x,. However, '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 249}, page_content='this approach requires computation that grows linearly with the number of training \\nexamples. Criterion three is a good approximation to criterion two and has the \\nadvantage that computational cost is independent of the total number of training \\nexamples; its cost depends only on the number k of neighbors considered. \\nIf we choose criterion three above and rederive the gradient descent rule \\nusing the same style of argument as in Chapter 4, we obtain the following training \\nrule (see Exercise 8.1): \\nNotice the only differences between this new rule and the rule given by Equa- \\ntion (8.6) are that the contribution of instance x to the weight update is now \\nmultiplied by the distance penalty K(d(x,, x)), and that the error is summed over \\nonly the k nearest training examples. In fact, if we are fitting a linear function \\nto a fixed set of training examples, then methods much more efficient than gra- \\ndient descent are available to directly solve for the desired coefficients wo . . . urn. \\nAtkeson et al. (1997a) and Bishop (1995) survey several such methods. \\n8.3.2 Remarks on Locally Weighted Regression \\nAbove we considered using a linear function to approximate f in the neigh- \\nborhood of the query instance x,. The literature on locally weighted regression \\ncontains a broad range of alternative methods for distance weighting the training \\nexamples, and a range of methods for locally approximating the target function. In \\nmost cases, the target function is approximated by a constant, linear, or quadratic \\nfunction. More complex functional forms are not often found because (1) the cost \\nof fitting more complex functions for each query instance is prohibitively high, \\nand (2) these simple approximations model the target function quite well over a \\nsufficiently small subregion of the instance space. \\n8.4 RADIAL BASIS FUNCTIONS \\nOne approach to function approximation that is closely related to distance-weighted \\nregression and also to artificial neural networks is learning with radial basis func- \\ntions (Powell 1987; Broomhead and \\nLowe \\n1988; Moody and Darken 1989). In \\nthis approach, the learned hypothesis is a function of the form \\nwhere each xu is an instance from X and where the kernel function K,(d(x,, x)) \\nis defined so that it decreases as the distance d(x,, x) increases. Here k is a user- \\nprovided constant that specifies the number of kernel functions to be included. \\nEven though f(x) is a global approximation to f (x), the contribution from each \\nof the Ku(d (xu, x)) terms is localized to a region nearby the point xu. It is common '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 250}, page_content='CHmR 8 INSTANCE-BASED LEARNING 239 \\nto choose each function K, (d (xu, x)) to be a Gaussian function (see Table 5.4) \\ncentered at the point xu with some variance a;. \\n+d2(xu,x) \\nK,(d(x,, x)) = e2\". \\nWe will restrict our discussion here to this common Gaussian kernel function. \\nAs shown by \\nHartman \\net al. \\n(1990), \\nthe functional form of Equation (8.8) can \\napproximate any function with arbitrarily small error, provided a sufficiently large \\nnumber k of such Gaussian kernels and provided the width a2 of each kernel can \\nbe separately specified. \\nThe function given by Equation (8.8) can be viewed as describing a \\ntwo- \\nlayer network where the first layer of units computes the values of the various \\nK,(d(x,, x)) and where the second layer computes a linear combination of these \\nfirst-layer unit values. An example radial basis function (RBF) network is illus- \\ntrated in Figure 8.2. \\nGiven a set of training examples of the target function, RBF networks are \\ntypically trained in a two-stage process. First, the number k of hidden units is \\ndetermined and each hidden unit u is defined by choosing the values of xu and a: \\nthat define its kernel function K,(d(x,, x)). Second, the weights w, are trained to \\nmaximize the fit of the network to the training data, using the global error criterion \\ngiven by Equation (8.5). Because the kernel functions are held fixed during this \\nsecond stage, the linear weight values w, can be trained very efficiently. \\nSeveral alternative methods have been proposed for choosing an appropriate \\nnumber of hidden units or, equivalently, kernel functions. One approach is to \\nallocate a Gaussian kernel function for each training example (xi, f (xi)), centering \\nthis Gaussian at the point xi. Each of these kernels may be assigned the same width \\na2. Given this approach, the RBF network learns a global approximation to the \\ntarget function in which each training example (xi, f (xi)) can influence the value \\nof f only in the neighborhood of xi. One advantage of this choice of kernel \\nfunctions is that it allows the \\nRBF \\nnetwork to fit the training data exactly. That \\nis, for any set of m training examples the weights wo . . . w, for combining the \\nm Gaussian kernel functions can be set so that f(xi) = f (xi) for each training \\nFIGURE 8.2 \\nA radial basis function network. Each hidden unit produces \\nan activation determined by a Gaussian function centered at \\nsome instance xu. Therefore, its activation will be close to zero \\nunless the input x is near xu. The output unit produces a linear \\ncombination of the hidden unit activations. Although the network \\nshown here has just one output, multiple output units can also \\nbe included. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 251}, page_content=\"A second approach is to choose a set of kernel functions that is smaller \\nthan the number of training examples. This approach can be much more effi- \\ncient than the first approach, especially when the number of training examples \\nis large. The set of kernel functions may be distributed with centers spaced uni- \\nformly throughout the instance space X. Alternatively, we may wish to distribute \\nthe centers nonuniformly, especially if the instances themselves are found to be \\ndistributed \\nnonuniformly \\nover X. In this later case, we can pick kernel function \\ncenters by randomly selecting a subset of the training instances, thereby sampling \\nthe underlying distribution of instances. Alternatively, we may identify prototyp- \\nical clusters of instances, then add a kernel function centered at each cluster. The \\nplacement of the kernel functions in this fashion can be accomplished using un- \\nsupervised clustering algorithms that fit the training instances (but not their target \\nvalues) to a mixture of Gaussians. The EM algorithm discussed in Section 6.12.1 \\nprovides one algorithm for choosing the means of a mixture of k Gaussians to \\nbest fit the observed instances. In the case of the EM algorithm, the means are \\nchosen to maximize the probability of observing the instances xi, given the k \\nestimated means. Note the target function value f (xi) of the instance does not \\nenter into the calculation of kernel centers by unsupervised clustering methods. \\nThe only role of the target values f (xi) in this case is to determine the output \\nlayer weights w,. \\nTo summarize, radial basis function networks provide a global approxima- \\ntion to the target function, represented by a linear combination of many local \\nkernel functions. The value for any given kernel function is non-negligible only \\nwhen the input x falls into the region defined by its particular center and width. \\nThus, the network can be viewed as a smooth linear combination of many local \\napproximations to the target function. One key advantage to RBF networks is that \\nthey can be trained much more efficiently than feedforward networks trained with \\nBACKPROPAGATION. This follows from the fact that the input layer and the output \\nlayer of an RBF are trained separately. \\n8.5 CASE-BASED REASONING \\nInstance-based methods such as k-NEAREST NEIGHBOR and locally weighted re- \\ngression share three key properties. First, they are lazy learning methods in that \\nthey defer the decision of how to generalize beyond the training data until a new \\nquery instance is observed. Second, they classify new query instances by ana- \\nlyzing similar instances while ignoring instances that are very different from the \\nquery. \\nThird, they represent instances as real-valued points in an n-dimensional \\nEuclidean space. Case-based reasoning (CBR) is a learning paradigm based on \\nthe first two of these principles, but not the third. In CBR, instances are typi- \\nca:'y \\nrepresented using more rich symbolic descriptions, and the methods used \\nto retrieve similar instances are correspondingly more elaborate. CBR has been \\napplied to problems such as conceptual design of mechanical devices based on \\na stored library of previous designs \\n(Sycara \\net al. \\n1992), \\nreasoning about new \\nlegal cases based on previous rulings (Ashley \\n1990), \\nand solving planning and \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 252}, page_content='CHAPTER 8 INSTANCEBASED LEARNING 241 \\nscheduling problems by reusing and combining portions of previous solutions to \\nsimilar problems (Veloso \\n1992). \\nLet us consider a prototypical example of a case-based reasoning system to \\nground our discussion. The CADET system (Sycara et al. 1992) employs case- \\nbased reasoning to assist in the conceptual design of simple mechanical devices \\nsuch as water faucets. It uses a library containing approximately 75 previous \\ndesigns and design fragments to suggest conceptual designs to meet the specifi- \\ncations of new design problems. Each instance stored in memory \\n(e.g., \\na water \\npipe) is represented by describing both its structure and its qualitative function. \\nNew design problems are then presented by specifying the desired function and \\nrequesting the corresponding structure. This problem setting is illustrated in Fig- \\nure 8.3. The top half of the figure shows the description of a typical stored case \\ncalled a T-junction pipe. Its function is represented in terms of the qualitative re- \\nlationships among the waterflow levels and temperatures at its inputs and outputs. \\nIn the functional description at its right, an arrow with a \"+\" label indicates that \\nthe variable at the arrowhead increases with the variable at its tail. For example, \\nthe output waterflow Q3 increases with increasing input waterflow Ql. Similarly, \\nA stored case: T-junction pipe \\nStructure: \\nQIJT T = temperature \\n\\'L \\nQ = watertlow \\nr Q3J5 Qz4 \\nA problem specification: Water faucet \\nStructure: \\nFunction: \\nFunction: \\nFIGURE 8.3 \\nA stored case and a new problem. The top half of the figure describes a typical design fragment \\nin the case library of CADET. The function is represented by the graph of qualitative dependencies \\namong the T-junction variables (described in the text). The bottom half of the figure shows a typical \\ndesign problem. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 253}, page_content='a \"-\" label indicates that the variable at the head decreases with the variable at \\nthe tail. The bottom half of this figure depicts a new design problem described \\nby its desired function. This particular function describes the required behavior of \\none type of water faucet. Here \\nQ, \\nrefers to the flow of cold water into the faucet, \\nQh \\nto the input flow of hot water, and \\nQ, \\nto the single mixed flow out of the \\nfaucet. Similarly, \\nT,, \\nTh, \\nand \\nT, \\nrefer to the temperatures of the cold water, hot \\nwater, and mixed water respectively. The variable C, denotes the control signal \\nfor temperature that is input to the faucet, and Cf denotes the control signal for \\nwaterflow. \\nNote the description of the desired function specifies that these con- \\ntrols C, and Cf are to influence the water flows Q, and Qh, thereby indirectly \\ninfluencing the faucet output flow \\nQ, \\nand temperature T,. \\nGiven this functional specification for the new design problem, CADET \\nsearches its library for stored cases whose functional descriptions match the design \\nproblem. If an exact match is found, indicating that some stored case implements \\nexactly the desired function, then this case can be returned as a suggested solution \\nto the design problem. If no exact match occurs, CADET may find cases that \\nmatch various subgraphs of the desired functional specification. In Figure 8.3, for \\nexample, the \\nT-junction \\nfunction matches a \\nsubgraph \\nof the water faucet function \\ngraph. More generally, CADET searches for subgraph isomorphisms between the \\ntwo function graphs, so that parts of a case can be found to match parts of the \\ndesign specification. Furthermore, the system may elaborate the original function \\nspecification graph in order to create functionally equivalent graphs that may \\nmatch still more cases. It uses general knowledge about physical influences to \\ncreate these elaborated function graphs. For example, it uses a rewrite rule that \\nallows it to rewrite the influence \\nThis rewrite rule can be interpreted as stating that if B must increase with A, \\nthen it is sufficient to find some other quantity x such that B increases with x, \\nand x increases with A. Here x is a universally quantified variable whose value \\nis bound when matching the function graph against the case library. In fact, the \\nfunction graph for the faucet shown in Figure 8.3 is an elaboration of the original - \\nfunctional specification produced by applying such rewrite rules. \\nBy retrieving multiple cases that match different subgraphs, the entire de- \\nsign can sometimes be pieced together. In general, the process of producing a \\nfinal solution from multiple retrieved cases can be very complex. It may require \\ndesigning portions of the system from first principles, in addition to merging re- \\ntrieved portions from stored cases. It may also require backtracking on earlier \\nchoices of design \\nsubgoals \\nand, therefore, rejecting cases that were previously \\nretrieved. CADET has very limited capabilities for combining and adapting multi- \\nple retrieved cases to form the final design and relies heavily on the user for this \\nadaptation stage of the process. As described by Sycara et al. (1992), CADET is '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 254}, page_content='CHAPTER 8 INSTANCE-BASED LEARMNG 243 \\na research prototype system intended to explore the potential role of case-based \\nreasoning in conceptual design. It does not have the range of analysis algorithms \\nneeded to refine these abstract conceptual designs into final designs. \\nIt is instructive to examine the correspondence between the problem setting \\nof CADET and the general setting for instance-based methods such as k-NEAREST \\nNEIGHBOR. In CADET each stored training example describes a function graph \\nalong with the structure that implements it. New queries correspond to new func- \\ntion graphs. Thus, we can map the CADET problem into our standard notation by \\ndefining the space of instances X to be the space of all function graphs. The tar- \\nget function f maps function graphs to the structures that implement them. Each \\nstored training example (x, f (x)) is a pair that describes some function graph x \\nand the structure f (x) that implements x. The system must learn from the training \\nexample cases to output the structure f (x,) that successfully implements the input \\nfunction graph query \\nx,. \\nThe above sketch of the CADET system illustrates several generic properties \\nof case-based reasoning systems that distinguish them from approaches such as \\nk-NEAREST NEIGHBOR. \\n0 Instances or cases may be represented by rich symbolic descriptions, such \\nas the function graphs used in CADET. This may require a similarity metric \\ndifferent from Euclidean distance, such as the size of the largest shared \\nsubgraph \\nbetween two function graphs. \\n0 Multiple retrieved cases may be combined to form the solution to the new \\nproblem. This is similar to the k-NEAREST NEIGHBOR approach, in that mul- \\ntiple similar cases are used to construct a response for the new query. \\nHowever, the process for combining these multiple retrieved cases can be \\nvery different, relying on knowledge-based reasoning rather than statistical \\nmethods. \\n0 There may be a tight coupling between case retrieval, knowledge-based \\nreasoning, and problem solving. One simple example of this is found in \\nCADET, which uses generic knowledge about influences to rewrite function \\ngraphs during its attempt to find matching cases. Other systems have been \\ndeveloped that more fully integrate case-based reasoning into general \\nsearch- \\nbased problem-solving systems. Two examples are \\nANAPRON \\n(Golding and \\nRosenbloom \\n199 \\n1) \\nand \\nPRODIGY/ANALOGY \\n(Veloso \\n1992). \\nTo summarize, case-based reasoning is an instance-based learning method \\nin which instances (cases) may be rich relational descriptions and in which the re- \\ntrieval and combination of cases to solve the current query may rely on \\nknowledge- \\nbased reasoning and search-intensive problem-solving methods. One current re- \\nsearch issue in case-based reasoning is to develop improved methods for indexing \\ncases. The central issue here is that syntactic similarity measures \\n(e.g., \\nsubgraph \\nisomorphism between function graphs) provide only an approximate indication of \\nthe relevance of a particular case to a particular problem. When the CBR system \\nattempts to reuse the retrieved cases it may uncover difficulties that were not '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 255}, page_content='244 MACHINE LEARNING \\ncaptured by this syntactic similarity measure. For example, in CADET the multi- \\nple retrieved design fragments may turn out to be incompatible with one another, \\nmaking it impossible to combine them into a consistent final design. When this \\noccurs in general, the CBR system may backtrack and search for additional cases, \\nadapt the existing cases, or resort to other problem-solving methods. Importantly, \\nwhen such difficulties are detected they also provide training data for improving \\nthe similarity metric or, equivalently, the indexing structure for the case library. \\nIn particular, if a case is retrieved based on the similarity metric, but found to be \\nirrelevant based on further analysis, then the similarity metric should be refined \\nto reject this case for similar subsequent queries. \\n8.6 REMARKS ON LAZY AND EAGER LEARNING \\nIn this chapter we considered three lazy learning methods: the k-NEAREST NEIGH- \\nBOR algorithm, locally weighted regression, and case-based reasoning. We call \\nthese methods lazy because they defer the decision of how to generalize beyond \\nthe training data until each new query instance is encountered. We also discussed \\none eager learning method: the method for learning radial basis function networks. \\nWe call this method eager because it generalizes beyond the training data before \\nobserving the new query, committing at training time to the network structure and \\nweights that define its approximation to the target function. In this same sense, \\nevery other algorithm discussed elsewhere in this book \\n(e.g., \\nBACKPROPAGATION, \\nC4.5) is an eager learning algorithm. \\nAre \\nthere important differences in what can be achieved by lazy versus eager \\nlearning? Let us distinguish between two kinds of differences: differences in com- \\nputation time and differences in the classifications produced for new queries. There \\nare obviously differences in computation time between eager and lazy methods. \\nFor example, lazy methods will generally require less computation during training, \\nbut more computation when they must predict the target value for a new query. \\nThe more fundamental question is whether there are essential differences in \\nthe inductive bias that can be achieved by lazy versus eager methods. The key \\ndifference between lazy and eager methods in this regard is \\n0 Lazy methods may consider the query instance x, when deciding how to \\ngeneralize beyond the training data D. \\n0 Eager methods cannot. By the time they observe the query instance x, they \\nhave already chosen their (global) approximation to the target function. \\nDoes this distinction affect the generalization accuracy of the learner? It does if we \\nrequire that the lazy and eager learner employ the same hypothesis space H. To \\nillustrate, consider the hypothesis space consisting of linear functions. The locally \\nweighted linear regression algorithm discussed earlier is a lazy learning method \\nbased on this hypothesis space. For each new query x, it generalizes from the \\ntraining data by choosing a new hypothesis based on the training examples near x,. \\nIn contrast, an eager learner that uses the same hypothesis space of linear functions '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 256}, page_content='CHAPTER 8 INSTANCE-BASED LEARNING 245 \\nmust choose its approximation before the queries are observed. The eager learner \\nmust therefore commit to a single linear function hypothesis that covers the entire \\ninstance space and all future queries. The lazy method effectively uses a richer \\nhypothesis space because it uses many different local linear functions to form its \\nimplicit global approximation to the target function. Note this same situation holds \\nfor other learners and hypothesis spaces as well. A lazy version of \\nBACKPROPAGA- \\nTION, for example, could learn a different neural network for each distinct query \\npoint, compared to the eager version of \\nBACKPROPAGATION \\ndiscussed in Chapter 4. \\nThe key point in the above paragraph is that a lazy learner has the option \\nof (implicitly) representing the target function by a combination of many local \\napproximations, whereas an eager learner must commit at training time to a single \\nglobal approximation. The distinction between eager and lazy learning is thus \\nrelated to the distinction between global and local approximations to the target \\nfunction. \\nCan we create eager methods that use multiple local approximations to \\nachieve the same effects as lazy local methods? Radial basis function networks can \\nbe seen as one attempt to achieve this. The RBF learning methods we discussed \\nare eager methods that commit to a global approximation to the target function \\nat training time. However, an \\nRBF \\nnetwork represents this global function as a \\nlinear combination of multiple local kernel functions. Nevertheless, because RBF \\nlearning methods must commit to the hypothesis before the query point is known, \\nthe local approximations they create are not specifically targeted to the query \\npoint to the same degree as in a lazy learning method. Instead, \\nRBF \\nnetworks are \\nbuilt eagerly from local approximations centered around the training examples, or \\naround clusters of training examples, but not around the unknown future query \\npoints. \\nTo summarize, lazy methods have the option of selecting a different hypoth- \\nesis or local approximation to the target function for each query instance. Eager \\nmethods using the same hypothesis space are more restricted because they must \\ncommit to a single hypothesis that covers the entire instance space. Eager methods \\ncan, of course, employ hypothesis spaces that combine multiple local approxima- \\ntions, as in RBF networks. However, even these combined local approximations do \\nnot give eager methods the full ability of lazy methods to customize to unknown \\nfuture query instances. \\n8.7 SUMMARY AND FURTHER READING \\nThe main points of this chapter include: \\nInstance-based learning methods differ from other approaches to function ap- \\nproximation because they delay processing of training examples until they \\nmust label a new query instance. As a result, they need not form an explicit \\nhypothesis of the entire target function over the entire instance space, in- \\ndependent of the query instance. Instead, they may form a different local \\napproximation to the target function for each query instance. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 257}, page_content='246 MACHINE LEARNING \\n1 \\n0 Advantages of instance-based methods include the ability to model complex \\ntarget functions by a collection of less complex local approximations and the \\nfact that information present in the training examples is never lost (because \\nthe examples themselves are stored explicitly). The main practical difficul- \\nties include efficiency of labeling new instances (all processing is done at \\nquery time rather than in advance), difficulties in determining an appropriate \\ndistance metric for retrieving \"related\" instances (especially when examples \\nare represented by complex symbolic descriptions), and the negative impact \\nof irrelevant features on the distance metric. \\n0 k-NEAREST NEIGHBOR is an instance-based algorithm for approximating real- \\nvalued or discrete-valued target functions, assuming instances correspond to \\npoints in an n-dimensional Euclidean space. The target function value for \\na new query is estimated from the known values of the k nearest training \\nexamples. \\n0 Locally weighted regression methods are a generalization of k-NEAREST \\nNEIGHBOR in which an explicit local approximation to the target function \\nis constructed for each query instance. The local approximation to the target \\nfunction may be based on a variety of functional forms such as constant, \\nlinear, or quadratic functions or on spatially localized kernel functions. \\n0 Radial basis function (RBF) networks are a type of artificial neural network \\nconstructed from spatially localized kernel functions. These can be seen as a \\nblend of instance-based approaches (spatially localized influence of each ker- \\nnel function) and neural network approaches (a global approximation to the \\ntarget function is formed at training time rather than a local approximation \\nat query time). Radial basis function networks have been used successfully \\nin applications such as interpreting visual scenes, in which the assumption \\nof spatially local influences is well-justified. \\n0 Case-based reasoning is an instance-based approach in which instances are \\nrepresented by complex logical descriptions rather than points in a Euclidean \\nspace. Given these complex symbolic descriptions of instances, a rich variety \\nof methods have been proposed for mapping from the training examples to \\ntarget function values for new instances. Case-based reasoning methods have \\nbeen used in applications such as modeling legal reasoning and for guiding \\nsearches in complex manufacturing and transportation planning problems. \\nThe k-NEAREST NEIGHBOR algorithm is one of the most thoroughly analyzed \\nalgorithms in machine learning, due in part to its age and in part to its simplicity. \\nCover and Hart (1967) present early theoretical results, and Duda and Hart (1973) \\nprovide a good overview. Bishop (1995) provides a discussion of k-NEAREST \\nNEIGHBOR and its relation to estimating probability densities. An excellent current \\nsurvey of methods for locally weighted regression is given by Atkeson et al. \\n(1997). The application of these methods to robot control is surveyed by Atkeson \\net al. \\n(1997b). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 258}, page_content=\"A thorough discussion of radial basis functions is provided by Bishop (1995). \\nOther treatments are given by Powell (1987) and Poggio and Girosi (1990). See \\nSection 6.12 of this book for a discussion of the EM algorithm and its application \\nto selecting the means of a mixture of Gaussians. \\nKolodner (1993) provides a general introduction to case-based reasoning. \\nOther general surveys and collections describing recent research are given by \\nAamodt et al. (1994), Aha et al. (1991), Haton et al. (1995), Riesbeck and Schank \\n(1989), Schank et al. (1994), Veloso and Aamodt (1995), Watson (1995), and \\nWess et al. (1994). \\nEXERCISES \\n8.1. Derive the gradient descent rule for a distance-weighted local linear approximation \\nto the target function, given by Equation (8.1). \\n8.2. Consider the following alternative method for accounting for distance in weighted \\nlocal regression. Create a virtual set of training examples D' as follows: For each \\ntraining example (x, f (x)) in the original data set D, create some (possibly fractional) \\nnumber of copies of \\n(x, f (x)) in D', where the number of copies is K (d(x,, x)). Now \\ntrain a linear approximation to minimize the error criterion \\nThe idea here is to make more copies of training examples that are near the query \\ninstance, and fewer of those that are distant. Derive the gradient descent rule for \\nthis criterion. Express the rule in the form of a sum over members of D rather than \\nD', and compare it with the rules given by Equations (8.6) and (8.7). \\n8.3. Suggest a lazy version of the eager decision tree learning algorithm ID3 (see Chap- \\nter 3). What are the advantages and disadvantages of your lazy algorithm compared \\nto the original eager algorithm? \\nREFERENCES \\nAamodt, A., & Plazas, E. (1994). Case-based reasoning: Foundational issues, methodological varia- \\ntions, and system approaches. A1 Communications, 7(1), 39-52. \\nAha, D., & Kibler, D. (1989). Noise-tolerant instance-based learning algorithms. Proceedings of the \\nIJCAI-89 (794-799). \\nAha, D., Kibler, D., & Albert, M. (1991). Instance-based learning algorithms. Machine Learning, 6, \\n37-66. \\nAshley, K. D. (1990). Modeling legal argument: Reasoning with cases and hypotheticals. Cambridge, \\nMA: MIT Press. \\nAtkeson, C. G., Schaal, S. A., & Moore, A. W. (1997a). Locally weighted learning. AIReview, (to \\nappear). \\nAtkeson, \\nC. G., Moore, A. W., & Schaal, S. A. (1997b). Locally weighted learning for control. A1 \\nReview, (to appear). \\nBareiss, E. R., Porter, B., & Weir, C. C. (1988). PROTOS: An exemplar-based learning apprentice. \\nInternational Journal of Man-Machine Studies, 29, 549-561. \\nBentley, J. L. (1975). Multidimensional binary search trees used for associative searching. Cornmu- \\nnications of the ACM, 18(9), 509-517. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 259}, page_content='248 MACHINE LEARNING \\n1 \\nBishop, C. M. (1995). Neural networks for pattern recognition. Oxford, England: Oxford University \\nPress. \\nBisio, R., & Malabocchia, F. (1995). Cost estimation of software projects through case-based reason- \\ning. In M. Veloso and A. Aamodt (Eds.), Lecture Notes in Artificial Intelligence (pp. 11-22). \\nBerlin: Springer-Verlag. \\nBroomhead, D. S., & Lowe, D. (1988). Multivariable functional interpolation and adaptive networks. \\nComplex Systems, 2, 321-355. \\nCover, T., & Hart, P. (1967). Nearest neighbor pattern classification. IEEE Transactions on Infonna- \\ntion \\nTheory, 13,21-27. \\nDuda, \\nR., & Hart, P. (1973). Pattern classification and scene analysis. New York: John Wiley & \\nSons. \\nFranke, R. (1982). Scattered data interpolation: Tests of some methods. Mathematics of Computation, \\n38, 181-200. \\nFriedman, J., \\nBentley, \\nJ., & Finkel, R. (1977). An algorithm for finding best matches in logarithmic \\nexpected time. ACM Transactions on Mathematical Software, 3(3), 209-226. \\nGolding, A., & Rosenbloom, P. (1991). Improving rule-based systems through case-based reasoning. \\nProceedings of the Ninth National Conference on Artificial Intelligence (pp. 22-27). Cam- \\nbridge: \\nAAAI \\nPressme MIT Press. \\nHartman, \\nE. J., Keller, J. D., & Kowalski, J. M. (1990). Layered neural networks with Gaussian \\nhidden units as universal approximations. Neural Computation, 2(2), 210-215. \\nHaton, \\nJ.-P., Keane, M., & Manago, M. (Eds.). (1995). Advances in case-based reasoning: Second \\nEuropean workshop. Berlin: Springer-Verlag. \\nKolodner, J. L. (1993). Case-Based Reasoning. San Francisco: Morgan Kaufmann. \\nMoody, J. E., & Darken, C. J. (1989). Fast learning in networks of locally-tuned processing units. \\nNeural Computation, 1(2), 281-294. \\nMoore, A. W., & Lee, M. S. (1994). Efficient algorithms for minimizing cross validation error. Pro- \\nceedings of the \\n11th \\nInternational Conference on Machine Learning. San Francisco: Morgan \\nKaufmann. \\nPoggio, T., & Girosi, F. (1990). Networks for approximation and learning. Proceedings of the IEEE, \\n78(9), 1481-1497. \\nPowell, M. J. D. (1987). Radial basis functions for multivariable interpolation: A review. In Mason, \\nJ., & Cox, M. (Eds.). Algorithms for approximation (pp. 143-167). Oxford: Clarendon Press. \\nRiesbeck, C., & Schank, R. (1989). Inside case-based reasoning. Hillsdale, NJ: Lawrence Erlbaum. \\nSchank, R. (1982). Dynamic Memory. Cambridge, England: Cambridge University Press. \\nSchank, R., Riesbeck, C., & Kass, A. (1994). Inside case-based explanation. Hillsdale, NJ: Lawrence \\nErlbaum. \\nShepard, D. (1968). A two-dimensional interpolation function for irregularly spaced data. Proceedings \\nof the 23rd National Conference of the ACM (pp. 517-523). \\nStanfill, C., & Waltz, D. (1986). Toward memory-based reasoning. Communications of the ACM, \\n29(12), 1213-1228. \\nSycara, \\nK., \\nGuttal, \\nR., Koning, J., Narasimhan, S., & Navinchandra, D. (1992). CADET: A case- \\nbased synthesis tool for engineering design. International Journal of Expert Systems, 4(2), \\n157-188. \\nVeloso, M. M. (1992). Planning and learning by analogical reasoning. Berlin: Springer-Verlag. \\nVeloso, M. M., & Aamodt, A. (Eds.). (1995). Case-based reasoning research and development. \\nLectwe Notes in Artificial Intelligence. Berlin: Springer-Verlag. \\nWatson, I. (Ed.). (1995). Progress in case-based reasoning: First United Kingdom workshop. Berlin: \\nSpringer-Verlag. \\nWess, S., \\nAlthoff, \\nK., & Richter, M. (Eds.). (1994). Topics in case-based reasoning. Berlin: Springer- \\nVerlag. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 260}, page_content='CHAPTER \\nGENETIC \\nALGORITHMS \\nGenetic algorithms provide an approach to learning that is based loosely on simulated \\nevolution. Hypotheses are often described by bit strings whose interpretation depends \\non the application, though hypotheses may also be described by symbolic expressions \\nor even computer programs. The search for an appropriate hypothesis begins with a \\npopulation, or collection, of initial hypotheses. Members of the current population \\ngive rise to the next generation population by means of operations such as random \\nmutation and crossover, which are patterned after processes in biological evolution. \\nAt each step, the hypotheses in the current population are evaluated relative to \\na given measure of fitness, with the most fit hypotheses selected probabilistically \\nas seeds for producing the next generation. Genetic algorithms have been applied \\nsuccessfully to a variety of learning tasks and to other optimization problems. For \\nexample, they have been used to learn collections of rules for robot control and to \\noptimize the topology and learning parameters for artificial neural networks. This \\nchapter covers both genetic algorithms, in which hypotheses are typically described \\nby bit strings, and genetic programming, in which hypotheses are described by \\ncomputer programs. \\n9.1 MOTIVATION \\nGenetic algorithms (GAS) provide a learning method motivated by an analogy to \\nbiological evolution. Rather than search from general-to-specific hypotheses, or \\nfrom simple-to-complex, \\nGAS \\ngenerate successor hypotheses by repeatedly mutat- \\ning and recombining parts of the best currently known hypotheses. At each step, '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 261}, page_content='a collection of hypotheses called the current population is updated by replacing \\nsome fraction of the population by offspring of the most fit current hypotheses. \\nThe process forms a generate-and-test beam-search of hypotheses, in which vari- \\nants of the best current hypotheses are most likely to be considered next. The \\npopularity of \\nGAS \\nis motivated by a number of factors including: \\nEvolution is known to be a successful, robust method for adaptation within \\nbiological systems. \\nGAS \\ncan search spaces of hypotheses containing complex interacting parts, \\nwhere the impact of each part on overall hypothesis fitness may be difficult \\nto model. \\n0 Genetic algorithms are easily parallelized and can take advantage of the \\ndecreasing costs of \\npowerful \\ncomputer hardware. \\nThis chapter describes the genetic algorithm approach, illustrates its use, and \\nexamines the nature of its hypothesis space search. We also describe a variant \\ncalled genetic programming, in which entire computer programs are evolved to \\ncertain fitness criteria. Genetic algorithms and genetic programming are two of \\nthe more popular approaches in a field that is sometimes called evolutionary \\ncomputation. In the final section we touch on selected topics in the study of \\nbiological evolution, including the \\nBaldwin \\neffect, which describes an interesting \\ninterplay between the learning capabilities of single individuals and the rate of \\nevolution of the entire population. \\n9.2 GENETIC ALGORITHMS - \\nThe problem addressed by GAS is to search a space of candidate hypotheses to \\nidentify the best hypothesis. In \\nGAS \\nthe \"best hypothesis\" is defined as the one \\nthat optimizes a predefined numerical measure for the problem at hand, called the \\nhypothesis Jitness. For example, if the learning task is the problem of approxi- \\nmating an unknown function given training examples of its input and output, then \\nfitness could be defined as the accuracy of the hypothesis over this training data. \\nIf the task is to learn a strategy for playing chess, fitness could be defined as the \\nnumber of games won by the individual when playing against other individuals \\nin the current population. \\nAlthough different implementations of genetic algorithms vary in their de- \\ntails, they typically share the following structure: The algorithm operates by itera- \\ntively updating a pool of hypotheses, called the population. On each iteration, all \\nmembers of the population are evaluated according to the fitness function. A new \\npopulation is then generated by probabilistically selecting the most fit individuals \\nfrom the current population. Some of these selected individuals are carried forward \\ninto the next generation population intact. Others are used as the basis for creating \\nnew offspring individuals by applying genetic operations such as crossover and \\nmutation. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 262}, page_content=\"Fitness: A function that assigns an evaluation score, given a hypothesis. \\nFitnessdhreshold: A threshold specifying the termination criterion. \\np: The number of hypotheses to be included in the population. \\nr: \\nThe fraction of the population to be replaced by Crossover at each step. \\nm: The mutation rate. \\nInitialize population: P c Generate p hypotheses at random \\nEvaluate: For each h in P, compute Fitness(h)' \\nWhile [max Fitness(h)] < Fitnessdhreshold do \\nh \\nCreate a new generation, Ps: \\n1. Select: F'robabilistically select (1 - r)p members of P to add to Ps. The probability Pr(hi) of \\nselecting hypothesis hi from P is given by \\n2. Crossover: Probabilistically select pairs of hypotheses from P, according to &(hi) given \\nabove. For each pair, (hl, h2), produce two offspring by applying the Crossover operator. \\nAdd all offspring to P,. \\n3. Mutate: Choose m percent of the members of P, with uniform probability. For each, invert \\none randomly selected bit in its representation. \\n4. Update: P t P,. \\n5. Evaluate: for each h in P, compute Fitness(h) \\nReturn the hypothesis from P that has the highest fitness. \\nTABLE 9.1 \\nA prototypical genetic algorithm. A population containing p hypotheses is maintained. On each itera- \\ntion, the successor population Ps is formed by probabilistically selecting current hypotheses according \\nto their fitness and by adding new hypotheses. New hypotheses are created by applying a crossover \\noperator to pairs of most fit hypotheses and by creating single point mutations in the resulting gener- \\nation of hypotheses. This process is iterated until sufficiently fit hypotheses are discovered. Typical \\ncrossover and mutation operators are defined in a subsequent table. \\nA prototypical genetic algorithm is described in Table 9.1. The inputs to \\nthis algorithm include the fitness function for ranking candidate hypotheses, a \\nthreshold defining an acceptable level of fitness for terminating the algorithm, \\nthe size of the population to be maintained, and parameters that determine how \\nsuccessor populations are to be generated: the fraction of the population to be \\nreplaced at each generation and the mutation rate. \\nNotice in this algorithm each iteration through the main loop produces a new \\ngeneration of hypotheses based on the current population. First, a certain number \\nof hypotheses from the current population are selected for inclusion in the next \\ngeneration. These are selected probabilistically, where the probability of selecting \\nhypothesis hi is given by \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 263}, page_content=\"Thus, the probability that a hypothesis will be selected is proportional to its \\nown fitness and is inversely proportional to the fitness of the other competing \\nhypotheses in the current population. \\nOnce these members of the current generation have been selected for inclu- \\nsion in the next generation population, additional members are generated using a \\ncrossover operation. Crossover, defined in detail in the next section, takes two par- \\nent hypotheses from the current generation and creates two offspring hypotheses \\nby recombining portions of both parents. The parent hypotheses are chosen \\nproba- \\nbilistically from the current population, again using the probability function given \\nby Equation (9.1). After new members have been created by this crossover opera- \\ntion, the new generation population now contains the desired number of members. \\nAt this point, a certain fraction m of these members are chosen at random, and \\nrandom mutations all performed to alter these members. \\nThis GA algorithm thus performs a randomized, parallel beam search for \\nhypotheses that perform well according to the fitness function. In the follow- \\ning subsections, we describe in more detail the representation of hypotheses and \\ngenetic operators used in this algorithm. \\n9.2.1 Representing Hypotheses \\nHypotheses in GAS are often represented by bit strings, so that they can be easily \\nmanipulated by genetic operators such as mutation and crossover. The hypotheses \\nrepresented by these bit strings can be quite complex. For example, sets of if-then \\nrules can easily be represented in this way, by choosing an encoding of rules \\nthat allocates specific substrings for each rule precondition and postcondition. \\nExamples of such rule representations in GA systems are described by Holland \\n(1986); Grefenstette (1988); and \\nDeJong \\net al. (1993). \\nTo see how if-then rules can be encoded by bit strings, \\n.first \\nconsider how we \\nmight use a bit string to describe a constraint on the value of a single attribute. To \\npick an example, consider the attribute Outlook, which can take on any of the three \\nvalues Sunny, Overcast, or Rain. One obvious way to represent a constraint on \\nOutlook is to use a bit string of length three, in which each bit position corresponds \\nto one of its three possible values. Placing a 1 in some position indicates that the \\nattribute is allowed to take on the corresponding value. For example, the string 010 \\nrepresents the constraint that Outlook must take on the second of these values, , \\nor Outlook = Overcast. Similarly, the string 011 represents the more general \\nconstraint that allows two possible values, or (Outlook = Overcast v Rain). \\nNote 11 1 represents the most general possible constraint, indicating that we don't \\ncare which of its possible values the attribute takes on. \\nGiven this method for representing constraints on a single attribute, con- \\njunctions of constraints on multiple attributes can easily be represented by con- \\ncatenating the corresponding bit strings. For example, consider a second attribute, \\nWind, that can take on the value Strong or Weak. A rule precondition such as \\n(Outlook = Overcast V Rain) A (Wind = Strong) \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 264}, page_content='can then be represented by the following bit string of length five: \\nOutlook Wind \\n01 1 10 \\nRule postconditions (such as PlayTennis = yes) can be represented in a \\nsimilar fashion. Thus, an entire rule can be described by concatenating the bit \\nstrings describing the rule preconditions, together with the \\nbit \\nstring describing \\nthe rule postcondition. For example, the rule \\nIF Wind = Strong THEN PlayTennis = yes \\nwould be represented by the string \\nOutlook Wind PlayTennis \\n111 10 10 \\nwhere the first three bits describe the \"don\\'t care\" constraint on Outlook, the next \\ntwo bits describe the constraint on Wind, and the final two bits describe the rule \\npostcondition (here we assume PlayTennis can take on the values Yes or No). \\nNote the bit string representing the rule contains a substring for each attribute \\nin the hypothesis space, even if that attribute is not constrained by the rule pre- \\nconditions. This yields a fixed length bit-string representation for rules, in which \\nsubstrings at specific locations describe constraints on specific attributes. Given \\nthis representation for single rules, we can represent sets of rules by similarly \\nconcatenating the bit string representations of the individual rules. \\nIn designing a bit string encoding for some hypothesis space, it is useful to \\narrange for every syntactically legal bit string to represent a well-defined hypoth- \\nesis. To illustrate, note in the rule encoding in the above paragraph the bit string \\n11 1 10 11 represents a rule whose postcondition does not constrain the target \\nattribute PlayTennis. If we wish to avoid considering this hypothesis, we may \\nemploy a different encoding \\n(e.g., \\nallocate just one bit to the PlayTennis post- \\ncondition to indicate whether the value is Yes or No), alter the genetic operators \\nso that they explicitly avoid constructing such bit strings, or simply assign a very \\nlow fitness to such bit strings. \\nIn some \\nGAS, \\nhypotheses are represented by symbolic descriptions rather \\nthan bit strings. For example, in Section 9.5 we discuss a genetic algorithm that \\nencodes hypotheses as computer programs. \\n9.2.2 Genetic Operators \\nThe generation of successors in a GA is determined by a set of operators that \\nrecombine and mutate selected members of the current population. Typical GA \\noperators for manipulating bit string hypotheses are illustrated in Table 9.1. These \\noperators correspond to idealized versions of the genetic operations found in bi- \\nological evolution. The two most common operators are crossover and mutation. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 265}, page_content='The crossover operator produces two new offspring from two parent strings, \\nby copying selected bits from each parent. The bit at position i in each offspring \\nis copied from the bit at position i in one of the two parents. The choice of which \\nparent contributes the bit for position i is determined by an additional string called \\nthe crossover mask. To illustrate, consider the single-point crossover operator at \\nthe top of Table 9.2. Consider the topmost of the two offspring in this case. This \\noffspring takes its first five bits from the first parent and its remaining six bits \\nfrom the second parent, because the crossover mask 11 11 \\n1000000 \\nspecifies these \\nchoices for each of the bit positions. The second offspring uses the same crossover \\nmask, but switches the roles of the two parents. Therefore, it contains the bits that \\nwere not used by the first offspring. In single-point crossover, the crossover mask \\nis always constructed so that it begins with a string containing n contiguous Is, \\nfollowed by the necessary number of \\n0s \\nto complete the string. This results in \\noffspring in which the first n bits are contributed by one parent and the remaining \\nbits by the second parent. Each time the single-point crossover operator is applied, \\nInitial strings Crossover Mask Offspring \\nSingle-point crossover: \\nTwo-point crossover: \\nUniform \\ncrossover: \\nPoint mutation: lllOloo_1000 111010~1000 \\nTABLE 9.2 \\nCommon operators for genetic algorithms. These operators form offspring of hypotheses represented \\nby bit strings. The crossover operators create two descendants from two parents, using the crossover \\nmask to determine which parent contributes which bits. Mutation creates a single descendant from a \\nsingle parent by changing the \\nvalue \\nof a randomly chosen bit. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 266}, page_content='the crossover point n is chosen at random, and the crossover mask is then created \\nand applied. \\nIn two-point crossover, offspring are created by substituting intermediate \\nsegments of one parent into the middle of the second parent string. Put another \\nway, the crossover mask is a string beginning with no zeros, followed by a con- \\ntiguous string of nl ones, followed by the necessary number of zeros to complete \\nthe string. Each time the two-point crossover operator is applied, a mask is gen- \\nerated by randomly choosing the integers no and nl. For instance, in the example \\nshown in Table 9.2 the offspring are created using a mask for which no = 2 and \\nn 1 = 5. Again, the two offspring are created by switching the roles played by the \\ntwo parents. \\nUniform crossover combines bits sampled uniformly from the two parents, \\nas illustrated in Table 9.2. In this case the crossover mask is generated as a random \\nbit string with each bit chosen at random and independent of the others. \\nIn addition to recombination operators that produce offspring by combining \\nparts of two parents, a second type of operator produces offspring from a single \\nparent. In particular, the mutation operator produces small random changes to the \\nbit string by choosing a single bit at random, then changing its value. Mutation is \\noften performed after crossover has been applied as in our prototypical algorithm \\nfrom Table 9.1. \\nSome GA systems employ additional operators, especially operators that are \\nspecialized to the particular hypothesis representation used by the system. For \\nexample, Grefenstette et al. (1991) describe a system that learns sets of rules \\nfor robot control. It uses mutation and crossover, together with an operator for \\nspecializing rules. Janikow (1993) describes a system that learns sets of rules \\nusing operators that generalize and specialize rules in a variety of directed ways \\n(e.g., \\nby explicitly replacing the condition on an attribute by \"don\\'t care\"). \\n9.2.3 Fitness Function and Selection \\nThe fitness function defines the criterion for ranking potential hypotheses and for \\nprobabilistically selecting them for inclusion in the next generation population. If \\nthe task is to learn classification rules, then the fitness function typically has a \\ncomponent that scores the classification accuracy of the rule over a set of provided \\ntraining examples. Often other criteria may be included as well, such as the com- \\nplexity or generality of the rule. More generally, when the bit-string hypothesis is \\ninterpreted as a complex procedure (e.g., when the bit string represents a collec- \\ntion of if-then rules that will be chained together to control a robotic device), the \\nfitness function may measure the overall performance of the resulting procedure \\nrather than performance of individual rules. \\nIn our prototypical GA shown in Table 9.1, the probability that a hypothesis \\nwill be selected is given by the ratio of its fitness to the fitness of other members \\nof the current population as seen in Equation (9.1). This method is sometimes \\ncalled jitness proportionate selection, or roulette wheel selection. Other methods \\nfor using fitness to select hypotheses have also been proposed. For example, in '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 267}, page_content=\"256 MACHINE LEARNING \\n1 \\ntournament selection, two hypotheses are first chosen at random from the current \\npopulation. With some predefined probability p the more fit of these two is then \\nselected, and with probability (1 - p) the less fit hypothesis is selected. Tourna- \\nment selection often yields a more diverse population than fitness proportionate \\nselection (Goldberg and Deb 1991). In another method called rank selection, the \\nhypotheses in the current population are first sorted by fitness. The probability \\nthat a hypothesis will be selected is then proportional to its rank in this sorted \\nlist, rather than its fitness. \\n9.3 AN ILLUSTRATIVE EXAMPLE \\nA genetic algorithm can be viewed as a general optimization method that searches \\na large space of candidate objects seeking one that performs best according to the \\nfitness function. Although not guaranteed to find an optimal object, \\nGAS \\noften \\nsucceed in finding an object with high fitness. \\nGAS \\nhave been applied to a number \\nof optimization problems outside machine learning, including problems such as \\ncircuit layout and job-shop scheduling. Within machine learning, they have been \\napplied both to function-approximation problems and to tasks such as choosing \\nthe network topology for artificial neural network learning systems. \\nTo illustrate the use of \\nGAS \\nfor concept learning, we briefly summarize \\nthe GABIL system described by DeJong et al. (1993). GABIL uses a GA to \\nlearn boolean concepts represented by a disjunctive set of propositional rules. \\nIn experiments over several concept learning problems, GABIL was found to be \\nroughly comparable in generalization accuracy to other learning algorithms such \\nas the decision tree learning algorithm C4.5 and the rule learning system AQ14. \\nThe learning tasks in this study included both artificial learning tasks designed to \\nexplore the systems' generalization accuracy and the real world problem of breast \\ncancer diagnosis. \\nThe algorithm used by GABIL is exactly the algorithm described in Ta- \\nble 9.1. In experiments reported by DeJong et al. \\n(1993), \\nthe parameter r, which \\ndetermines the fraction of the parent population replaced by crossover, was set \\nto 0.6. The parameter m, which determines the mutation rate, was set to 0.001. \\nThese are typical settings for these parameters. The population size p was varied \\nfrom 100 to 1000, depending on the specific learning task. \\nThe specific instantiation of the GA algorithm in GABIL can be summarized \\nas follows: \\n0 Representation. Each hypothesis in GABIL corresponds to a disjunctive set \\nof propositional rules, encoded as described in Section 9.2.1. In particular, \\nthe hypothesis space of rule preconditions consists of a conjunction of con- \\nstraints on a fixed set of attributes, as described in that earlier section. To \\nrepresent a set of rules, the bit-string representations of individual rules are \\nconcatenated. To illustrate, consider a hypothesis space in which rule precon- \\nditions are conjunctions of constraints over two boolean attributes, a1 and a2. \\nThe rule postcondition is described by a single bit that indicates the predicted \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 268}, page_content='value of the target attribute c. Thus, the hypothesis consisting of the two rules \\nIFal=Tr\\\\az=F \\nTHEN \\nc=T; IF a2=T THEN c=F \\nwould be represented by the string \\nNote the length of the bit string grows with the number of rules in the hy- \\npothesis. This variable bit-string length requires a slight modification to the \\ncrossover operator, as described below. \\na Genetic operators. GABIL uses the standard mutation operator of Table 9.2, \\nin which a single bit is chosen at random and replaced by its complement. \\nThe crossover operator that it uses is a fairly standard extension to the \\ntwo-point crossover operator described in Table 9.2. In particular, to accom- \\nmodate the variable-length bit strings that encode rule sets, and to constrain \\nthe system so that crossover occurs only between like sections of the bit \\nstrings that encode rules, the following approach is taken. To perform a \\ncrossover operation on two parents, two crossover points are first chosen \\nat random in the first parent string. Let \\ndl \\n(dz) \\ndenote the distance from \\nthe \\nleftmost \\n(rightmost) of these two crossover points to the rule boundary \\nimmediately to its left. The crossover points in the second parent are now \\nrandomly chosen, subject to the constraint that they must have the same \\ndl \\nand d2 value. For example, if the two parent strings are \\nand \\nand the crossover points chosen for the first parent are the points following \\nbit positions 1 and 8, \\nwhere \"[\" and \"1\" indicate crossover points, then dl = 1 and dz = 3. Hence \\nthe allowed pairs of crossover points for the second parent include the pairs \\nof bit positions \\n(1,3), \\n(1,8), \\nand (6,8). If the pair (1,3) happens to be \\nchosen, '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 269}, page_content='then the two resulting offspring will be \\nand \\nAs this example illustrates, this crossover operation enables offspring to \\ncontain a different number of rules than their parents, while assuring that all \\nbit strings generated in this fashion represent well-defined rule sets. \\nFitness function. The fitness of each hypothesized rule set is based on its \\nclassification accuracy over the training data. In particular, the function used \\nto measure fitness is \\nwhere correct (h) is the percent of all training examples correctly classified \\nby hypothesis h. \\nIn experiments comparing the behavior of GABIL to decision tree learning \\nalgorithms such as C4.5 and \\nID5R, \\nand to the rule learning algorithm \\nAQ14, \\nDeJong et al. (1993) report roughly comparable performance among these systems, \\ntested on a variety of learning problems. For example, over a set of 12 synthetic \\nproblems, GABIL achieved an average generalization accuracy of 92.1 %, whereas \\nthe performance of the other systems ranged from 91.2 % to 96.6 %. \\n9.3.1 Extensions \\nDeJong et al. (1993) also explore two interesting extensions to the basic design \\nof GABIL. In one set of experiments they explored the addition of two new ge- \\nnetic operators that were motivated by the generalization operators common in \\nmany symbolic learning methods. The first of these operators, AddAlternative, \\ngeneralizes the constraint on a specific attribute by changing a 0 to a 1 in the \\nsubstring corresponding to the attribute. For example, if the constraint on an at- \\ntribute is represented by the string 10010, this operator might change it to 101 10. \\nThis operator was applied with probability .O1 to selected members of the popu- \\nlation on each generation. The second operator, Dropcondition performs a more \\ndrastic generalization step, by replacing all bits for a particular attribute by a 1. \\nThis operator corresponds to generalizing the rule by completely dropping the \\nconstraint on the attribute, and was applied on each generation with probability \\n.60. \\nThe authors report this revised system achieved an average performance of \\n95.2% over the above set of synthetic learning tasks, compared to 92.1% for the \\nbasic GA algorithm. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 270}, page_content='In the above experiment, the two new operators were applied with the same \\nprobability to each hypothesis in the population on each generation. In a second \\nexperiment, the bit-string representation for hypotheses was extended to include \\ntwo bits that determine which of these operators may be applied to the hypothesis. \\nIn this extended representation, the bit string for a typical rule set hypothesis \\nwould be \\nwhere the final two bits indicate in this case that the AddAlternative operator may \\nbe applied to this bit string, but that the Dropcondition operator may not. These \\ntwo new bits define part of the search strategy used by the GA and are themselves \\naltered and evolved using the same crossover and mutation operators that operate \\non other bits in the string. While the authors report mixed results with this approach \\n(i.e., \\nimproved performance on some problems, decreased performance on others), \\nit provides an interesting illustration of how \\nGAS \\nmight in principle be used to \\nevolve their own hypothesis search methods. \\n9.4 HYPOTHESIS SPACE SEARCH \\nAs illustrated above, GAS employ a randomized beam search method to seek a \\nmaximally fit hypothesis. This search is quite different from that of other learning \\nmethods we have considered in this book. To contrast the hypothesis space search \\nof \\nGAS \\nwith that of neural network BACKPROPAGATION, for example, the gradient \\ndescent search in \\nBACKPROPAGATION \\nmoves smoothly from one hypothesis to a \\nnew hypothesis that is very similar. In contrast, the GA search can move much \\nmore abruptly, replacing a parent hypothesis by an offspring that may be radically \\ndifferent from the parent. Note the GA search is therefore less likely to fall into \\nthe same kind of local minima that can plague gradient descent methods. \\nOne \\npractical difficulty in some GA applications is the problem of crowding. \\nCrowding is a phenomenon in which some individual that is more highly fit than \\nothers in the population quickly reproduces, so that copies of this individual and \\n1 very similar individuals take over a large fraction of the population. The negative \\nimpact of crowding is that it reduces the diversity of the population, thereby slow- \\ning further progress by the GA. Several strategies have been explored for reducing \\ncrowding. One approach is to alter the selection function, using criteria such as \\ntournament selection or rank selection in place of fitness proportionate roulette \\nwheel selection. A related strategy is \"fitness sharing,\" in which the measured \\nfitness of an individual is reduced by the presence of other, similar individuals \\nin \\nthe population. A third approach is to restrict the kinds of individuals allowed \\nto \\nrecombine to form offspring. For example, by allowing only the most similar \\nindividuals to recombine, we can encourage the formation of clusters of similar \\nindividuals, or multiple \"subspecies\" within the population. A related approach is \\nto spatially distribute individuals and allow only nearby individuals to recombine. \\nMany of these techniques are inspired by the analogy to biological evolution. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 271}, page_content='9.4.1 Population Evolution and the Schema Theorem \\nIt is interesting to ask whether one can mathematically characterize the evolution \\nover time of the population within a GA. The schema theorem of Holland (1975) \\nprovides one such characterization. It is based on the concept of schemas, or pat- \\nterns that describe sets of bit strings. To be precise, a schema is any string com- \\nposed of Os, \\nIs, \\nand \\n*\\'s. \\nEach schema represents the set of bit strings containing the \\nindicated \\n0s \\nand \\nIs, \\nwith each \"*\" interpreted as a \"don\\'t care.\" For example, the \\nschema \\n0*10 \\nrepresents the set of bit strings that includes exactly \\n0010 \\nand 01 10. \\nAn individual bit string can be viewed as a representative of each of the \\ndifferent schemas that it matches. For example, the bit string \\n0010 \\ncan be thought \\nof as a representative of 24 distinct schemas including 00**, O* 10, ****, etc. Sim- \\nilarly, a population of bit strings can be viewed in terms of the set of schemas that \\nit represents and the number of individuals associated with each of these schema. \\nThe schema theorem characterizes the evolution of the population within a \\nGA in terms of the number of instances representing each schema. Let m(s, t) \\ndenote the number of instances of schema s in the population at time t (i.e., \\nduring the tth generation). The schema theorem describes the expected value of \\nm(s, t + 1) in terms of m(s, t) and other properties of the schema, population, and \\nGA algorithm parameters. \\nThe evolution of the population in the GA depends on the selection step, \\nthe recombination step, and the mutation step. Let us start by considering just the \\neffect of the selection step. Let f (h) denote the fitness of the individual bit string \\nh and f(t) denote the average fitness of all individuals in the population at time t. \\nLet n be the total number of individuals in the population. Let h E s n p, indicate \\nthat the individual h is both a representative of schema s and a member of the \\npopulation at time t. Finally, let 2(s, t) denote the average fitness of instances of \\nschema s in the population at time t. \\nWe are interested in calculating the expected value of m(s, t + l), which \\nwe denote E[m(s, t + I)]. We can calculate E[m (s, t + I)] using the probability \\ndistribution for selection given in Equation (9. \\nI), \\nwhich can be restated using our \\ncurrent terminology as follows: \\nNow if we select one member for the new population according to this probability \\ndistribution, then the probability that we will select a representative of schema s is '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 272}, page_content='The second step above follows from the fact that by definition, \\nEquation (9.2) gives the probability that a single hypothesis selected by the GA \\nwill be an instance of schema s. Therefore, the expected number of instances \\nof s resulting from the n independent selection steps that create the entire new \\ngeneration is just \\nn \\ntimes this probability. \\nEquation (9.3) states that the expected number of instances of schema s at gener- \\nation t + 1 is proportional to the average fitness i(s, t) of instances of this schema \\nat time t, and inversely proportional to the average fitness f(t) of all members \\nof the population at time t. Thus, we can expect schemas with above average fit- \\nness to be represented with increasing frequency on successive generations. If we \\nview the GA as performing a virtual parallel search through the space of possible \\nschemas at the same time it performs its explicit parallel search through the space \\nof individuals, then Equation (9.3) indicates that more fit schemas will grow in \\ninfluence over time. \\nWhile the above analysis considered only the selection step of the GA, the \\ncrossover and mutation steps must be considered as well. The schema theorem con- \\nsiders only the possible negative influence of these genetic operators \\n(e.g., \\nrandom \\nmutation may decrease the number of representatives of s, independent of O(s, t)), \\nand considers only the case of single-point crossover. The full schema theorem \\nthus provides a lower bound on the expected frequency of schema s, as follows: \\nHere, p, is the probability that the single-point crossover operator will be applied \\nto an arbitrary individual, and p, is the probability that an arbitrary bit of an \\narbitrary individual will be mutated by the mutation operator. o(s) is the number \\nI of defined bits in schema s, where 0 and 1 are defined bits, but * is not. d(s) is \\nthe distance between the \\nleftmost \\nand rightmost defined bits in s. Finally, 1 is the \\nlength of the individual bit strings in the population. Notice the \\nleftmost \\nterm in \\nEquation (9.4) is identical to the term from Equation (9.3) and describes the ef- \\nfect of the selection step. The middle term describes the effect of the single-point \\ncrossover operator-in particular, it describes the probability that an arbitrary in- \\ndividual representing s will still represent s following application of this crossover \\noperator. The rightmost term describes the probability that an arbitrary individual \\nrepresenting schema s will still represent schema s following application of the \\nmutation operator. Note that the effects of single-point crossover and mutation \\nincrease with the number of defined bits o(s) in the schema and with the distance \\nd(s) between the defined bits. Thus, the schema theorem can be roughly interpreted \\nas stating that more fit schemas will tend to grow in influence, especially schemas '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 273}, page_content=\"containing a small number of defined bits (i.e., containing a large number of *'s), \\nand especially when these defined bits are near one another within the bit string. \\nThe schema theorem is perhaps the most widely cited characterization of \\npopulation evolution within a GA. One way in which it is incomplete is that it fails \\nto consider the (presumably) positive effects of crossover and mutation. Numerous \\nmore recent theoretical analyses have been proposed, including analyses based on \\nMarkov chain models and on statistical mechanics models. See, for example, \\nWhitley \\nand Vose (1995) and Mitchell (1996). \\n9.5 GENETIC PROGRAMMING \\nGenetic programming (GP) is a form of evolutionary computation in which the in- \\ndividuals in the evolving population are computer programs rather than bit strings. \\nKoza (1992) describes the basic genetic programming approach and presents a \\nbroad range of simple programs that can be successfully learned by GP. \\n9.5.1 Representing Programs \\nPrograms manipulated by a GP are typically represented by trees correspond- \\ning to the parse tree of the program. Each function call is represented by a \\nnode in the tree, and the arguments to the function are given by its descendant \\nnodes. For example, Figure 9.1 illustrates this tree representation for the function \\nsin(x) + J-. To apply genetic programming to a particular domain, the user \\nmust define the primitive functions to be considered \\n(e.g., \\nsin, cos, J, +, -, ex- \\nponential~), as well as the terminals \\n(e.g., \\nx, y, constants such as 2). The genetic \\nprogramming algorithm then uses an evolutionary search to explore the vast space \\nof programs that can be described using these primitives. \\nAs in a genetic algorithm, the prototypical genetic programming algorithm \\nmaintains a population of individuals (in this case, program trees). On each it- \\neration, it produces a new generation of individuals using selection, crossover, \\nand mutation. The fitness of a given individual program in the population is typ- \\nically determined by executing the program on a set of training data. Crossover \\noperations are performed by replacing a randomly chosen \\nsubtree \\nof one parent \\nFIGURE 9.1 \\nProgram tree representation in genetic programming. \\nArbitrary programs are represented by their parse trees. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 274}, page_content='FIGURE 9.2 \\nCrossover operation applied to two parent program trees (top). Crossover points (nodes shown in \\nbold at top) are chosen at random. The \\nsubtrees \\nrooted at these crossover points are then exchanged \\nto create children trees (bottom). \\nprogram by a subtree from the other parent program. Figure 9.2 illustrates a typical \\ncrossover operation. \\nKoza (1992) describes a set of experiments applying a GP to a number of \\napplications. In his experiments, 10% of the current population, selected prob- \\nabilistically according to fitness, is retained unchanged in the next generation. \\nThe remainder of the new generation is created by applying crossover to pairs \\nof programs from the current generation, again selected probabilistically accord- \\ning to their fitness. The mutation operator was not used in this particular set of \\nexperiments. \\n9.5.2 Illustrative Example \\nOne illustrative example presented by Koza (1992) involves learning an algorithm \\nfor stacking the blocks shown in Figure 9.3. The task is to develop a general algo- \\nrithm for stacking the blocks into a single stack that spells the word \"universal,\" '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 275}, page_content='FIGURE 9.3 \\nA block-stacking problem. The task for GP is to discover a program that can transform an arbitrary \\ninitial configuration of blocks into a stack that spells the word \"universal.\" A set of 166 such initial \\nconfigurations was provided to evaluate fitness of candidate programs (after Koza 1992). \\nindependent of the initial configuration of blocks in the world. The actions avail- \\nable for manipulating blocks allow moving only a single block at a time. In \\nparticular, the top block on the stack can be moved to the table surface, or a \\nblock on the table surface can be moved to the top of the stack. \\nAs in most GP applications, the choice of problem representation has a \\nsignificant impact on the ease of solving the problem. In Koza\\'s formulation, the \\nprimitive functions used to compose programs for this task include the following \\nthree terminal arguments: \\n0 CS (current stack), which refers to the name of the top block on the stack, \\nor F if there is no current stack. \\nTB (top correct block), which refers to the name of the topmost block on \\nthe stack, such that it and those blocks beneath it are in the correct order. \\n0 NN (next necessary), which refers to the name of the next block needed \\nabove TB in the stack, in order to spell the word \"universal,\" or F if no \\nmore blocks are needed. \\nAs can be seen, this particular choice of terminal arguments provides a natu- \\nral representation for describing programs for manipulating blocks for this task. \\nImagine, in contrast, the relative difficulty of the task if we were to instead define \\nthe terminal arguments to be the x and y coordinates of each block. \\nIn addition to these terminal arguments, the program language in this appli- \\ncation included the following primitive functions: \\n(MS x) (move to stack), if block x is on the table, this operator moves x to \\nthe top of the stack and returns the value T. Otherwise, it does nothing and \\nreturns the value F. \\n0 (MT x) (move to table), if block x is somewhere in the stack, this moves the \\nblock at the top of the stack to the table and returns the value T. Otherwise, \\nit returns the value F. \\n0 (EQ x y) (equal), which returns T if x equals y, and returns F otherwise. \\n0 (NOT x), which returns T if x = F, and returns F if x = T. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 276}, page_content='0 (DU x y) (do until), which executes the expression x repeatedly until ex- \\npression y returns the value T. \\nTo allow the system to evaluate the fitness of any given program, Koza \\nprovided a set of 166 training example problems representing a broad variety of \\ninitial block configurations, including problems of differing degrees of difficulty. \\nThe fitness of any given program was taken to be the number of these examples \\nsolved by the algorithm. The population was initialized to a set of 300 random \\nprograms. After 10 generations, the system discovered the following program, \\nwhich solves all 166 problems. \\n(EQ (DU (MT \\nCS)(NOT \\nCS)) (DU (MS \\nNN)(NOT \\nNN)) ) \\nNotice this program contains a sequence of two DU, or \"Do Until\" state- \\nments. The first repeatedly moves the current top of the stack onto the table, until \\nthe stack becomes empty. The second \"Do Until\" statement then repeatedly moves \\nthe next necessary block from the table onto the stack. The role played by the \\ntop level EQ expression here is to provide a syntactically legal way to sequence \\nthese two \"Do Until\" loops. \\nSomewhat surprisingly, after only a few generations, this GP was able to \\ndiscover a program that solves all 166 training problems. Of course the ability \\nof the system to accomplish this depends strongly on the primitive arguments \\nand functions provided, and on the set of training example cases used to evaluate \\nfitness. \\n9.5.3 Remarks on Genetic Programming \\nAs illustrated in the above example, genetic programming extends genetic algo- \\nrithms to the evolution of complete computer programs. Despite the huge size of \\nthe hypothesis space it must search, genetic programming has been demonstrated \\nto produce intriguing results in a number of applications. A comparison of GP \\nto other methods for searching through the space of computer programs, such as \\nhillclimbing and simulated annealing, is given by \\nO\\'Reilly \\nand Oppacher (1994). \\nWhile the above example of GP search is fairly simple, Koza et al. (1996) \\nsummarize the use of a GP in several more complex tasks such as designing \\nelectronic filter circuits and classifying segments of protein molecules. The fil- \\nter circuit design problem provides an example of a considerably more complex \\nproblem. Here, programs are evolved that transform a simple fixed seed circuit \\ninto a final circuit design. The primitive functions used by the GP to construct its \\nprograms are functions that edit the seed circuit by inserting or deleting circuit \\ncomponents and wiring connections. The fitness of each program is calculated \\nby \\nsimulating the circuit it outputs (using the SPICE circuit simulator) to de- \\ntermine how closely this circuit meets the design specifications for the desired \\nfilter. More precisely, the fitness score is the sum of the magnitudes of errors \\nbetween the desired and actual circuit output at 101 different input frequen- \\ncies. In this case, a population of size 640,000 was maintained, with selection '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 277}, page_content='producing 10% of the successor population, crossover producing 89%, and mu- \\ntation producing 1%. The system was executed on a 64-node parallel proces- \\nsor. Within the first randomly generated population, the circuits produced were \\nso unreasonable that the SPICE simulator could not even simulate the behav- \\nior of 98% of the circuits. The percentage of unsimulatable circuits dropped to \\n84.9% following the first generation, to 75.0% following the second generation, \\nand to an average of 9.6% over succeeding generations. The fitness score of the \\nbest circuit in the initial population was 159, compared to a score of 39 after \\n20 generations and a score of 0.8 after 137 generations. The best circuit, pro- \\nduced after 137 generations, exhibited performance very similar to the desired \\nbehavior. \\nIn most cases, the \\nperformance \\nof genetic programming depends crucially \\non the choice of representation and on the choice of fitness function. For this \\nreason, an active area of current research is aimed at the automatic discovery \\nand incorporation of subroutines that improve on the original set of primitive \\nfunctions, thereby allowing the system to dynamically alter the primitives from \\nwhich it constructs individuals. See, for example, Koza (1994). \\n9.6 MODELS OF EVOLUTION AND LEARNING \\nIn many natural systems, individual organisms learn to adapt significantly during \\ntheir lifetime. At the same time, biological and social processes allow their species \\nto adapt over a time frame of many generations. One interesting question regarding \\nevolutionary systems is \"What is the relationship between learning during the \\nlifetime of a single individual, and the longer time frame species-level learning \\nafforded by evolution?\\' \\n9.6.1 Lamarckian Evolution \\nLarnarck was a scientist who, in the late nineteenth century, proposed that evo- \\nlution over many generations was directly influenced by the experiences of indi- \\nvidual organisms during their lifetime. In particular, he proposed that experiences \\nof a single organism directly affected the genetic makeup of their offspring: If \\nan individual learned during its lifetime to avoid some toxic food, it could pass \\nthis trait on genetically to its offspring, which therefore would not need to learn \\nthe trait. This is an attractive conjecture, because it would presumably allow for \\nmore efficient evolutionary progress than a generate-and-test process (like that of \\nGAS \\nand \\nGPs) \\nthat ignores the experience gained during an individual\\'s lifetime. \\nDespite the attractiveness of this theory, current scientific evidence overwhelm- \\ningly contradicts \\nLamarck\\'s \\nmodel. The currently accepted view is that the genetic \\nmakeup of an individual is, in fact, unaffected by the lifetime experience of one\\'s \\nbiological parents. Despite this apparent biological fact, recent computer studies \\nhave shown that Lamarckian processes can sometimes improve the effectiveness \\nof computerized genetic algorithms (see Grefenstette 1991; \\nAckley \\nand \\nLittman \\n1994; and Hart and Belew 1995). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 278}, page_content='9.6.2 Baldwin Effect \\nAlthough Lamarckian evolution is not an accepted model of biological evolution, \\nother mechanisms have been suggested by which individual learning can alter \\nthe course of evolution. One such mechanism is called the \\nBaldwin \\neffect, after \\nJ. M. Baldwin (1896), who first suggested the idea. The Baldwin effect is based \\non the following observations: \\n0 If a species is evolving in a changing environment, there will be evolution- \\nary pressure to favor individuals with the capability to learn during their \\nlifetime. For example, if a new predator appears in the environment, then \\nindividuals capable of learning to avoid the predator will be more successful \\nthan individuals who cannot learn. In effect, the ability to learn allows an \\nindividual to perform a small local search during its lifetime to maximize its \\nfitness. In contrast, nonlearning individuals whose fitness is fully determined \\nby their genetic makeup will operate at a relative disadvantage. \\n0 Those individuals who are able to learn many traits will rely less strongly \\non their genetic code to \"hard-wire\" traits. As a result, these individuals \\ncan support a more diverse gene pool, relying on individual learning to \\novercome the \"missing\" or \"not quite optimized\" traits in the genetic code. \\nThis more diverse gene pool can, in turn, support more rapid evolutionary \\nadaptation. Thus, the ability of individuals to learn can have an indirect \\naccelerating effect on the rate of evolutionary adaptation for the entire pop- \\nulation. \\nTo illustrate, imagine some new change in the environment of some species, \\nsuch as a new predator. Such a change will selectively favor individuals capa- \\nble of learning to avoid the predator. As the proportion of such self-improving \\nindividuals in the population grows, the population will be able to support a \\nmore diverse gene pool, allowing evolutionary processes (even \\nnon-Lamarckian \\ngenerate-and-test processes) to adapt more rapidly. This accelerated adaptation \\nmay in turn enable standard evolutionary processes to more quickly evolve a \\ngenetic (nonlearned) trait to avoid the predator \\n(e.g., \\nan instinctive fear of this \\nanimal). Thus, the \\nBaldwin \\neffect provides an indirect mechanism for individ- \\nual learning to positively impact the rate of evolutionary progress. By increas- \\ning survivability and genetic diversity of the species, individual learning sup- \\nports more rapid evolutionary progress, thereby increasing the chance that the \\nspecies will evolve genetic, nonlearned traits that better fit the new environ- \\nment. \\nThere have been several attempts to develop computational models to study \\nthe \\nBaldwin \\neffect. For example, \\nHinton \\nand Nowlan (1987) experimented with \\nevolving a population of simple neural networks, in which some network weights \\nwere fixed during the individual network \"lifetime,\" while others were trainable. \\nThe genetic makeup of the individual determined which weights were train- \\nable and which were fixed. In their experiments, when no individual learning '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 279}, page_content='268 MACHINE LEARNING \\nwas allowed, the population failed to improve its fitness over time. However, \\nwhen individual learning was allowed, the population quickly improved its fit- \\nness. During early generations of evolution the population contained a greater \\nproportion of individuals with many trainable weights. However, as evolution \\nproceeded, the number of fixed, correct network weights tended to increase, as \\nthe population evolved toward genetically given weight values and toward less \\ndependence on individual learning of weights. Additional computational stud- \\nies of the \\nBaldwin \\neffect have been reported by Belew \\n(1990), \\nHarvey \\n(1993), \\nand French and Messinger (1994). An excellent overview of this topic can be \\nfound in Mitchell (1996). A special issue of the journal Evolutionary Computa- \\ntion on this topic (Turney et al. 1997) contains several articles on the Baldwin \\neffect. \\n9.7 PARALLELIZING GENETIC ALGORITHMS \\nGAS are naturally suited to parallel implementation, and a number of approaches \\nto parallelization have been explored. Coarse grain approaches to paralleliza- \\ntion \\nsubdivide the population into somewhat distinct groups of individuals, called \\ndemes. Each deme is assigned to a different computational node, and a standard \\nGA search is performed at each node. Communication and cross-fertilization be- \\ntween demes occurs on a less frequent basis than within demes. Transfer between \\ndemes occurs by a migration process, in which individuals from one deme are \\ncopied or transferred to other demes. This process is modeled after the kind of \\ncross-fertilization that might occur between physically separated subpopulations \\nof biological species. One benefit of such approaches is that it reduces the crowd- \\ning problem often encountered in nonparallel \\nGAS, \\nin which the system falls into \\na local optimum due to the early appearance of a genotype that comes to dominate \\nthe entire population. Examples of coarse-grained parallel \\nGAS \\nare described by \\nTanese (1989) and by \\nCohoon \\net al. (1987). \\nIn contrast to coarse-grained parallel implementations of \\nGAS, \\nfine-grained \\nimplementations typically assign one processor per individual in the population. \\nRecombination then takes place among neighboring individuals. Several differ- \\nent types of neighborhoods have been proposed, ranging from planar grid to \\ntorus. Examples of such systems are described by Spiessens and Manderick \\n(1991). An edited collection of papers on parallel \\nGAS \\nis available in Stender \\n(1993). \\n9.8 SUMMARY AND FURTHER READING \\nThe main points of this chapter include: \\n0 Genetic algorithms (GAS) conduct a randomized, parallel, hill-climbing \\nsearch for hypotheses that optimize a predefined fitness function. \\n0 The search performed by GAS is based on an analogy to biological evolu- \\ntion. A diverse population of competing hypotheses is maintained. At each '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 280}, page_content='iteration, the most fit members of the population are selected to produce new \\noffspring that replace the least fit members of the population. Hypotheses \\nare often encoded by strings that are combined by crossover operations, and . \\nsubjected to random mutations. \\na GAS illustrate how learning can be viewed as a special case of optimization. \\nIn particular, the learning task is to find the optimal hypothesis, according to \\nthe predefined fitness function. This suggests that other optimization tech- \\nniques such as simulated annealing can also be applied to machine learning \\nproblems. \\na GAS have most commonly been applied to optimization problems outside \\nmachine learning, such as design optimization problems. When applied to \\nlearning tasks, \\nGAS \\nare especially suited to tasks in which hypotheses are \\ncomplex \\n(e.g., \\nsets of rules for robot control, or computer programs), and \\nin which the objective to be optimized may be an indirect function of \\nthe hypothesis \\n(e.g., \\nthat the set of acquired rules successfully controls a \\nrobot). \\n0 Genetic programming is a variant of genetic algorithms in which the hy- \\npotheses being manipulated are computer programs rather than bit strings. \\nOperations such as crossover and mutation are generalized to apply to pro- \\ngrams rather than bit strings. Genetic programming has been demonstrated \\nto learn programs for tasks such as simulated robot control (Koza 1992) and \\nrecognizing objects in visual scenes (Teller and Veloso 1994). \\nEvolution-based computational approaches have been explored since the \\nearly days of computer science \\n(e.g., \\nBox 1957 and Bledsoe 1961). Several \\ndifferent evolutionary approaches were introduced during the 1960s and have \\nbeen further explored since that time. Evolution strategies, developed by \\nRechen- \\nberg (1965, 1973) to optimize numerical parameters in engineering design, were \\nfollowed up by Schwefel (1975, 1977, 1995) and others. Evolutionary program- \\nming, developed by Folgel, Owens, and Walsh (1966) as a method for evolv- \\ning finite-state machines, was followed up by numerous researchers \\n(e.g., \\nFogel \\nand \\nAtmar \\n1993). Genetic algorithms, introduced by Holland (1962, 1975) \\nincluded the notion of maintaining a large population of individuals and em- \\nphasized crossover as a key operation in such systems. Genetic programming, \\nintroduced by Koza \\n(1992), \\napplies the search strategy of genetic algorithms to \\nhypotheses consisting of computer programs. As computer hardware continues to \\nbecome faster and less expensive, interest in evolutionary approaches continues \\nto \\ngrow. \\nOne approach to using \\nGAS \\nto learn sets of rules was developed by \\nK. DeJong and his students at the University of Pittsburgh (e.g., Smith 1980). \\nIn \\nthis approach, each rule set is one member in the population of competing \\nhypotheses, as in the GABIL system discussed in this chapter. A somewhat dif- \\nferent approach was developed at University of Michigan by Holland and his \\nstudents (Holland \\n1986), \\nin which each rule is a member of the population, and '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 281}, page_content=\"the population itself is the rule set. A biological perspective on the roles of muta- \\ntion, inbreeding, cross-breeding, and selection in evolution is provided by Wright \\n(1977). \\nMitchell (1996) and \\nGoldberg \\n(1989) are two textbooks devoted to the sub- \\nject of genetic algorithms. Forrest (1993) provides an overview of the technical \\nissues in \\nGAS, \\nand \\nGoldberg \\n(1994) provides an overview of several recent ap- \\nplications. Koza's (1992) monograph on genetic programming is the standard \\nreference for this extension of genetic algorithms to manipulation of computer \\nprograms. The primary conference in which new results are published is the In- \\nternational Conference on Genetic Algorithms. Other relevant conferences include \\nthe Conference on Simulation of Adaptive Behavior, the International Confer- \\nence on Artijicial Neural Networks and Genetic Algorithms, and the IEEE In- \\nternational Conference on Evolutionary Computation. An annual conference is \\nnow held on genetic programming, as well (Koza et al. 1996b). The Evolution- \\nary Computation Journal is one source of recent research results in the field. \\nSeveral special issues of the journal Machine Learning have also been devoted \\nto \\nGAS. \\nEXERCISES \\n9.1. Design a genetic algorithm to learn conjunctive classification rules for the Play- \\nTennis problem described in Chapter 3. Describe precisely the bit-string encoding \\nof hypotheses and a set of crossover operators. \\n9.2. Implement a simple GA for Exercise 9.1. Experiment with varying population size p, \\nthe fraction r of the population replaced at each generation, and the mutation rate m. \\n9.3. Represent the program discovered by the GP (described in Section 9.5.2) as a tree. \\nIllustrate the operation of the GP crossover operator by applying it using two copies \\nof your tree as the two parents. \\n9.4. Consider applying GAS to the task of finding an appropriate set of weights for \\nan artificial neural network (in particular, a \\nfeedforward \\nnetwork identical to those \\ntrained by BACKPROPAGATION (Chapter 4)). Consider a 3 x 2 x 1 layered, feedfor- \\nward network. Describe an encoding of network weights as a bit string, and describe \\nan appropriate set of crossover operators. Hint: Do not allow all possible crossover \\noperations on bit strings. State one advantage and one disadvantage of using \\nGAS \\nin contrast to BACKPROPAGATION to train network weights. \\nREFERENCES \\nAckley, D., & Littman, M. (1994). A case for Lamarckian evolution. In C. Langton (Ed.), Am$cial \\nlife III. Reading, MA: Addison Wesley. \\nBack, T. (1996). Evolutionary algorithms in theory andpractice. Oxford, England: Oxford University \\nPress. \\nBaldwin, J. M. (1896). A new factor in evolution. American Naturalist, 3, 441-451, 536-553. \\nhttp://www.santafe.edu/sfi/publications/B \\nBelew, R. (1990). Evolution, learning, and culture: Computational metaphors for adaptive algorithms. \\nComplex Systems, 4, 11-49. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 282}, page_content='Belew, R. K., & Mitchell, M. (Eds.). (1996). Adaptive individuals in evolving populations: Models \\nand algorithms. Reading, MA: Addison-Wesley. \\nBledsoe, \\nW. (1961). The use of biological concepts in the analytical study of systems. Proceedings \\nof the ORSA-TIMS National Meeting, San Francisco. \\nBooker, L. B., Goldberg, D. E., & Holland, J. H. (1989). Classifier systems and genetic algorithms. \\nArtificial Intelligence, 40, 235-282. \\nBox, G. (1957). Evolutionary operation: A method for increasing industrial productivity. Jountal of \\nthe Royal Statistical Society, 6(2), 81-101. \\nCohoon, \\nJ. P., Hegde, S. U., Martin, W. N., & Richards, D. (1987). Punctuated equilibria: A parallel \\ngenetic algorithm. Proceedings of the Second International Conference on Genetic Algorithms \\n(pp. 148-154). \\nDeJong, \\nK. A. (1975). An analysis of behavior of a class of genetic adaptive systems (Ph.D. disser- \\ntation). University of Michigan. \\nDeJong, K. A., Spears, W. M., & Gordon, D. F. (1993). Using genetic algorithms for concept learning. \\nMachine Learning, 13, 161-188. \\nFolgel, L. J., Owens, A. J., & Walsh, M. J. (1966). Artificial intelligence through simulated evolution. \\nNew York: John Wiley & Sons. \\nFogel, L. J., & Atmar, W. (Eds.). (1993). Proceedings of the Second Annual Conference on Evolu- \\ntionary Programming. Evolutionary Programming Society. \\nForrest, S. (1993). Genetic algorithms: Principles of natural selection applied to computation. Science, \\n261, 872-878. \\nFrench, R., & Messinger A. (1994). Genes, phenes, and the Baldwin effect: Learning and evolution \\nin a simulated population. In R. Brooks and P. Maes (Eds.), ArtiJicial Life IV. Cambridge, \\nMA: MIT Press. \\nGoldberg, D. (1989). Genetic algorithms in search, optimization, and machine learning. Reading, \\nMA: Addison-Wesley. \\nGoldberg, D. (1994). Genetic and evolutionary algorithms come of age. Communications of the ACM, \\n37(3), 113-1 19. \\nGreen, D. P., & Smith, S. F. (1993). Competition based induction of decision models from examples. \\nMachine Learning, 13,229-257. \\nGrefenstette, J. J. (1988). Credit assignment in \\nrule \\ndiscovery systems based on genetic algorithms. \\nMachine Learning, 3, 225-245. \\nGrefenstette, J. J. \\n(1991). \\nLamarckian \\nlearning in multi-agent environments. In R. Belew and L. \\nBooker (Eds.), Proceedings of the Fourth International Conference on Genetic Algorithms. \\nSan Mateo, CA: Morgan Kaufmann. \\nHart, W., & Belew, R. (1995). Optimization with genetic algorithm hybrids that use local search. In \\nR. Below and M. Mitchell (Eds.), Adaptive individuals in evolving populations: Models and \\nalgorithms. Reading, MA: Addison-Wesley. \\nHarvey, I. (1993). The puzzle of the persistent question marks: A case study of genetic drift. In \\nForrest (Ed.), Proceedings of the Fzfth International Conference on Genetic Algorithms. San \\nMateo, CA: Morgan Kaufmann. \\nHinton, \\nG. &, & Nowlan, S. J. (1987). How learning can guide evolution. Complex Systems, 1, \\n495-502. \\nHolland, \\nJ. H. (1962). Outline for a logical theory of adaptive systems. Journal of the Association \\nfor Computing Machinery, 3, 297-314. \\nHolland, J. H. (1975). Adaptation in natural and art$cial systems. University of Michigan Press \\n(reprinted in 1992 by MIT Press, Cambridge, MA). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 283}, page_content=\"Holland, J. H. (1986). Escaping brittleness: The possibilities of general-purpose learning algorithms \\napplied to parallel rule-based systems. In R. \\nMichalski, \\nJ. Carbonell, & T. Mitchell (Eds.), \\nMachine learning: An artijicial intelligence approach (Vol. 2). San Mateo, CA: Morgan Kauf- \\nmann. \\nHolland, J. H. (1989). Searching nonlinear functions for high values. Applied Mathematics and Com- \\nputation, 32, 255-274. \\nJanikow, \\nC. Z. (1993). A knowledge-intensive GA for supervised learning. Machine Learning, 13, \\n189-228. \\nKoza, J. (1992). Genetic programming: On the programming of computers by means of natural se- \\nlection. Cambridge, MA: MIT Press. \\nKoza, J. R. (1994). Genetic Programming 11: Automatic discovery of reusable programs. Cambridge, \\nMA: The MIT Press. \\nKoza, J. R., Bennett \\n111, \\nF. H., Andre, D., & Keane, M. A. (1996). Four problems for which a \\ncomputer program evolved by genetic programming is competitive with human performance. \\nProceedings of the 1996 IEEE International Conference on Evolutionary Computation (pp. \\n1-10). IEEE Press. \\nKoza, J. R., Goldberg, D. E., \\nFogel, \\nD. B., & Riolo, R. L. (Eds.). (1996b). Genetic programming \\n19%: Proceedings of the First Annual Conference. Cambridge, MA: MIT Press. \\nMachine Learning: Special Issue on Genetic Algorithms (1988) 3:2-3, October. \\nMachine Learning: Special Issue on Genetic Algorithms (1990) 5:4, October. \\nMachine karning: Special Issue on Genetic Algorithms (1 993) l3:2,3, November. \\nMitchell, M. (1996). An introduction to genetic algorithms. Cambridge, MA: MIT Press. \\nO'Reilly, \\nU-M., & Oppacher, R. (1994). Program search with a hierarchical variable length repre- \\nsentation: Genetic programming, simulated annealing, and hill climbing. In Y. Davidor et al. \\n(Eds.), Parallel problem solving from nature-PPSN I11 (Vol. 866) (Lecture notes in computer \\nscience). Springer-Verlag. \\nRechenberg, I. (1965). Cybernetic solution path of an experimental problem. Ministry of aviation, \\nRoyal Aircraft Establishment, U.K. \\nRechenberg, I. (1973). Evolutionsstrategie: Optimierung technischer systeme nach prinzipien der \\nbiolgischen evolution. Stuttgart: Frommann-Holzboog. \\nSchwefel, H. P. (1975). Evolutionsstrategie und numerische optimiemng (Ph.D. thesis). Technical \\nUniversity of Berlin. \\nSchwefel, H. P. (1977). Numerische optimierung von computer-modellen mittels der evolutionsstrate- \\ngie. Basel: Birkhauser. \\nSchwefel, H. P. (1995). Evolution and optimum seeking. New York: John Wiley & Sons. \\nSpiessens, P., & Manderick, B. (1991). A massively parallel genetic algorithm: Implementation and \\nfirst analysis. Proceedings of the 4th International Conference on Genetic Algorithms (pp. \\n279-286). \\nSmith, S. (1980). A learning system based on genetic adaptive algorithms (Ph.D. dissertation). Com- \\nputer Science, University of Pittsburgh. \\nStender, J. (Ed.) (1993). Parallel genetic algorithms. Amsterdam: IOS Publishing. \\nTanese, R. (1989). Distributed genetic algorithms. Proceedings of the 3rd International Conference \\non Genetic Algorithms (pp. 434-439). \\nTeller, A., & Veloso, M. (1994). PADO: A new learning architecture for object recognition. In K. \\nIkeuchi & M. Veloso (Eds.), Symbolic visual learning @p. 81-116). Oxford, England: Oxford \\nUniv. Press. \\nTurney, P. D. (1995). Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision \\ntree induction algorithm. Journal of Al Research, 2, 369-409. http://www.cs.washington.edu/ \\nresearch/jair/home.htmI. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 284}, page_content='Tumey, P. D., Whitley, D., & Anderson, R. (1997). Evolutionary Computation. Special issue: \\nThe \\nBaldwin \\neffect, \\n4(3). \\nCambridge, MA: MIT Press. http://www-mitpress.mit.eduljmls- \\ncatalog/evolution-abstracts/evol.html. \\nWhitley, L. D., & Vose, M. D. (Eds.). (1995). Foundations of genetic algorithms 3. Morgan Kauf- \\nmann. \\nWright, S. (1977). Evolution and the genetics of populations. Vol. 4: Variability within and among \\nNatural Populations. Chicago: University of Chicago Press. \\nZbignlew, M. (1992). Genetic algorithms + data structures = evolution programs. Berlin: Springer- \\nVerlag. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 285}, page_content='CHAPTER \\nLEARNING \\nSETS OF RULES \\nOne of the most expressive and human readable representations for learned hypothe- \\nses is sets of if-then rules. This chapter explores several algorithms for learning such \\nsets of rules. \\nOne \\nimportant special case involves learning sets of rules containing \\nvariables, called first-order \\nHorn \\nclauses. Because sets of first-order Horn clauses \\ncan be interpreted as programs in the logic programming language PROLOG, learning \\nthem is often called inductive logic programming (ILP). This chapter examines sev- \\neral approaches to learning sets of rules, including an approach based on inverting \\nthe deductive operators of mechanical theorem provers. \\n10.1 INTRODUCTION \\nIn many cases it is useful to learn the target function represented as a set of \\nif-then rules that jointly define the function. As shown in Chapter 3, one way to \\nlearn sets of rules is to first learn a decision tree, then translate the tree into an \\nequivalent set of rules-one rule for each leaf node in the tree. A second method, \\nillustrated in Chapter 9, is to use a genetic algorithm that encodes each rule set \\nas a bit string and uses genetic search operators to explore this hypothesis space. \\nIn this chapter we explore a variety of algorithms that directly learn rule sets and \\nthat differ from these algorithms in two key respects. First, they are designed to \\nlearn sets of first-order rules that contain variables. This is significant because \\nfirst-order rules are much more expressive than propositional rules. Second, the \\nalgorithms discussed here use sequential covering algorithms that learn one rule \\nat a time to incrementally grow the final set of rules. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 286}, page_content='As an example of first-order rule sets, consider the following two rules \\nthat jointly describe the target concept Ancestor. Here we use the predicate \\nParent(x, y) to indicate that y is the mother or father of x, and the predicate \\nAncestor(x, y) to indicate that y is an ancestor of x related by an arbitrary num- \\nber of family generations. \\nIF Parent (x, y) THEN Ancestor(x,y) \\nIF Parent(x, z) A Ancestor(z, y) THEN Ancestor(x, y) \\nNote these two rules compactly describe a recursive function that would be very \\ndifficult to represent using a decision tree or other propositional representation. \\nOne way to see the representational power of first-order rules is to consider the \\ngeneral purpose programming language PROLOG. In PROLOG, programs are sets of \\nfirst-order rules such as the two shown above (rules of this form are also called \\nHorn clauses). In fact, when stated in a slightly different syntax the above rules \\nform a valid PROLOG program for computing the Ancestor relation. In this light, \\na general purpose algorithm capable of learning such rule sets may be viewed \\nas an algorithm for automatically inferring PROLOG programs from examples. In \\nthis chapter we explore learning algorithms capable of learning such rules, given \\nappropriate sets of training examples. \\nIn practice, learning systems based on first-order representations have been \\nsuccessfully applied to problems such as learning which chemical bonds fragment \\nin a mass spectrometer (Buchanan 1976; Lindsay \\n1980), \\nlearning which chemical \\nsubstructures produce mutagenic activity (a property related to carcinogenicity) \\n(Srinivasan et al. \\n1994), \\nand learning to design finite element meshes to analyze \\nstresses in physical structures (Dolsak and Muggleton 1992). In each of these \\napplications, the hypotheses that must be represented involve relational assertions \\nthat can be conveniently expressed using first-order representations, while they \\nare very difficult to describe using propositional representations. \\nIn this chapter we begin by considering algorithms that learn sets of propo- \\nsitional rules; that is, rules without variables. Algorithms for searching the hy- \\npothesis space to learn disjunctive sets of rules are most easily understood in \\nthis setting. We then consider extensions of these algorithms to learn first-order \\nrules. Two general approaches to inductive logic programming are then consid- \\nered, and the fundamental relationship between inductive and deductive inference \\nis explored. \\n10.2 SEQUENTIAL COVERING ALGORITHMS \\nHere we consider a family of algorithms for learning rule sets based on the strategy \\nof learning one rule, removing the data it covers, then iterating this process. Such \\nalgorithms are called sequential covering algorithms. To elaborate, imagine we \\nhave a subroutine LEARN-ONE-RULE that accepts a set of positive and negative \\ntraining examples as input, then outputs a single rule that covers many of the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 287}, page_content='positive examples and few of the negative examples. We require that this iaarput \\nrule have high accuracy, but not necessarily high coverage. By high accuracy, we \\nmean the predictions it makes should be correct. By accepting low coverage, we \\nmean it need not make predictions for every training example. \\nGiven this LEARN-ONE-RULE subroutine for learning a single rule, one obvi- \\nous approach to learning a set of rules is to invoke LEARN-ONE-RULE on all the \\navailable training examples, remove any positive examples covered by the rule it \\nlearns, then invoke it again to learn a second rule based on the remaining train- \\ning examples. This procedure can be iterated as many times as desired to learn \\na disjunctive set of rules that together cover any desired fraction of the positive \\nexamples. This is called a sequential covering algorithm because it sequentially \\nlearns a set of rules that together cover the full set of positive examples. The \\nfinal set of rules can then be sorted so that more accurate rules will be considered \\nfirst when a new instance must be classified. A prototypical sequential covering \\nalgorithm is described in Table 10.1. \\nThis sequential covering algorithm is one of the most widespread approaches \\nto learning disjunctive sets of rules. It reduces the problem of learning a disjunc- \\ntive set of rules to a sequence of simpler problems, each requiring that a single \\nconjunctive rule be learned. Because it performs a greedy search, formulating a \\nsequence of rules without backtracking, it is not guaranteed to find the smallest \\nor best set of rules that cover the training examples. \\nHow shall we design LEARN-ONE-RULE to meet the needs of the sequential \\ncovering algorithm? We require an algorithm that can formulate a single rule \\nwith high accuracy, but that need not cover all of the positive examples. In this \\nsection we present a variety of algorithms and describe the main variations that \\nhave been explored in the research literature. In this section we consider learning \\nonly propositional rules. In later sections, we extend these algorithms to learn \\nfirst-order Horn clauses. \\nSEQUENTIAL-COVERING(T~~~~~~~~~~~~~~, Attributes, Examples, Threshold) \\n0 Learnedxules c {} \\n0 Rule c ~~~~~-o~~-~~~~(Targetattribute, Attributes, Examples) \\n0 while PERFORMANCE(RU~~, Examples) > Threshold, do \\n0 Learned~ules c Learnedxules + Rule \\n0 Examples c Examples - {examples correctly classified by Rule] \\n0 Rule c ~~~~~-oN~-RuL~(Targetllttribute, Attributes, Examples) \\n0 Learnedxules c sort Learned-rules accord to PERFORMANCE over Examples \\n0 return Learnedxules \\nTABLE 10.1 \\nThe sequential covering algorithm for learning a disjunctive set of rules. LEARN-ONE-RULE must \\nreturn a single rule that covers at least some of the Examples. PERFORMANCE is a user-provided \\nsubroutine to evaluate rule quality. This covering algorithm \\nlearns \\nrules until it can no longer learn \\na rule whose performance is above the given Threshold. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 288}, page_content='10.2.1 General to Specific Beam Search \\nOne effective approach to implementing LEARN-ONE-RULE is to organize the hy- \\npothesis space search in the same general fashion as the ID3 algorithm, but to \\nfollow only the most promising branch in the tree at each step. As illustrated in the \\nsearch tree of Figure 10.1, the search begins by considering the most general rule \\nprecondition possible (the empty test that matches every instance), then greed- \\nily adding the attribute test that most improves rule performance measured over \\nthe training examples. Once this test has been added, the process is repeated by \\ngreedily adding a second attribute test, and so on. Like \\nID3, \\nthis process grows the \\nhypothesis by greedily adding new attribute tests until the hypothesis reaches an \\nacceptable level of performance. Unlike ID3, this implementation of \\nLEARN-ONE- \\nRULE follows only a single descendant at each search step-the attribute-value \\npair yielding the best performance-rather than growing a \\nsubtree \\nthat covers all \\npossible values of the selected attribute. \\nThis approach to implementing LEARN-ONE-RULE performs a \\ngeneral-to- \\nspecific search through the space of possible rules in search of a rule with high \\naccuracy, though perhaps incomplete coverage of the data. As in decision tree \\nlearning, there are many ways to define a measure to select the \"best\" descendant. \\nTo follow the lead of ID3 let us for now define the best descendant as the one \\nwhose covered examples have the lowest entropy (recall Equation \\nf3.31). \\nThe general-to-specific search suggested above for the LEARN-ONE-RULE al- \\ngorithm is a greedy depth-first search with no backtracking. As with any greedy \\nIF \\nTHEN PlayTennis=yes \\nIF Wind=strong t IF Humidity=high \\nTHEN PlayTennis=no IF Hum\\'ditv=norntal THEN PlayTennis=no \\nTHEN PlayTennis=yes \\nIF Humidify=normal \\nWind=weak \\nTHEN PlayTennis=yes IF Humidity=normal \\nA/\\\\\\\\----- IF Humidity=nowl ... \\nWind=strong IF Humidity=normal Outlook=rain \\nTHEN PlayTennis=yes Outlook=sunny THEN PlnyTennis=yes \\nTHEN PlayTennis=yes \\nFIGURE 10.1 \\nThe search for rule preconditions as LEARN-ONE-RULE proceeds from general to specific. At each \\nstep, the preconditions of the best rule are specialized in all possible ways. Rule postconditions are \\ndetermined by the examples found to satisfy the preconditions. This figure illustrates a beam search \\nof \\nwidth 1. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 289}, page_content='search, there is a danger that a suboptimal choice will be made at any step. To \\nreduce this risk, we can extend the algorithm to perform a beam search; that is, \\na search in which the algorithm maintains a list of the k best candidates at each \\nstep, rather than a single best candidate. On each search step, descendants (spe- \\ncializations) are generated for each of these k best candidates, and the resulting \\nset is again reduced to the k most promising members. Beam search keeps track \\nof the most promising alternatives to the current top-rated hypothesis, so that all \\nof their successors can be considered at each search step. This general to specific \\nbeam search algorithm is used by the CN2 program described by Clark and Niblett \\n(1989). The algorithm is described in Table 10.2. \\nLEARN-ONE-RULE(T~~~~~~~~~~~U~~, Attributes, Examples, k) \\nReturns a single rule that covers some of the Examples. Conducts a generalJotospec$c \\ngreedy beam search for the best rule, guided by the PERFORMANCE metric. \\na Initialize Besthypothesis to the most general hypothesis 0 \\na Initialize Candidatehypotheses to the set (Besthypothesis) \\na While Candidatehypotheses is not empty, Do \\nI. Generate the next more spec@ candidatehypotheses \\na Allronstraints c the set of all constraints of the form (a = v), where a is a member \\nof Attributes, and v is a value of a that occurs in the current set of Examples \\na Newrandidatehypotheses c \\nfor each h in Candidatehypotheses, \\nfor each c in Alll-onstraints, \\ncreate a specialization of h by adding the constraint c \\na Remove from Newl-andidatehypotheses any hypotheses that are duplicates, inconsis- \\ntent, or not maximally specific \\n2. Update Besthypothesis \\na For all h in Newnandidatehypotheses do \\na If  PERFORMANCE(^, Examples, Targetattribute) \\nz PERFORMANCE(Besthypothesis, Examples, Targetattribute)) \\nThen Besthypothesis t h \\n3. Update Candidatehypotheses \\na Candidatehypotheses c the k best members of New-candidatehypotheses, according \\nto the PERFORMANCE measure. \\na Return a rule of the form \\n\"IF Best hypothesis THEN prediction\" \\nwhere prediction is the most frequent value of Targetattribute among those Examples \\nthat match Besthypothesis. \\nPERM)RMANCE(~, \\nExamples, Target \\nattribute) \\na hxxamples t the subset of Examples that match h \\nreturn -Entropy(hxxarnples), where entropy is with respect to Targetattribute \\nTABLE 10.2 \\nOne implementation for LEARN-ONE-RULE is a general-to-specific beam search. The frontier of current \\nhypotheses is represented by the variable Candidatehypotheses. This algorithm is similar to that \\nused by the CN2 program, described by Clark and Niblett (1989). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 290}, page_content='A few remarks on the LEARN-ONE-RULE algorithm of Table 10.2 are in order. \\nFirst, note that each hypothesis considered in the main loop of the algorithm is \\na conjunction of attribute-value constraints. \\nEach \\nof these conjunctive hypotheses \\ncorresponds to a candidate set of preconditions for the rule to be learned and is \\nevaluated by the entropy of the examples it covers. The search considers increas- \\ningly specific candidate hypotheses until it reaches a maximally specific hypothesis \\nthat contains all available attributes. The rule that is output by the algorithm is the \\nrule encountered during the search whose PERFORMANCE is greatest-not necessar- \\nily the final hypothesis generated in the search. The postcondition for the output \\nrule is chosen only in the final step of the algorithm, after its precondition (rep- \\nresented by the variable Besthypothesis) has been determined. The algorithm \\nconstructs the rule postcondition to predict the value of the target attribute that \\nis most common among the examples covered by the rule precondition. Finally, \\nnote that despite the use of beam search to reduce the risk, the greedy search may \\nstill produce suboptimal rules. However, even when this occurs the \\nSEQUENTIAL- \\nCOVERING algorithm can still learn a collection of rules that together cover the \\ntraining examples, because it repeatedly calls LEARN-ONE-RULE on the remaining \\nuncovered examples. \\n10.2.2 Variations \\nThe SEQUENTIAL-COVERING algorithm, together with the LEARN-ONE-RULE algo- \\nrithm, learns a set of if-then rules that covers the training examples. Many varia- \\ntions on this approach have been explored. For example, in some cases it might \\nbe desirable to have the program learn only rules that cover positive examples \\nand to include a \"default\" that assigns a negative classification to instances not \\ncovered by any rule. This approach might be desirable, say, if one is attempting \\nto learn a target concept such as \"pregnant women who are likely to have twins.\" \\nIn this case, the fraction of positive examples in the entire population is small, so \\nthe rule set will be more compact and intelligible to humans if it identifies only \\nclasses of positive examples, with the default classification of all other examples \\nas negative. This approach also corresponds to the \"negation-as-failure\" strategy \\nof PROLOG, in which any expression that cannot be proven to be true is by default \\nassumed to be false. In order to learn such rules that predict just a single target \\nvalue, the LEARN-ONE-RULE algorithm can be modified to accept an additional in- \\nput argument specifying the target value of interest. The general-to-specific beam \\nsearch is conducted just as before, changing only the PERFORMANCE subroutine \\nthat evaluates hypotheses. Note the definition of PERFORMANCE as negative en- \\ntropy is no longer appropriate in this new setting, because it assigns a maximal \\nscore to hypotheses that cover exclusively negative examples, as well as those \\nthat cover exclusively positive examples. Using a measure that evaluates the frac- \\ntion of positive examples covered by the hypothesis would be more appropriate \\nin this case. \\nAnother variation is provided by a family of algorithms called AQ (Michal- \\nski 1969, Michalski et al. 1986), that predate the CN2 algorithm on which the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 291}, page_content='above discussion is based. Like CN2, AQ learns a disjunctive set of rules that \\ntogether cover the target function. However, AQ differs in several ways from \\nthe algorithms given here. First, the covering algorithm of AQ differs from the \\nSEQUENTIAL-COVERING algorithm because it explicitly seeks rules that cover a par- \\nticular target value, learning a disjunctive set of rules for each target value in \\nturn. Second, \\nAQ\\'s \\nalgorithm for learning a single rule differs from \\nLEARN-ONE- \\nRULE. While it conducts a general-to-specific beam search for each rule, it uses a \\nsingle positive example to focus this search. In particular, it considers only those \\nattributes satisfied by the positive example as it searches for progressively more \\nspecific hypotheses. Each time it learns a new rule it selects a new positive ex- \\nample from those that are not yet covered, to act as a seed to guide the search for \\nthis new disjunct. \\n10.3 LEARNING RULE SETS: SUMMARY \\nThe SEQUENTIAL-COVERING algorithm described above and the decision tree learn- \\ning algorithms of Chapter 3 suggest a variety of possible methods for learning \\nsets of rules. This section considers several key dimensions in the design space \\nof such rule learning algorithms. \\nFirst, sequential covering algorithms learn one rule at a time, removing \\nthe covered examples and repeating the process on the remaining examples. In \\ncontrast, decision tree algorithms such as ID3 learn the entire set of disjuncts \\nsimultaneously as part of the single search for an acceptable decision tree. We \\nmight, therefore, call algorithms such as ID3 simultaneous covering algorithms, in \\ncontrast to sequential covering algorithms such as CN2. Which should we prefer? \\nThe key difference occurs in the choice made at the most primitive step in the \\nsearch. At each search step ID3 chooses among alternative attributes by com- \\nparing the partitions of the data they generate. In contrast, CN2 chooses among \\nalternative attribute-value pairs, by comparing the subsets of data they cover. \\nOne way to see the significance of this difference is to compare the number of \\ndistinct choices made by the two algorithms in order to learn the same set of \\nrules. To learn a set of n rules, each containing k attribute-value tests in their \\npreconditions, sequential covering algorithms will perform n . k primitive search \\nsteps, making an independent decision to select each precondition of each rule. \\nIn contrast, simultaneous covering algorithms will make many fewer independent \\nchoices, because each choice of a decision node in the decision tree corresponds \\nto choosing the precondition for the multiple rules associated with that node. In \\nother words, if the decision node tests an attribute that has m possible values, the \\nchoice of the decision node corresponds to choosing a precondition for each of the \\nm corresponding rules (see Exercise 10.1). Thus, sequential covering algorithms \\nsuch as CN2 make a larger number of independent choices than simultaneous \\ncovering algorithms such as ID3. Still, the question remains, which should we \\nprefer? The answer may depend on how much training data is available. If data is \\nplentiful, then it may support the larger number of independent decisions required \\nby the sequential covering algorithm, whereas if data is scarce, the \"sharing\" of '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 292}, page_content='decisions regarding preconditions of different rules may be more effective. An \\nadditional consideration is the task-specific question of whether it is desirable \\nthat different rules test the same attributes. In the simultaneous covering deci- \\nsion tree learning algorithms, they will. In sequential covering algorithms, they \\nneed not. \\nA second dimension along which approaches vary is the direction of the \\nsearch in LEARN-ONE-RULE. In the algorithm described above, the search is from \\ngeneral to specijic hypotheses. Other algorithms we have discussed (e.g., FIND-S \\nfrom Chapter 2) search from specijic to general. One advantage of general to \\nspecific search in this case is that there is a single maximally general hypothesis \\nfrom which to begin the search, whereas there are very many specific hypotheses \\nin most hypothesis spaces \\n(i.e., \\none for each possible instance). Given many \\nmaximally specific hypotheses, it is unclear which to select as the starting point of \\nthe search. One program that conducts a specific-to-general search, called \\nGOLEM \\n(Muggleton and Feng \\n1990), \\naddresses this issue by choosing several positive \\nexamples at random to initialize and to guide the search. The best hypothesis \\nobtained through multiple random choices is then selected. \\nA third dimension is whether the LEARN-ONE-RULE search is a generate then \\ntest search through the syntactically legal hypotheses, as it is in our suggested \\nimplementation, or whether it is example-driven so that individual training exam- \\nples constrain the generation of hypotheses. Prototypical example-driven search \\nalgorithms include the FIND-S and CANDIDATE-ELIMINATION algorithms of Chap- \\nter 2, the AQ algorithm, and the CIGOL algorithm discussed later in this chapter. \\nIn each of these algorithms, the generation or revision of hypotheses is driven \\nby the analysis of an individual training example, and the result is a revised \\nhypothesis designed to correct performance for this single example. This con- \\ntrasts to the generate and test search of LEARN-ONE-RULE in Table 10.2, in which \\nsuccessor hypotheses are generated based only on the syntax of the hypothesis \\nrepresentation. The training data is considered only after these candidate hypothe- \\nses are generated and is used to choose among the candidates based on their \\nperformance over the entire collection of training examples. One important ad- \\nvantage of the generate and test approach is that each choice in the search is \\nbased on the hypothesis performance over many examples, so that the impact \\nof noisy data is minimized. In contrast, example-driven algorithms that refine \\nthe hypothesis based on individual examples are more easily misled by a sin- \\ngle noisy training example and are therefore less robust to errors in the training \\ndata. \\nA fourth dimension is whether and how rules are post-pruned. As in decision \\ntree learning, it is possible for LEARN-ONE-RULE to formulate rules that perform \\nvery well on the training data, but less well on subsequent data. As in decision \\ntree \\nlearning, one way to address this issue is to post-prune each rule after it \\nis \\nlearned from the training data. In particular, preconditions can be removed \\nfrom the rule whenever this leads to improved performance over a set of pruning \\nexamples distinct from the training examples. A more detailed discussion of rule \\npost-pruning is provided in Section 3.7.1.2. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 293}, page_content='A final dimension is the particular definition of rule PERFORMANCE used to \\nguide the search in LEARN-ONE-RULE. Various evaluation functions have been used. \\nSome common evaluation functions include: \\n0 Relative frequency. Let n denote the number of examples the rule matches \\nand let nc denote the number of these that it classifies correctly. The relative \\nfrequency estimate of rule performance is \\nRelative frequency is used to evaluate rules in the AQ program. \\n0 m-estimate of accuracy. This accuracy estimate is biased toward the default \\naccuracy expected of the rule. It is often preferred when data is scarce and \\nthe rule must be evaluated based on few examples. As above, let n and nc \\ndenote the number of examples matched and correctly predicted by the rule. \\nLet p be the prior probability that a randomly drawn example from the entire \\ndata set will have the classification assigned by the rule \\n(e.g., \\nif 12 out of \\n100 examples have the value predicted by the rule, then p = .12). Finally, \\nlet m be the weight, or equivalent number of examples for weighting this \\nprior p. The m-estimate of rule accuracy is \\nNote if m is set to zero, then the m-estimate becomes the above relative fre- \\nquency estimate. As m is increased, a larger number of examples is needed \\nto override the prior assumed accuracy p. The m-estimate measure is advo- \\ncated by Cestnik and Bratko (1991) and has been used in some versions of \\nthe CN2 algorithm. It is also used in the naive Bayes classifier discussed in \\nSection 6.9.1. \\n0 Entropy. This is the measure used by the PERFORMANCE subroutine in the \\nalgorithm of Table 10.2. Let S be the set of examples that match the rule \\npreconditions. Entropy measures the uniformity of the target function values \\nfor this set of examples. We take the negative of the entropy so that better \\nrules will have higher scores. \\nC \\n-Entropy (S) = pi logl pi \\nwhere c is the number of distinct values the target function may take on, \\nand where pi is the proportion of examples from S for which the target \\nfunction takes on the ith value. This entropy measure, combined with a test \\nfor statistical significance, is used in the CN2 algorithm of Clark and \\nNiblett \\n(1989). It is also the basis for the information gain measure used by many \\ndecision tree learning algorithms. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 294}, page_content='10.4 LEARNING FIRST-ORDER RULES \\nIn the previous sections we discussed algorithms for learning sets of propositional \\n(i.e., \\nvariable-free) rules. In this section, we consider learning rules that con- \\ntain variables-in particular, learning first-order Horn theories. Our motivation \\nfor considering such rules is that they are much more expressive than proposi- \\ntional rules. Inductive learning of first-order rules or theories is often referred to \\nas inductive logic programming (or LP for short), because this process can be \\nviewed as automatically inferring PROLOG programs from examples. PROLOG is a \\ngeneral purpose, Turing-equivalent programming language in which programs are \\nexpressed as collections of Horn clauses. \\n10.4.1 First-Order Horn Clauses \\nTo see the advantages of first-order representations over propositional (variable- \\nfree) representations, consider the task of learning the simple target concept \\nDaughter (x, y), defined over pairs of people x and y. The value of Daughter(x, y) \\nis True when x is the daughter of y, and False otherwise. Suppose each person \\nin the data is described by the attributes Name, Mother, Father, Male, Female. \\nHence, each training example will consist of the description of two people in \\nterms of these attributes, along with the value of the target attribute Daughter. \\nFor example, the following is a positive example in which Sharon is the daughter \\nof Bob: \\n(Namel = Sharon, Motherl = Louise, Fatherl = Bob, \\nMalel = False, Female1 = True, \\nName2 = Bob, Mother2 = Nora, Father2 = Victor, \\nMale2 = True, Female2 = False, Daughterl.2 = True) \\nwhere the subscript on each attribute name indicates which of the.two persons is \\nbeing described. Now if we were to collect a number of such training examples for \\nthe target concept Daughterlv2 and provide them to a propositional rule learner \\nsuch as CN2 or C4.5, the result would be a collection of very specific rules \\nsuch as \\nIF (Father1 = Bob) A (Name2 = Bob) A (Femalel = True) \\nTHEN  daughter^,^ = True \\nAlthough it is correct, this rule is so specific that it will rarely, if ever, be useful in \\nclassifying future pairs of people. The problem is that propositional representations \\noffer no general way to describe the essential relations among the values of the \\nattributes. In contrast, a program using first-order representations could learn the \\nfollowing general rule: \\nIF Father(y, x) r\\\\ Female(y), THEN Daughter(x, y) \\nwhere x and y are variables that can be bound to any person. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 295}, page_content='First-order Horn clauses may also refer to variables in the preconditions that \\ndo not occur in the postconditions. For example, one rule for GrandDaughter \\nmight be \\nIF Father(y, z) A Mother(z, x) A Female(y) \\nTHEN GrandDaughter(x, y) \\nNote the variable z in this rule, which refers to the father of y, is not present in the \\nrule postconditions. Whenever such a variable occurs only in the preconditions, \\nit is assumed to be existentially quantified; that is, the rule preconditions are \\nsatisfied as long as there exists at least one binding of the variable that satisfies \\nthe corresponding literal. \\nIt is also possible to use the same predicates in the rule postconditions and \\npreconditions, enabling the description of recursive rules. For example, the two \\nrules at the beginning of this chapter provide a recursive definition of the concept \\nAncestor (x, y). ILP learning methods such-as those described below have been \\ndemonstrated to learn a variety of simple recursive functions, such as the above \\nAncestor function, and functions for sorting the elements of a list, removing a \\nspecific element from a list, and appending two lists. \\nPOs4.2 Terminology \\nBefore moving on to algorithms for learning sets of Horn clauses, let us intro- \\nduce some basic terminology from formal logic. All expressions are composed \\nof constants (e.g., Bob, Louise), variables (e.g., x, y), predicate symbols (e.g., \\nMarried, Greater-Than), and function symbols (e.g., age). The difference be- \\ntween predicates and functions is that predicates take on values of True or False, \\nwhereas functions may take on any constant as their value. We will use lowercase \\nsymbols for variables and capitalized symbols for constants. Also, we will use \\nlowercase for functions and capitalized symbols for predicates. \\nFrom these symbols, we build up expressions as follows: A term is any con- \\nstant, any variable, or any function applied to any term \\n(e.g., Bob, x, age(Bob)). \\nA literal is any predicate or its negation applied to any term (e.g., Married(Bob, \\nLouise), \\n-Greater-Than(age(Sue), 20)). If a literal contains a negation (1) sym- \\nbol, we call it a negative literal, otherwise a positive literal. \\nA clause is any disjunction of literals, where all variables are assumed to be \\nuniversally quantified. A Horn clause is a clause containing at most one positive \\nliteral, such as \\nwhere H is the positive literal, and -Ll . . . -Ln are negative literals. Because of \\nthe equalities (B v -A) = (B t A) and -(A A B) = (-A v -B), the above Horn \\nclause can alternatively be written in the form '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 296}, page_content='Every well-formed expression is composed of constants (e.g., Mary, 23, or Joe), variables (e.g., \\nx), predicates (e.g., Female, as in Female(Mary)), and functions (e.g., age, as in age(Mary)). \\nA term is any constant, any variable, or any function applied to any term. Examples include Mary, \\nx, \\nage(Mary), \\nage(x). \\nA literal is any predicate (or its negation) applied to any set of terms. Examples include \\nFemal \\ne(Mary), - Female(x), Greaterf han (age(Mary), 20). \\nA ground literal is a literal that does not contain any variables (e.g., -Female(Joe)). \\nA negative literal is a literal containing a negated predicate (e.g., -Female(Joe)). \\nA positive literal is a literal with no negation sign (e.g., Female(Mary)). \\nA clause is any disjunction of literals M1 v . . . Mn whose variables are universally quantified. \\nA Horn clause is an expression of the form \\nwhere H, L1 . . . Ln are positive literals. H is called the head or consequent of the Horn clause. \\nThe conjunction of literals L1 A L2 A .. .A L, is called the body or antecedents of the Horn clause. \\nFor any literals A and B, the expression (A t B) is equivalent to (A v -B), and the expression \\n-(A A B) is equivalent to (-A v -B). Therefore, a Horn clause can equivalently be written as the \\ndisjunction \\nHv-L1 \\nv...v-L, \\nA substitution is any function that replaces variables by terms. For example, the substitution \\n{x/3, \\ny/z) \\nreplaces the variable x by the term 3 and replaces the variable y by the term z. Given \\na substitution \\n0 \\nand a literal L we write \\nLO \\nto denote the result of applying substitution \\n0 to L. \\nA unrfying substitution for two literals L1 and L2 is any substitution 0 such that L10 = L1B. \\nTABLE 10.3 \\nBasic definitions from first-order logic. \\nwhich is equivalent to the following, using our earlier rule notation \\nIF L1 A ... A L,, THEN H \\nWhatever the notation, the Horn clause preconditions L1 A . . . A L, are called the \\nclause body or, alternatively, the clause antecedents. The literal H that forms the \\npostcondition is called the clause head or, alternatively, the clause consequent. \\nFor easy reference, these definitions are summarized in Table 10.3, along with \\nother definitions introduced later in this chapter. \\n10.5 LEARNING SETS OF FIRST-ORDER RULES: FOIL \\nA variety of algorithms has been proposed for learning first-order rules, or Horn \\nclauses. In this section we consider a program called FOIL (Quinlan 1990) that \\nemploys an approach very similar to the SEQUENTIAL-COVERING and LEARN-ONE- \\nRULE algorithms of the previous section. In fact, the FOIL program is the natural \\nextension of these earlier algorithms to first-order representations. Formally, the \\nhypotheses learned by FOIL are sets of first-order rules, where each rule is sim- \\nilar to a Horn clause with two exceptions. First, the rules learned by FOIL are '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 297}, page_content=\"more restricted than general Horn clauses, because the literals are not pennitted \\nto contain function symbols (this reduces the complexity of the hypothesis space \\nsearch). Second, FOIL rules are more expressive than Horn clauses, because the \\nliterals appearing in the body of the rule may be negated. FOIL has been applied \\nto a variety of problem domains. For example, it has been demonstrated to learn a \\nrecursive definition of the QUICKSORT algorithm and to learn to discriminate legal \\nfrom illegal chess positions. \\nThe FOIL algorithm is summarized in Table 10.4. Notice the outer loop \\ncorresponds to a variant of the SEQUENTIAL-COVERING algorithm discussed earlier; \\nthat is, it learns new rules one at a time, removing the positive examples covered by \\nthe latest rule before attempting to learn the next rule. The inner loop corresponds \\nto a variant of our earlier LEARN-ONE-RULE algorithm, extended to accommodate \\nfirst-order rules. Note also there are a few minor differences between FOIL and \\nthese earlier algorithms. In particular, FOIL seeks only rules that predict when \\nthe target literal is True, whereas our earlier algorithm would seek both rules \\nthat predict when it is True and rules that predict when it is False. Also, FOIL \\nperforms a simple hillclimbing search rather than a beam search (equivalently, it \\nuses a beam of width one). \\nThe hypothesis space search performed by FOIL is best understood by view- \\ning it hierarchically. Each iteration through \\nFOIL'S \\nouter loop adds a new rule to \\nits disjunctive hypothesis, Learned~ules. The effect of each new rule is to gen- \\n-- \\nFOIL(Target-predicate, Predicates, Examples) \\nPos c those Examples for which the Target-predicate is True \\nNeg c those Examples for which the Target-predicate is False \\nwhile Pos, do \\nLearn a NewRule \\nNew Rule t the rule that predicts Target-predicate with no preconditions \\nNewRuleNeg t Neg \\nwhile NewRuleNeg, do \\nAdd a new literal to specialize New Rule \\nCandidateliterals t generate candidate new literals for NewRule, based on \\nPredicates \\nBestliteralt argmax Foil-Gain(L,NewRule) \\nLECandidateliterals \\nadd Bestliteral to preconditions of NewRule \\nNewRuleNeg c subset of NewRuleNeg that satisfies NewRule preconditions \\nLearned~ul es c Learned-rules + NewRule \\nPos t Pos - {members of Pos covered by NewRule) \\nReturn Learned-rules \\nTABLE 10.4 \\nThe basic FOIL algorithm. The specific method for generating Candidateliterals and the defini- \\n~ \\ntion of Foil-Gain are given in the text. This basic algorithm can be modified slightly to better \\naccommodate noisy data, as described in the text. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 298}, page_content=\"eralize the current disjunctive hypothesis (i.e., to increase the number of instances \\nit classifies as positive), by adding \\na,new \\ndisjunct. Viewed at this level, the search \\nis a specific-to-general search through the space of hypotheses, beginning with the \\nmost specific empty disjunction and terminating when the hypothesis is sufficiently \\ngeneral to cover all positive training examples. The inner loop of FOIL performs a \\nfiner-grained search to determine the exact definition of each new rule. This inner \\nloop searches a second hypothesis space, consisting of conjunctions of literals, to \\nfind a conjunction that will form the preconditions for the new rule. Within this \\nhypothesis space, it conducts a general-to-specific, hill-climbing search, beginning \\nwith the most general preconditions possible (the empty precondition), then adding \\nliterals one at a time to specialize the rule until it avoids all negative examples. \\nThe two most substantial differences between FOIL and our earlier \\nSEQUENTIAL-COVERING and LEARN-ONE-RULE algorithm follow from the require- \\nment that it accommodate first-order rules. These differences are: \\n1. In its general-to-specific search to 'learn each new rule, FOIL employs dif- \\nferent detailed steps to generate candidate specializations of the rule. This \\ndifference follows from the need to accommodate variables in the rule pre- \\nconditions. \\n2. FOIL employs a PERFORMANCE measure, Foil-Gain, that differs from the \\nentropy measure shown for LEARN-ONE-RULE in Table 10.2. This difference \\nfollows from the need to distinguish between different bindings of the rule \\nvariables and from the fact that FOIL seeks only rules that cover positive \\nexamples. \\nThe following two subsections consider these two differences in greater \\ndetail. \\n10.5.1 Generating Candidate Specializations in FOIL \\nTo generate candidate specializations of the current rule, FOIL generates a variety \\nof new literals, each of which may be individually added to the rule preconditions. \\nMore precisely, suppose the current rule being considered is \\nwhere L1.. . L, are literals forming the current rule preconditions and where \\nP(x1, x2, . . . , xk) is the literal that forms the rule head, or postconditions. FOIL \\ngenerates candidate specializations of this rule by considering new literals L,+I \\nthat fit one of the following forms: \\nQ(vl, . . . , v,), where Q is any predicate name occurring in Predicates and \\nwhere the vi are either new variables or variables already present in the rule. \\nAt least one of the vi in the created literal must already exist as a variable \\nin the rule. \\na Equal(xj, xk), where xi and xk are variables already present in the rule. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 299}, page_content='0 The negation of either of the above forms of literals. \\nTo illustrate, consider learning rules to predict the target literal Grand- \\nDaughter(x, \\ny), where the other predicates used to describe examples are Father \\nand Female. The general-to-specific search in FOIL begins with the most general \\nrule \\nGrandDaughter(x, y) t \\nwhich asserts that GrandDaughter(x, y) is true of any x and y. To specialize \\nthis initial rule, the above procedure generates the following literals as candi- \\ndate additions to the rule preconditions: Equal (x, y) , Female(x), Female(y), \\nFather(x, \\ny), \\nFather(y, \\nx), \\nFather(x, \\nz), \\nFather(z, \\nx), \\nFather(y, \\nz), \\nFather- \\n(z, y), and the negations of each of these literals (e.g., -Equal(x, y)). Note that \\nz is a new-variable here, whereas x and y exist already within the current rule. \\nNow suppose that among the above literals FOIL greedily selects Father- \\n(y, z) as the most promising, leading to the more specific rule \\nGrandDaughter(x, y) t Father(y, z) \\nIn generating candidate literals to further specialize this rule, FOIL will now con- \\nsider all of the literals mentioned in the previous step, plus the additional literals \\nFemale(z), Equal(z, x), Equal(z, y), Father(z, w), Father(w, z), and their nega- \\ntions. These new literals are considered at this point because the variable z was \\nadded to the rule in the previous step. Because of this, FOIL now considers an \\nadditional new variable w. \\nIf FOIL at this point were to select the literal Father(z, x) and on the \\nnext iteration select the literal Female(y), this would lead to the following rule, \\nwhich covers only positive examples and hence terminates the search for further \\nspecializations of the rule. \\nAt this point, FOIL will remove all positive examples covered by this new \\nrule. If additional positive examples remain to be covered, then it will begin yet \\nanother general-to-specific search for an additional rule. \\n10.5.2 Guiding the Search in FOIL \\nTo select the most promising literal from the candidates generated at each step, \\nFOIL considers the performance of the rule over the training data. In doing this, \\nit considers all possible bindings of each variable in the current rule. To illustrate \\nthis process, consider again the example in which we seek to learn a set of rules \\nfor the target literal GrandDaughter(x, y). For illustration, assume the training \\ndata includes the following simple set of assertions, where we use the convention \\nthat P(x, y) can be read as \"The P of x is y .\" \\nGrandDaughter(Victor, Sharon) Father(Sharon, Bob) Father(Tom, Bob) \\nFemale(Sharon) \\nFather(Bob, \\nVictor) '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 300}, page_content=\"Here let us also make the closed world assumption that any literal involving the \\npredicate GrandDaughter, Father, or Female and the constants Victor, Sharon, \\nBob, and Tom that is not listed above can be assumed to be false (i.e., we also im- \\nplicitly assert -.GrandDaughter(Tom, Bob), -GrandDaughter(Victor, Victor), \\netc.). \\nTo select the best specialization of the current rule, FOIL considers each \\ndistinct way in which the rule variables can bind to constants in the training \\nexamples. For example, in the initial step when the rule is \\nthe rule variables x and y are not constrained by any preconditions and may \\ntherefore bind in any combination to the four constants Victor, Sharon, Bob, and \\nTom. We will use the notation {x/Bob, y/Shar on} to denote a particular variable \\nbinding; that is, a substitution mapping each variable to a constant. Given the four \\npossible constants, there are 16 possible variable bindings for this initial rule. The \\nbinding {xlvictor, ylSharon} corresponds to a positive example binding, be- \\ncause the training data includes the assertion GrandDaughter(Victor, Sharon). \\nThe other 15 bindings allowed by the rule (e.g., the binding {x/Bob, y/Tom}) \\nconstitute negative evidence for the rule in the current example, because no cor- \\nresponding assertion can be found in the training data. \\nAt each stage, the rule is evaluated based on these sets of positive and neg- \\native variable bindings, with preference given to rules that possess more positive \\nbindings and fewer negative bindings. As new literals are added to the rule, the \\nsets of bindings will change. Note if a literal is added that introduces a new \\nvariable, then the bindings for the rule will grow in length \\n(e.g., \\nif Father(y, z) \\nis added to the above rule, then the original binding {xlvictor, y/Sharon) will \\nbecome the more lengthy {xlvictor, ylSharon, z/Bob}. Note also that if the new \\nvariable can bind to several different constants, then the number of bindings fitting \\nthe extended rule can be greater than the number associated with the original rule. \\nThe evaluation function used by FOIL to estimate the utility of adding a \\nnew literal is based on the numbers of positive and negative bindings covered \\nbefore and after adding the new literal. More precisely, consider some rule R, and \\na candidate literal L that might be added to the body of R. Let R' be the rule \\ncreated by adding literal L to rule R. The value Foil-Gain(L, R) of adding L to \\nR is defined as \\nP1 ) (10.1) Foil -Gain(L, R) = t - - log2 - \\nP1+ nl PO + no \\nwhere po is the number of positive bindings of rule R, no is the number of \\nnegative bindings of R, pl is the number of positive bindings of rule R', and \\nnl is the number of negative bindings of R'. Finally, t is the number of positive \\nbindings of rule R that are still covered after adding literal L to R. When a new \\nvariable is introduced into R by adding L, then any original binding is considered \\nto be covered so long as some binding extending it is present in the bindings \\nof R'. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 301}, page_content=\"This Foil-Gain function has a straightforward interpretation in terms of \\ninformation theory. According to information theory, - log2 --/& is the minimum \\nnumber of bits needed to encode the classification of an arbitrary positive binding \\namong the bindings covered by rule R. Similarly, -log2 A is the number \\nof bits required if the binding is one of those covered by rule R'. Since t is \\njust the number of positive bindings covered by R that remain covered by R', \\nFoil-Gain(L, R) can be seen as the reduction due to L in the total number of \\nbits needed to encode the classification of all positive bindings of R. \\n10.5.3 Learning Recursive Rule Sets \\nIn the above discussion, we ignored the possibility that new literals added to the \\nrule body could refer to the target predicate itself \\n(i.e., \\nthe predicate occurring \\nin the rule head). However, if we include the target predicate in the input list of \\nPredicates, then FOIL will consider it as well when generating candidate literals. \\nThis will allow it to form recursive rules-rules that use the same predicate in \\nthe body and the head of the rule. For instance, recall the following rule set that \\nprovides a recursive definition of the Ancestor relation. \\nIF Parent (x, y) THEN Ancestor(x, y) \\nIF Parent (x, z) A Ancestor(z, y) THEN Ancestor@, y) \\nGiven an appropriate set of training examples, these two rules can be learned \\nfollowing a trace similar to the one above for GrandDaughter. Note the second \\nrule is among the rules that are potentially within reach of \\nFOIL'S \\nsearch, provided \\nAncestor is included in the list Predicates that determines which predicates may \\nbe considered when generating new literals. Of course whether this particular \\nrule would be learned or not depends on whether these particular literals \\noutscore \\ncompeting candidates during \\nFOIL'S \\ngreedy search for increasingly specific rules. \\nCameron-Jones and \\nQuinlan \\n(1993) discuss several examples in which FOIL has \\nsuccessfully discovered recursive rule sets. They also discuss important subtleties \\nthat arise, such as how to avoid learning rule sets that produce infinite recursion. \\n10.5.4 Summary of FOIL \\nTo summarize, FOIL extends the sequential covering algorithm of CN2 to handle \\nthe case of learning first-order rules similar to Horn clauses. To learn each rule \\nFOIL performs a general-to-specific search, at each step adding a single new literal \\nto the rule preconditions. The new literal may refer to variables already mentioned \\nin the rule preconditions or postconditions, and may introduce new variables as \\nwell. At each step, it uses the Foil-Gain function of Equation (10.1) to select \\namong the candidate new literals. If new literals are allowed to refer to the target \\npredicate, then FOIL can, in principle, learn sets of recursive rules. While this in- \\ntroduces the complexity of avoiding rule sets that result in infinite recursion, FOIL \\nhas been demonstrated to successfully learn recursive rule sets in several cases. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 302}, page_content='In the case of noise-free training data, FOIL may continue adding new literals \\nto the rule until it covers no negative examples. To handle noisy data, the search \\nis continued until some \\ntradeoff \\noccurs between rule accuracy, coverage, and \\ncomplexity. FOIL uses a \\nminimum \\ndescription length approach to halt the growth \\nof rules, in which new literals are added only when their description length is \\nshorter than the description length of the training data they explain. The details \\nof this strategy are given in Quinlan (1990). In addition, FOIL post-prunes each \\nrule it learns, using the same rule post-pruning strategy used for decision trees \\n(Chapter 3). \\n10.6 INDUCTION AS INVERTED DEDUCTION \\nA second, quite different approach to inductive logic programming is based on \\nthe simple observation that induction is just the inverse of deduction! In general, \\nmachine learning involves building theories that explain the observed data. Given \\nsome data D and some partial background knowledge B, learning can be described \\nas generating a hypothesis h that, together with B, explains D. Put more precisely, \\nassume as usual that the training data D is a set of training examples, each of \\nthe form (xi, f (xi)). Here xi denotes the ith training instance and f (xi) denotes \\nits target value. Then learning is the problem of discovering a hypothesis h, such \\nthat the classification f (xi) of each training instance xi follows deductively from \\nthe hypothesis h, the description of \\nxi, \\nand any other background knowledge B \\nknown to the system. \\n(V(xi, f (xi)) E D) (B Ah A xi) f (xi) (10.2) \\nThe expression X F Y is read \"Y follows deductively from X,\" or alternatively \\n\"X entails Y.\" Expression (10.2) describes the constraint that must be satisfied \\nby the learned hypothesis h; namely, for every training instance xi, the target \\nclassification f (xi) must follow deductively from B, h, and xi. \\nAs an example, consider the case where the target concept to be learned is \\n\"pairs of people (u, v) such that the child of u is v,\" represented by the predicate \\nChild(u, \\nv). Assume we are given a single positive example \\nChild(Bob, \\nSharon), \\nwhere the instance is described by the literals \\nMale(Bob), \\nFemale(Sharon), \\nand \\nFather(Sharon, \\nBob). Furthermore, suppose we have the general background \\nknowledge Parent (u, v) t Father (u, v). We can describe this situation in the \\nterms of Equation (10.2) as follows: \\nxi : Male(Bob), Female(Sharon), Father(Sharon, Bob) \\nf (xi) : Child(Bob, Sharon) \\nIn this case, two of the many hypotheses that satisfy the constraint \\n(B \\nAh A xi) t- \\nf (xi) are \\nhl : Child(u, v) t Father(v, u) \\nh2 : Child(u, v) t Parent (v, u) '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 303}, page_content='Note that the target literal Child(Bob, Sharon) is entailed by hl  AX^ with no need \\nfor the background information B. In the case of hypothesis h2, however, the \\nsituation is different. The target Child(Bob, Sharon) follows from B ~h2  AX^, but \\nnot from h2  AX^ alone. This example illustrates the role of background knowledge \\nin expanding the set of acceptable hypotheses for a given set of training data. It also \\nillustrates how new predicates \\n(e.g., Parent) can be introduced into hypotheses \\n(e.g., h2), even when the predicate is not present in the original description of the \\ninstance xi. This process of augmenting the set of predicates, based on background \\nknowledge, is often referred to as constructive induction. \\nThe significance of Equation (10.2) is that it casts the learning problem in the \\nframework of deductive inference and formal logic. In the case of propositional \\nand first-order logics, there exist well-understood algorithms for automated deduc- \\ntion. Interestingly, it is possible to develop inverses of these procedures in order \\nto automate the process of inductive generalization. The insight that induction \\nmight be performed by inverting deduction appears to have been first observed \\nby the nineteenth century economist W. S. Jevons, who wrote: \\nInduction is, in fact, the inverse operation of deduction, and cannot be con- \\nceived to exist without the corresponding operation, so that the question of relative \\nimportance cannot arise. Who thinks of asking whether addition or subtraction is \\nthe more important process in arithmetic? But at the same time much difference in \\ndifficulty may exist between a direct and inverse operation; . . . it must be allowed \\nthat inductive investigations are of a far higher degree of difficulty and complexity \\nthan any questions of deduction.. . . (Jevons 1874) \\nIn the remainder of this chapter we will explore this view of induction \\nas the inverse of deduction. The general issue we will be interested in here is \\ndesigning inverse entailment operators. An inverse entailment operator, O(B, D) \\ntakes the training data D = {(xi, f (xi))} and background knowledge B as input \\nand produces as output a hypothesis h satisfying Equation (10.2). \\nO(B, D) = h such that (V(xi, f (xi)) E D) (B ~h A xi) F f (xi) \\nOf course there will, in general, be many different hypotheses h that satisfy \\n(V(X~, f (xi)) E D) (B A h A xi) F f (xi). One common heuristic in ILP for choos- \\ning among such hypotheses is to rely on the heuristic known as the Minimum \\nDescription Length principle (see Section 6.6). \\nThere are several attractive features to formulating the learning task as find- \\ning a hypothesis h that solves the relation (V(xi, f (xi)) E D) (B A h A xi) F f (xi). \\n0 This formulation subsumes the common definition of learning as finding \\nsome general concept that matches a given set of training examples (which \\ncorresponds to the special case where no background knowledge B is avail- \\nable). \\n0 By incorporating the notion of background information B, this formulation \\nallows a more rich definition of when a hypothesis may be said to \"fit\" \\nthe data. Up until now, we have always determined whether a hypothesis '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 304}, page_content='(e.g., neural network) fits the data based solely on the description of the \\nhypothesis and data, independent of the task domain under study. In contrast, \\nthis formulation allows the domain-specific background information B to \\nbecome part of the definition of \"fit.\" In particular, h fits the training example \\n(xi, f (xi)) as long as f (xi) follows deductively from B A h A xi. \\n0 By incorporating background information B, this formulation invites learning \\nmethods that use this background information to guide the search for h, \\nrather than merely searching the space of syntactically legal hypotheses. \\nThe inverse resolution procedure described in the following section uses \\nbackground knowledge in this fashion. \\nAt the same time, research on inductive logic programing following this \\nformulation has encountered several practical difficulties. \\na The requirement @\\'(xi, f (xi)) E D) (B A h A xi) t f (xi) does not naturally \\naccommodate noisy training data. The problem is that this expression does \\nnot allow for the possibility that there may be errors in the observed de- \\nscription of the instance xi or its target value f (xi). Such errors can produce \\nan inconsistent set of constraints on h. Unfortunately, most formal logic \\nframeworks completely lose their ability to distinguish between truth and \\nfalsehood once they are given inconsistent sets of assertions. \\n0 The language of first-order logic is so expressive, and the number of hy- \\npotheses that satisfy \\n(V(xi , f (xi)) E D) (B A h A xi) t f (xi) is SO large, \\nthat the search through the space of hypotheses is intractable in the general \\ncase. Much recent work has sought restricted forms of first-order expres- \\nsions, or additional second-order knowledge, to improve the tractability of \\nthe hypothesis space search. \\n0 Despite our intuition that background knowledge B should help constrain \\nthe search for a hypothesis, in most ILP systems (including all discussed \\nin this chapter) the complexity of the hypothesis space search increases as \\nbackground knowledge B is increased. (However, see Chapters 11 and 12 for \\nalgorithms that use background knowledge to decrease rather than increase \\nsample complexity). \\nIn the following section, we examine one quite general inverse entailment \\noperator that constructs hypotheses by inverting a deductive inference rule. \\n10.7 INVERTING RESOLUTION \\nA general method for automated deduction is the resolution rule introduced by \\nRobinson (1965). The resolution rule is a sound and complete rule for deductive \\ninference in first-order logic. Therefore, it is sensible to ask whether we can invert \\nthe resolution rule to form an inverse entailment operator. The answer is yes, and \\nit is just this operator that forms the basis of the CIGOL program introduced by \\nMuggleton and \\nBuntine (1988). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 305}, page_content='It is easiest to introduce the resolution rule in propositional form, though it is \\nreadily extended to first-order representations. Let L be an arbitrary propositional \\nliteral, and let P and R be arbitrary propositional clauses. The resolution rule is \\nPVL \\n-L v R \\nPVR \\nwhich should be read as follows: Given the two clauses above the line, conclude \\nthe clause below the line. Intuitively, the resolution rule is quite sensible. Given \\nthe two assertions P v L and -L v R, it is obvious that either L or -L must be \\nfalse. Therefore, either P or R must be true. Thus, the conclusion P v R of the \\nresolution rule is intuitively satisfying. \\nThe general form of the propositional resolution operator is described in \\nTable 10.5. Given two clauses C1 and C2, the resolution operator first identifies \\na literal L that occurs as a positive literal in one of these two clauses and as \\na negative literal in the other. It then draws the conclusion given by the above \\nformula. For example, consider the application of the resolution operator illustrated \\non the left side of Figure 10.2. Given clauses C1 and C2, the first step of the \\nprocedure identifies the literal L = -KnowMaterial, which is present in C1, and \\nwhose negation -(-KnowMaterial) = KnowMaterial is present in C2. Thus the \\nconclusion is the clause formed by the union of the literals C1- (L} = Pass Exam \\nand C2 - (-L} = -Study. As another example, the result of applying the resolution \\nrule to the clauses C1 = A v B v C v -D and C2 = -B v E v F is the clause \\nAvCV-DvEvF. \\nIt is easy to invert the resolution operator to form an inverse entailment \\noperator O(C, C1) that performs inductive inference. In general, the inverse en- \\ntailment operator must derive one of the initial clauses, C2, given the resolvent C \\nand the other initial clause C1. Consider an example in which we are given the \\nresolvent C = A v B and the initial clause C1 = B v D. How can we derive a \\nclause C2 such that C1 A C2 F C? First, note that by the definition of the resolution \\noperator, any literal that occurs in C but not in C1 must have been present in C2. \\nIn our example, this indicates that C2 must contain the literal A. Second, the literal \\n1. Given initial clauses C1 and C2, find a literal L from clause C1 such that -L occurs in clause C2. \\n2. Form the resolvent C by including all literals from C1 and C2, except for L and -L. More \\nprecisely, the set of literals occurring in the conclusion C is \\nwhere u denotes set union, and \"-\" denotes set difference. \\nTABLE 10.5 \\nResolution operator (propositional form). Given clauses C1 and C2, the resolution operator constructs \\na clause C such that \\nC1 A C2 k C. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 306}, page_content='C : KnowMaterial v -Study C: KnowMaterial V 7SNdy \\nC : Passh v ~KnawMaferial C : PIISS~ V 1KnowMafcrial I I \\nFIGURE 10.2 \\nOn the left, an application of the (deductive) resolution rule inferring clause C from the given clauses \\nC1 and C2. On the right, an application of its (inductive) inverse, inferring Cz from C and C1. \\nthat occurs in C1 but not in C must be the literal removed by the resolution rule, \\nand therefore its negation must occur in C2. In our example, this indicates that C2 \\nmust contain the literal -D. Hence, C:! = A v -D. The reader can easily verify \\nthat applying the resolution rule to C1 and C2 does, in fact, produce the desired \\nresolvent C. \\nNotice there is a second possible solution for C2 in the above example. In \\nparticular, C2 can also be the more specific clause A v -D v B. The difference \\nbetween this and our first solution is that we have now included in C2 a lit- \\neral that occurred in C1. The general point here is that inverse resolution is not \\ndeterministic-in general there may be multiple clauses C2 such that C1 and C2 \\nproduce the resolvent C. One heuristic for choosing among the alternatives is to \\nprefer shorter clauses over longer clauses, or equivalently, to assume C2 shares no \\nliterals in common with C1. If we incorporate this bias toward short clauses, the \\ngeneral statement of this inverse resolution procedure is as shown in Table 10.6. \\nWe can develop rule-learning algorithms based on inverse entailment op- \\nerators such as inverse resolution. In particular, the learning algorithm can use \\ninverse entailment to construct hypotheses that, together with the background \\ninformation, entail the training data. One strategy is to use a sequential cover- \\ning algorithm to iteratively learn a set of Horn clauses in this way. On each \\niteration, the algorithm selects a training example (xi, f (xi)) that is not yet cov- \\nered by previously learned clauses. The inverse resolution rule is then applied to \\n- -- - - \\n1. Given initial clauses C1 and C, find a literal L that occurs in clause C1, but not in clause C. \\n2. Form the second clause Cz by including the following literals \\nTABLE 10.6 \\nInverse resolution operator (propositional form). Given two clauses C and Cl. this computes a clause \\nC2 such that C1 A Cz I- C. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 307}, page_content='generate candidate hypotheses hi that satisfy (B A hi A xi) I- f (xi), where B is the \\nbackground knowledge plus any clauses learned on previous iterations. Note this \\nis an example-driven search, because each candidate hypothesis is constructed to \\ncover a particular example. Of course if multiple candidate hypotheses exist, then \\none strategy for selecting among them is to choose the one with highest accuracy \\nover the other examples as well. The \\nCIGOL \\nprogram uses inverse resolution with \\nthis kind of sequential covering algorithm, interacting with the user along the \\nway to obtain training examples and to obtain guidance in its search through the \\nvast space of possible inductive inference steps. However, \\nCIGOL \\nuses first-order \\nrather than propositional representations. Below we describe the extension of the \\nresolution rule required to accommodate first-order representations. \\n10.7.1 First-Order Resolution \\nThe resolution rule extends easily to first-order expressions. As in the propositional \\ncase, it takes two clauses as input and produces a third clause as output. The key \\ndifference from the propositional case is that the process is now based on the \\nnotion of unifying substitutions. \\nWe define a substitution to be any mapping of variables to terms. For ex- \\nample, the substitution 6 = {x/Bob, y/z} indicates that the variable x is to be \\nreplaced by the term Bob, and that the variable y is to be replaced by the term \\nz. We use the notation WO to denote the result of applying the substitution 6 to \\nsome expression W. For example, if L is the literal Father(x, Bill) and 6 is the \\nsubstitution defined above, then LO = Father(Bob, Bill). \\nWe say that 6 is a unifying substitution for two literals L1 and L2, provided \\nLlO = L2O. For example, if L1 = Father(x, y), L2 = Father(Bil1, z), and O = \\n(x/Bill, z/y}, then 6 is a unifying substitution for L1 and L2 because LlO = \\nL2O = Father(Bil1, y). The significance of a unifying substitution is this: In the \\npropositional form of resolution, the resolvent of two clauses C1 and C2 is found \\nby identifying a literal L that appears in C1 such that -L appears in C2. In first- \\norder resolution, this generalizes to finding one literal L1 from clause C1 and one \\nliteral L2 from C2, such that some unifying substitution 6 can be found for L1 \\nand -L2 (i.e., such that LIO = -L20). The resolution rule then constructs the \\nresolvent C according to the equation \\nThe general statement of the resolution rule is shown in Table 10.7. To \\nillustrate, suppose C1 = White(x) t Swan(x) and suppose C2 = Swan(Fred). \\nTo apply the resolution rule, we first re-express C1 in clause form as the equivalent \\nexpression C1 = White(x) v -Swan(x). The resolution rule can now be applied. \\nIn the first step, it finds the literal L1 = -Swan(x) from C1 and the literal L2 = \\nSwan(Fred) from C2. If we choose the unifying substitution O = {x/Fred} then \\nthese two literals satisfy LIB = -L20 = -Swan(Fred). Therefore, the conclusion \\nC is the union of (C1 - {L1})O = White(Fred) and (C2 - {L2})0 = 0, or C = \\nWhite(Fred). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 308}, page_content='CHAPTER 10 LEARNING SETS OF RULES 2!)7 \\n1. Find a literal L1 from clause C1, literal Lz from clause Cz, and substitution 0 such that LIB = \\n-L28. \\n2. Form the resolvent C by including all literals from CIB and C28, except for L1 B and -L2B. More \\nprecisely, the set of literals occurring in the conclusion C is \\nc = (Cl - (L11)O lJ (C2 - ILzI)@ \\nTABLE 10.7 \\nResolution operator (first-order form). \\n10.7.2 Inverting Resolution: First-Order Case \\nWe can derive the inverse resolution operator analytically, by algebraic manipula- \\ntion of Equation (10.3) which defines the resolution rule. First, note the unifying \\nsubstitution \\n8 \\nin Equation (10.3) can be uniquely factored into \\n81 \\nand \\n82, \\nwhere \\n0 = Ole2, where contains all substitutions involving variables from clause C1, \\nand where O2 contains all substitutions involving variables from C2. This factor- \\nization is possible because \\nC1 \\nand \\nC2 \\nwill always begin with distinct variable \\nnames (because they are distinct universally quantified statements). Using this \\nfactorization of 8, we can restate Equation (10.3) as \\nKeep in mind that \"-\" here stands for set difference. Now if we restrict inverse \\nresolution to infer only clauses \\nC2 \\nthat contain no literals in common with \\nC1 \\n(corresponding to a preference for shortest \\nC2 \\nclauses), then we can re-express \\nthe above as \\nc - (Cl - {LlHel = (C2 - IL2W2 \\nFinally we use the fact that by definition of the resolution rule \\nL2 = -~1818;\\', \\nand solve for \\nC2 \\nto obtain \\nInverse resolution: \\ncz = (c - (CI - {~~~)e~)e,-l u {-~,e~e;\\'~ (10.4) \\nEquation (10.4) gives the inverse resolution rule for first-order logic. As in the \\npropositional case, this inverse entailment operator is nondeterministic. In partic- \\nular, in applying it we may in general find multiple choices for the clause \\nCr \\nto \\nbe resolved and for the unifying substitutions and \\n82. \\nEach set of choices may \\nyield a different solution for C2. \\nFigure 10.3 illustrates a multistep application of this inverse resolution rule \\nfor a simple example. In this figure, we wish to learn rules for the target predicate \\nGrandChild(y, x), given the training data D = GrandChild(Bob, Shannon) and \\nthe background information B = {Father (Shannon, Tom), Father (Tom, Bob)). \\nConsider the bottommost step in the inverse resolution tree of Figure 10.3. Here, \\nwe set the conclusion C to the training example GrandChild(Bob, Shannon) '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 309}, page_content=\"GrandChild(Bob, Shannon) \\nFather (Shannon, Tom) \\nFIGURE 10.3 \\nA multistep inverse resolution. In each case, the boxed clause is the result of the inference step. For \\neach step, C is the clause at the bottom, C1 the clause to the left, and C2 the boxed clause to the \\nright. In both inference steps here, el is the empty substitution (1, and 0;' is the substitution shown \\nbelow C2. Note the final conclusion (the boxed clause at the top right) is the alternative form of the \\nHorn clause GrandChild(y, x) c Father(x, z) A Father(z, y). \\nGrandChild(Bob,x) v Father(x,Tom) I \\nand select the clause C1 = Father(Shannon, Tom) from the background in- \\nformation. To apply the inverse resolution operator we have only one choice \\nfor the literal L1, namely Father(Shannon, Tom). Suppose we choose the in- \\nverse substitutions 9;' = {} and 9;' = {Shannon/x}. In this case, the result- \\ning clause C2 is the union of the clause (C - (C1 - {Ll})91)9;~ = (~91)9;' \\n= GrandChild(Bob, x), and the clause {-~~9~9,') = -.Father(x, Tom). Hence \\nthe result is the clause GrandChild(Bob, x) v -Father(x, Tom), or equivalently \\n(GrandChild(Bob, x) t Father(x, Tom)). Note this general rule, together with \\nC1 entails the training example GrandChild(Bob, Shannon). \\nIn similar fashion, this inferred clause may now be used as the conclusion \\nC for a second inverse resolution step, as illustrated in Figure 10.3. At each such \\nstep, note there are several possible outcomes, depending on the choices for the \\nsubstitutions. (See Exercise 10.7.) In the example of Figure 10.3, the particular set \\nof choices produces the intuitively satisfying final clause GrandChild(y, x) t \\nFather(x, 2) A Father(z, y). \\n10.7.3 Summary of Inverse Resolution \\nTo summarize, inverse resolution provides a general approach to automatically \\ngenerating hypotheses h that satisfy the constraint (B A h A xi) t- f (xi). This is \\naccomplished by inverting the general resolution rule given by Equation (10.3). \\nBeginning with the resolution rule and solving for the clause C2, the inverse \\nresolution rule of Equation (10.4) is easily derived. \\nGiven a set of beginning clauses, multiple hypotheses may be generated by \\nrepeated application of this inverse resolution rule. Note the inverse resolution rule \\nhas the advantage that it generates only hypotheses that satisfy (B ~h  AX^) t- f (xi). \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 310}, page_content='In contrast, the generate-and-test search of FOIL generates many hypotheses at \\neach search step, including some that do not satisfy this constraint. FOIL then \\nconsiders the data D to choose among these hypotheses. Given this difference, \\nwe might expect the search based on inverse resolution to be more focused and \\nefficient. However, this will not necessarily be the case. One reason is that the \\ninverse resolution operator can consider only a small fraction of the available \\ndata when generating its hypothesis at any given step, whereas FOIL considers \\nall available data to select among its syntactically generated hypotheses. The \\ndifferences between search strategies that use inverse entailment and those that \\nuse generate-and-test search is a subject of ongoing research. Srinivasan et al. \\n(1995) provide one experimental comparison of these two approaches. \\n10.7.4 Generalization, 8-Subsumption, and Entailment \\nThe previous section pointed out the correspondence between induction and in- \\nverse entailment. Given our earlier focus on using the general-to-specific ordering \\nto organize the hypothesis search, it is interesting to consider the relationship be- \\ntween the \\nmore-general~han \\nrelation and inverse entailment. To illuminate this \\nrelationship, consider the following definitions. \\n0 more-general-than. In Chapter 2, we defined the more_general_than_or- \\nequal20 \\nrelation (z,) as follows: Given two boolean-valued functions hj(x) \\nand \\nhk(x), \\nwe say that \\nhj 2, hk if and only if (Vx)hk(x) + hj(x). This >, \\nrelation is used by many learning algorithms to guide search through the \\nhypothesis space. \\n0 8-subsumption. Consider two clauses Cj and Ck, both of the form H v L1 v \\n. . . L,, where H is a positive literal, and the Li are arbitrary literals. Clause \\nCj \\nis said to 8-subsume clause \\nCk \\nif and only if there exists a substitution \\n0 such that CjO G Ck (where we here describe any clause C by the set of \\nliterals in its disjunctive form). This definition is due to \\nPlotkin (1970). \\n0 Entailment. Consider two clauses Cj and Ck. Clause Cj is said to entail \\nclause \\nCk \\n(written \\nCj k Ck) if and only if Ck follows deductively from C,. \\nWhat is the relationship among these three definitions? First, let us re-express \\nthe definition of 2, using the same first-order notation as the other two definitions. \\nIf we consider a boolean-valued hypothesis \\nh(x) \\nfor some target concept \\nc(x), \\nwhere \\nh(x) \\nis expressed by a conjunction of literals, then we can re-express the \\nhypothesis as the clause \\nHere we follow the usual PROLOG interpretation that x is classified a negative \\nexample if it cannot be proven to be a positive example. Hence, we can see that \\nour earlier definition of 1, applies to the preconditions, or bodies, of Horn clauses. \\nThe implicit postcondition of the Horn clause is the target concept \\nc(x). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 311}, page_content='What is the relationship between this definition of 2, and the definition \\nof 8-subsumption? Note that if hl p, h2, then the clause C1 : c(x) t hl(x) \\n8-subsumes the clause C2 : c(x) t h2(x). Furthermore, 8-subsumption can hold \\neven when the clauses have different heads. For example, clause A 8-subsumes \\nclause B in the following case: \\nA : Mother(x, y) t Father(x, z) A Spouse(z, y) \\nB : Motker(x, Louise) t Father(x, Bob) A Spouse(Bob, y) A Female@) \\nbecause A8 G B if we choose 8 = {ylLouise, zlBob). The key difference here is \\nthat >, implicitly assumes two clauses for which the heads are the same, whereas \\n8-subsumption can hold even for clauses with different heads. \\nFinally, 8-subsumption is a special case of entailment. That is, if clause A \\n8-subsumes clause B, then A k B. However, we can find clauses A and B such \\nthat A F B, but where A does not 8-subsume B. One example is the following \\npair of clauses \\nA : Elephant(father_of (x)) t Elephant (x) \\nB : Elephant (f athersf (f ather-f (y))) t Elephant (y) \\nwhere f ather-of (x) is a function that refers to the individual who is the father \\nof x. Note that although B can be proven from A, there is no substitution 8 that \\nallows B to be &subsumed by A. \\nAs shown by these examples, our earlier notion of more-general~han is a \\nspecial case of 8-subsumption, which is itself a special case of entailment. There- \\nfore, searching the hypothesis space by generalizing or specializing hypotheses \\nis more limited than searching by using general inverse entailment operators. \\nUnfortunately, in its most general form, inverse entailment produces intractable \\nsearches. However, the intermediate notion of 8-subsumption provides one conve- \\nnient notion that lies midway between our earlier definition of more-general~han \\nand entailment. \\nAlthough inverse resolution is an intriguing method for generating candidate hy- \\npotheses, in practice it can easily lead to a combinatorial explosion of candidate \\nhypotheses. An alternative approach is to use inverse entailment to generate just \\nthe single most specific hypothesis that, together with the background informa- \\ntion, entails the observed data. This most specific hypothesis can then be used \\nto bound a general-to-specific search through the hypothesis space similar to that \\nused by FOIL, but with the additional constraint that the only hypotheses consid- \\nered are hypotheses more general than this bound. This approach is employed by \\nthe PROGOL system, whose algorithm can be summarized as follows: \\n1. The user specifies a restricted language of first-order expressions to be used \\nas the hypothesis space H. Restrictions are stated using \"mode declarations,\" '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 312}, page_content='which enable the user to specify the predicate and function symbols to be \\nconsidered, and the types and formats of arguments for each. \\n2. PROGOL uses a sequential covering algorithm to learn a set of expressions \\nfrom H that cover the data. For each example (xi, f (xi)) that is not yet \\ncovered by these learned expressions, it first searches for the most specific \\nhypothesis hi within H such that (B A hi A xi) l- f (xi). More precisely, it \\napproximates this by calculating the most specific hypothesis among those \\nthat entail f (xi) within k applications of the resolution rule (where k is a \\nuser-specified parameter). \\n3. PROGOL then performs a general-to-specific search of the hypothesis space \\nbounded by the most general possible hypothesis and by the specific bound \\nhi calculated in step 2. Within this set of hypotheses, it seeks the hypothesis \\nhaving minimum description length (measured by the number of literals). \\nThis part of the search is guided by an A*-like heuristic that allows pruning \\nwithout running the risk of pruning away the shortest hypothesis. \\nThe details of the PROGOL algorithm are described by Muggleton (1992, \\n1995). \\n10.8 SUMMARY AND FURTHER READING \\nThe main points of this chapter include: \\nThe sequential covering algorithm learns a disjunctive set of rules by first \\nlearning a single accurate rule, then removing the positive examples covered \\nby this rule and iterating the process over the remaining training examples. \\nIt provides an efficient, greedy algorithm for learning rule sets, and an al- \\nternative to top-down decision tree learning algorithms such as ID3, which \\ncan be viewed as simultaneous, rather than sequential covering algorithms. \\n0 In the context of sequential covering algorithms, a variety of methods have \\nbeen explored for learning a single rule. These methods vary in the search \\nstrategy they use for examining the space of possible rule preconditions. One \\npopular approach, exemplified by the CN2 program, is to conduct a \\ngeneral- \\nto-specific beam search, generating and testing progressively more specific \\nrules until a sufficiently accurate rule is found. Alternative approaches search \\nfrom specific to general hypotheses, use an example-driven search rather than \\ngenerate and test, and employ different statistical measures of rule accuracy \\nto guide the search. \\nSets of first-order rules (i.e., rules containing variables) provide a highly \\nexpressive representation. For example, the programming language PROLOG \\nrepresents general programs using collections of first-order Horn clauses. \\nThe problem of learning first-order Horn clauses is therefore often referred \\nto as the problem of inductive logic programming. \\nOne approach to learning sets of first-order rules is to extend the sequential \\ncovering algorithm of CN2 from propositional to first-order representations. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 313}, page_content='This approach is exemplified by the FOIL program, which can learn sets of \\nfirst-order rules, including simple recursive rule sets. \\n0 A second approach to learning first-order rules is based on the observation \\nthat induction is the inverse of deduction. In other words, the problem of \\ninduction is to find a hypothesis h that satisfies the constraint \\nwhere B is general background information, XI. . . x, are descriptions of the \\ninstances in the training data D, and f (XI). . . f (x,) are the target values of \\nthe training instances. \\n0 Following the view of induction as the inverse of deduction, some programs \\nsearch for hypotheses by using operators that invert the well-known opera- \\ntors for deductive reasoning. For example, \\nCIGOL \\nuses inverse resolution, an \\noperation that is the inverse of the deductive resolution operator commonly \\nused for mechanical theorem proving. PROGOL combines an inverse entail- \\nment strategy with a general-to-specific strategy for searching the hypothesis \\nspace. \\nEarly work on learning relational descriptions includes \\nWinston\\'s \\n(1970) \\nwell-known program for learning network-style descriptions for concepts such \\nas \"arch.\" Banerji7s (1964, 1969) work and \\nMichalski7s \\nseries of AQ programs \\n(e.g., \\nMichalski \\n1969; \\nMichalski \\net al. 1986) were among the earliest to ex- \\nplore the use of logical representations in learning. Plotkin\\'s (1970) definition of \\n8-subsumption provided an early formalization of the relationship between induc- \\ntion and deduction. Vere (1975) also explored learning logical representations, \\nand Buchanan\\'s (1976) \\nMETA-DENDRAL \\nprogram learned relational descriptions \\nrepresenting molecular substructures likely to fragment in a mass spectrometer. \\nThis program succeeded in discovering useful rules that were subsequently pub- \\nlished in the chemistry literature. Mitchell\\'s (1979) CANDIDATE-ELIMINATION ver- \\nsion space algorithm was applied to these same relational descriptions of chemical \\nstructures. \\nWith the popularity of the PROLOG language in the mid-1980~~ researchers \\nbegan to look more carefully at learning relational descriptions represented by \\nHorn clauses. Early work on learning Horn clauses includes Shapiro\\'s (1983) \\nMIS and Sammut and Banerji\\'s (1986) MARVIN. Quinlan7s (1990) FOIL algo- \\nrithm, discussed here, was quickly followed by a number of algorithms employ- \\ning a general-to-specific search for first-order rules including MFOIL (Dieroski \\n1991), \\nFOCL (Pazzani et al. 1991), CLAUDIEN (De Raedt and Bruynooghe \\n1993), \\nand \\nMARKUS \\n(Grobelnik \\n1992). The FOCL algorithm is described in \\nChapter 12. \\nAn alternative line of research on learning Horn clauses by inverse entail- \\nment was spurred by Muggleton and \\nBuntine \\n(1988), \\nwho built on related ideas \\nby Sammut and Banerji (1986) and Muggleton (1987). More recent work along \\nthis line has focused on alternative search strategies and methods for constraining \\nthe hypothesis space to make learning more tractable. For example, \\nKietz \\nand '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 314}, page_content='Wrobel (1992) use rule schemata in their RDT program to restrict the form of \\nexpressions that may be considered, during learning, and Muggleton and Feng \\n(1992) discuss the restriction of first-order expressions to ij-determinate literals. \\nCohen (1994) discusses the \\nGRENDEL \\nprogram, which accepts as input an ex- \\nplicit description of the language for describing the clause body, thereby allowing \\nthe user to explicitly constrain the hypothesis space. \\nLavraC \\nand \\nDZeroski \\n(1994) provide a very readable textbook on inductive \\nlogic programming. Other useful recent monographs and edited collections include \\n(Bergadano and Gunetti 1995; Morik et al. 1993; Muggleton 1992, 1995b). The \\noverview chapter by \\nWrobel(1996) \\nalso provides a good perspective on the field. \\nBratko and Muggleton (1995) summarize a number of recent applications of ILP \\nto problems of practical importance. A series of annual workshops on ILP provides \\na good source of recent research papers \\n(e.g., \\nsee De Raedt 1996). \\nEXERCISES \\n10.1. Consider a sequential covering algorithm such as CN2 and a simultaneous covering \\nalgorithm such as ID3. Both algorithms are to be used to learn a target concept \\ndefined over instances represented by conjunctions of n boolean attributes. If ID3 \\nlearns a balanced decision tree of depth d, it will contain 2d - 1 distinct decision \\nnodes, and therefore will have made \\n2d - 1 distinct choices while constructing its \\noutput hypothesis. How many rules will be formed if this tree is re-expressed as \\nt a disjunctive set of rules? How many preconditions will each ru?e possess? How \\nmany distinct choices would a sequential covering algorithm have to make to learn \\nthis same set of rules? Which system do you suspect would be more prone to \\noverfitting \\nif both were given the same training data? \\n10.2. Refine the LEARN-ONE-RULE algorithm of Table 10.2 so that it can learn rules whose \\npreconditions include thresholds on real-valued attributes \\n(e.g., \\ntemperature > \\n42). Specify your new algorithm as a set of editing changes to the algorithm of \\nTable 10.2. Hint: Consider how this is accomplished for decision tree learning. \\n10.3. Refine the LEARN-ONE-RULE algorithm of Table 10.2 so that it can learn rules whose \\npreconditions include constraints such as nationality E {Canadian, Brazilian}, \\nwhere a discrete-valued attribute is allowed to take on any value in some specified \\nset. Your modified program should explore the hypothesis space containing all such \\nsubsets. Specify your new algorithm as a set of editing changes to the algorithm \\nof Table 10.2. \\n10.4. Consider the options for implementing LEARN-ONE-RULE in terms of the possible \\nstrategies for searching the hypothesis space. In particular, consider the following \\nattributes of the search \\n(a) generate-and-test versus data-driven \\n(b) general-to-specific versus specific-to-general \\n(c) sequential cover versus simultaneous cover \\nDiscuss the benefits of the choice made by the algorithm in Tables 10.1 and \\n10.2. For each of these three attributes of the search strategy, discuss the (positive \\nand negative) impact of choosing the alternative option. \\n10.5. Apply inverse resolution in propositional form to the clauses C = A v B, C1 = \\nA v B v G. Give at least two possible results for CZ. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 315}, page_content=\"10.6. Apply inverse resolution to the clauses C = R(B, x) v P(x, A) and CI = S(B, y) v \\nR(z, x). Give at least four possible results for C2. Here A and B are constants, x \\nand y are variables. \\n10.7. Consider the bottom-most inverse resolution step in Figure 10.3. Derive at least \\ntwo different outcomes that could result given different choices for the substi- \\ntutions el and 02. Derive a result for the inverse resolution step if the clause \\nFather(Tom, Bob) is used in place of Father(Shannon, Tom). \\n10.8. Consider the relationship between the definition of the induction problem in this \\nchapter \\nand our earlier definition of inductive bias from Chapter 2, Equation 2.1. There we \\ndefined the inductive bias, Bbias, by the expression \\nwhere L(xi, D) is the classification that the learner assigns to the new instance xi \\nafter learning from the training data D, and where X is the entire instance space. \\nNote the first expression is intended to describe the hypothesis we wish the learner \\nto output, whereas the second expression is intended to describe the learner's policy \\nfor generalizing beyond the training data. Invent a learner for which the inductive \\nbias Bbias of the learner is identical to the background knowledge B that it is \\nprovided. \\nREFERENCES \\nBanerji, R. (1964). A language for the description of concepts. General Systems, 9, 135-141. \\nBane rji, R. (1969). Theory of problem solving-an approach to artijicial intelligence. New York: \\nAmerican \\nElsevier \\nPublishing Company. \\nBergadano, F., & Gunetti, D. (1995). Inductive logic programming: From machine learning to soft- \\nware engineering. Cambridge, Ma: MIT Press. \\nBratko, I., & Muggleton, S. (1995). Applications of inductive logic programming. Communications \\nof \\nthe ACM, 38(1 I), 65-70. \\nBuchanan, B. G., Smith, D. H., White, W. C., Gritter, R., Feigenbaum, E. A., Lederberg, J., & \\nDjerassi, C. (1976). Applications of artificial intelligence for chemical inference, XXII: Auto- \\nmatic rule formation in mass spectrometry by means of the \\nmeta-DENDRAL \\nprogram. Journal \\nof the American Chemical Society, 98, 6168. \\nBuntine, W. (1986). Generalised subsumption. Proceedings of the European Conference on Artijicial \\nIntelligence, London. \\nBuntine, \\nW. (1988). Generalized subsumption and its applications to induction and redundancy. \\nArtificial Intelligence, 36, 149-176. \\nCameron-Jones, R., & Quinlan, J. R. (1993). Avoiding pitfalls when learning recursive theories. \\nProceedings of the Eighth International Workshop on Machine Learning (pp 389-393). San \\nMatw, CA: Morgan Kaufmann. \\nCestnik, \\nB., & Bratko, I. (1991). On estimating probabilities in tree pruning. Proceedings of the \\nEuropean Working Session on Machine Learning @p. 138-150). Porto, Portugal. \\nClark, P., & Niblett, R. (1989). The CN2 induction algorithm. Machine Learning, 3, 261-284. \\nCohen, W. (1994). Grammatically biased learning: Learning logic programs using an explicit an- \\ntecedent description language. ArtGcial Intelligence, 68(2), 303-366. \\nDe Raedt, L. (1992). Interactive theory revision: An inductive logic programming approach. London: \\nAcademic Press. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 316}, page_content=\"De Raedt, L., & Bruynooghe, M. (1993). A theory of clausal discovery. Proceedings of the Thirteenth \\nInternational Joint Conference on \\nArtGcial \\nIntelligence. San Mateo, CA: Morgan Kaufmann. \\nDe Raedt, L. (Ed.). (1996). \\nAdvancm \\nin inductive logic programming: Proceedings of the \\nFifh \\nIn- \\nternational Workshop on Inductive Logic Programming. Amsterdam: IOS Press. \\nDolsak, B., & Muggleton, S. (1992). The application of inductive logic programming to finite element \\nmesh design. In S. Muggleton (Ed.), Inductive Logic Programming. London: Academic Press. \\nDZeroski, \\nS. (1991). Handling noise in inductive logic programming (Master's thesis). Electrical \\nEngineering and Computer Science, University of Ljubljana, Ljubljana, Slovenia. \\nFlener, P. (1994). Logic program synthesis from incomplete information. The Kluwer international \\nseries in engineering and computer science. Boston: Kluwer Academic Publishers. \\nGrobelnik, M. (1992). \\nMARKUS: An optimized model inference system. Proceedings of the Work- \\nshop on Logical Approaches to Machine Learning, Tenth European Conference on \\nAI, \\nVienna, \\nAustria. \\nJevons, W. S. (1874). The principles of science: A treatise on logic and \\nscientijc \\nmethod. London: \\nMacmillan. \\nKietz, \\nJ-U., & Wrobel, S. (1992). Controlling the complexity of learning in logic through syntactic and \\ntask-oriented models. In S. Muggleton (Ed.), Inductive logic programming. London: Academic \\nPress. \\nLavraE, \\nN., & Dieroski, S. (1994). Inductive logicprogramming: Techniques and applications. Ellis \\nHorwood. \\nLindsay, R. K., Buchanan, B. G., Feigenbaurn, E. A., & Lederberg, J. (1980). Applications of art@cial \\nintelligence for organic chemistry. New York: McGraw-Hill. \\nMichalski, R. S., (1969). On the quasi-minimal solution of the general covering problem. Proceed- \\nings of the First International Symposium on Information Processing (pp. 125-128). Bled, \\nYugoslavia. \\nMichalski, \\nR. S., Mozetic, I., Hong, J., and Lavrac, H. (1986). The multi-purpose incremental learning \\nsystem \\nAQ15 \\nand its testing application to three medical domains. Proceedings of the Fifh \\nNational Conference on \\nA1 \\n(pp. 1041-1045). Philadelphia: Morgan-Kaufmann. \\nMitchell, T. M. (1979). Version spaces: An approach to concept learning (Ph.D. dissertation). Elec- \\ntrical Engineering Dept., Stanford University, Stanford, CA. \\nMorik, K., Wrobel, S., Kietz, J.-U., & Emde, W. (1993). Knowledge acquisition and machine learning: \\nTheory, methods, and applications. London: Academic Press. \\nMuggleton, S. (1987). DUCE: An oracle based approach to constructive induction. Proceedings of the \\nInternational Joint Conference on AI \\n@p. \\n287-292). San Mateo, CA: Morgan Kaufmann. \\nMuggleton, S. \\n(1995a). \\nInverse entailment and PROGOL. New Generation Computing, 13, 245-286. \\nMuggleton, S. \\n(1995b). \\nFoundations of inductive logic programming. Englewood Cliffs, NJ: Prentice \\nHall. \\nMuggleton, S., & Buntine, W. (1988). Machine invention of first-order predicates by inverting res- \\nolution. Proceedings of the Fzfth International Machine Learning Conference (pp. 339-352). \\nAnn Arbor, Michigan: Morgan Kaufmann. \\nMuggleton, S., & Feng, C. (1990). Efficient induction of logic programs. Proceedings of the First \\nConference on Algorithmic Learning Theory. \\nOhrnsha, \\nTokyo. \\nMuggleton, S., & Feng, C. (1992). Efficient induction of logic programs. In Muggleton (Ed.), Induc- \\ntive logic programming. London: Academic Press. \\nMuggleton, S. (Ed.). (1992). Inductive logic programming. London: Academic Press. \\nPazzani, \\nM., \\nBrunk, \\nC., & Silverstein, G. (1991). A knowledge-intensive approach to learning rela- \\ntional concepts. Proceedings of the Eighth International Workshop on Machine Learning (pp. \\n432-436). \\nSan Francisco: Morgan Kaufmann. \\nPlotkin, \\nG. D. (1970). A note on inductive generalization. In B. Meltzer & D. Michie (Eds.), Machine \\nIntelligence 5 (pp. 153-163). Edinburgh University Press. \\nPlotkin, \\nG. D. (1971). A further note on inductive generalization. In B. Meltzer & D. Michie (Eds.), \\nMachine Intelligence 6. New York: Elsevier. \\nQuinlan, J. R. (1990). Learning logical definitions from relations. Machine Learning, 5, 239-266. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 317}, page_content='Quinlan, J. R. (1991). Improved estimates for the accuracy of small disjuncts (Technical Note). \\nMachine Learning, 6(1), 93-98. Boston: Kluwer Academic Publishers. \\nRivest \\nR. L. (1987). Learning decision lists. Machine Learning, 2(3), 229-246. \\nRobinson, J. A. (1965). A machine-oriented logic based on the resolution principle. Journal of the \\nACM, 12(1), 23-41. \\nSammut, C. A. (1981). Concept learning by experiment. Seventh International Joint Conference on \\nArtijicial \\nIntelligence, Vancouver. \\nSammut, \\nC. A., & Banerji, R. B. (1986). Learning concepts by asking questions. In R. S. Michalski, \\nJ. G. Carbonell, & T. M. Mitchell (Eds.), Machine learning: An art$cial intelligence approach \\n(Vol 2, pp. 167-192). Los Altos, California: Morgan Kaufmann. \\nShapiro, E. (1983). Algorithmic program debugging. Cambridge MA: MIT Press. \\nSrinivasan, A., Muggleton, S., & King, R. D. (1995). Comparing the use of background knowl- \\nedge by inductive logic programming systems (PRG Technical report PRG-TR-9-95). Oxford \\nUniversity Computing Laboratory. \\nSrinivasan, \\nA,, \\nMuggleton, S., King, R. D., & Stemberg, M. J. E. (1994). Mutagenesis: ILP ex- \\nperiments in a non-determinate biological domain. Proceedings of the Fourth Inductive Logic \\nProgramming Workshop. \\nVere, S. (1975). Induction of concepts in the predicate calculus. Proceedings of the Fourth Intem- \\ntional Joint Conference on \\nArtijicial \\nIntelligence (pp. 351-356). \\nWinston, P. (1970). Learning structural descriptions from examples (Ph.D. dissertation) (MIT Tech- \\nnical Report AI-TR-231). \\nWrobel, S. (1994). Concept formation and knowledge revision. Boston: Kluwer Academic Publishers. \\nWrobel, S. (1996). Inductive logic programming. In G. Brewka (Ed)., Principles of knowledge rep- \\nresentation. Stanford, CA: CSLI Publications. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 318}, page_content=\"CHAPTER \\nANALYTICAL \\nLEARNING \\nInductive learning methods such as neural network and decision tree learning require \\na certain number of training examples to achieve a given level of generalization ac- \\ncuracy, as reflected in the theoretical bounds and experimental results discussed in \\nearlier chapters. Analytical learning uses prior knowledge and deductive reasoning to \\naugment the information provided by the training examples, so that it is not subject \\nto these same bounds. This chapter considers an analytical learning method called \\nexplanation-based learning (EBL). In explanation-based learning, prior knowledge \\nis used to analyze, or explain, how each observed training example satisfies the \\ntarget concept. This explanation is then used to distinguish the relevant features \\nof the training example from the irrelevant, so that examples can be generalized \\nbased on logical rather than statistical reasoning. Explanation-based learning has \\nbeen successfully applied to learning search control rules for a variety of planning \\nand scheduling tasks. This chapter considers explanation-based learning when the \\nlearner's prior knowledge is correct and complete. The next chapter considers com- \\nbining inductive and analytical learning in situations where prior knowledge is only \\napproximately correct. \\n11.1 INTRODUCTION \\nPrevious chapters have considered a variety of inductive learning methods: that is, \\nmethods that generalize from observed training examples by identifying features \\nthat empirically distinguish positive from negative training examples. Decision \\ntree learning, neural network learning, inductive logic programming, and genetic \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 319}, page_content='algorithms are all examples of inductive methods that operate in this fashion. The \\nkey practical limit on these inductive learners is that they perform poorly when \\ninsufficient data is available. In fact, as discussed in Chapter 7, theoretical analysis \\nshows that there are fundamental bounds on the accuracy that can be achieved \\nwhen learning inductively from a given number of training examples. \\nCan we develop learning methods that are not subject to these fundamental \\nbounds on learning accuracy imposed by the amount of training data available? \\nYes, if we are willing to reconsider the formulation of the learning problem itself. \\nOne way is to develop learning algorithms that accept explicit prior knowledge as \\nan input, in addition to the input training data. Explanation-based learning is one \\nsuch approach. It uses prior knowledge to analyze, or explain, each training exam- \\nple in order to infer which example features are relevant to the target function and \\nwhich are irrelevant. These explanations enable it to generalize more accurately \\nthan inductive systems that rely on the data alone. As we saw in the previous chap- \\nter, inductive logic programming systems such as CIGOL also use prior background \\nknowledge to guide learning. However, they use their background knowledge to \\ninfer features that augment the input descriptions of instances, thereby increasing \\nthe complexity of the hypothesis space to be searched. In contrast, \\nexplanation- \\nbased learning uses prior knowledge to reduce the complexity of the hypothesis \\nspace to be searched, thereby reducing sample complexity and improving gener- \\nalization accuracy of the learner. \\nTo capture the intuition underlying explanation-based learning, consider the \\ntask of learning to play chess. In particular, suppose we would like our chess \\nprogram to learn to recognize important classes of game positions, such as the \\ntarget concept \"chessboard positions in which black will lose its queen within \\ntwo moves.\" Figure 11.1 shows a positive training example of this target concept. \\nInductive learning methods could, of course, be employed to learn this target \\nconcept. However, because the chessboard is fairly complex (there are 32 pieces \\nthat may be on any of 64 squares), and because the particular patterns that capture \\nthis concept are fairly subtle (involving the relative positions of various pieces on \\nthe board), we would have to provide thousands of training examples similar to \\nthe one in Figure 1 1.1 to expect an inductively learned hypothesis to generalize \\ncorrectly to new situations. \\nFIGURE 11.1 \\nA positive example of the target concept \"chess positions in \\nwhich black will lose its queen within two moves.\" Note the \\nwhite knight is simultaneously \\nattacking both the black king and \\nqueen. Black must therefore move its king, enabling white to \\ncapture its queen. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 320}, page_content='What is interesting about this chess-learning task is that humans appear to \\nlearn such target concepts from just a handful of training examples! In fact, after \\nconsidering only the single example shown in Figure 1 1.1, most people would \\nbe willing to suggest a general hypothesis for the target concept, such as \"board \\npositions in which the black \\nking \\nand queen are simultaneously attacked,\" and \\nwould not even consider the (equally consistent) hypothesis \"board positions in \\nwhich four white pawns are still in their original locations.\" How is it that humans \\ncan generalize so successfully from just this one example? \\nThe answer appears to be that people rely heavily on explaining, or analyz- \\ning, the training example in terms of their prior knowledge about the legal moves \\nof chess. If asked to explain why the training example of Figure 11.1 is a positive \\nexample of \"positions in which the queen will be lost in two moves,\" most people \\nwould give an explanation similar to the following: \"Because white\\'s knight is \\nattacking \\nboth the king and queen, black must move out of check, thereby al- \\nlowing the knight to capture the queen.\" The importance of such explanations is \\nthat they provide the information needed to rationally generalize from the details \\nof the training example to a correct general hypothesis. Features of the training \\nexample that are mentioned by the explanation \\n(e.g., \\nthe position of the white \\nknight, black king, and black queen) are relevant to the target concept and should \\nbe included in the general hypothesis. In contrast, features of the example that are \\nnot mentioned by the explanation \\n(e.g., \\nthe fact that there are six black pawns on \\nthe board) can be assumed to be irrelevant details. \\nWhat exactly is the prior knowledge needed by a learner to construct the \\nexplanation in this chess example? It is simply knowledge about the legal rules of \\nchess: knowledge of which moves are legal for the knight and other pieces, the fact \\nthat players must alternate moves in the game, and the fact that to win the game one \\nplayer must capture his opponent\\'s \\nking. \\nNote that given just this prior knowledge \\nit is possible in principle to calculate the optimal chess move for any board \\nposition. However, in practice this calculation can be frustratingly complex and \\ndespite the fact that we humans ourselves possess this complete, perfect knowledge \\nof chess, we remain unable to play the game optimally. As a result, much of human \\nlearning in chess (and in other search-intensive problems such as scheduling and \\nplanning) involves a long process of uncovering the consequences of our prior \\nknowledge, guided by specific training examples encountered as we play the game. \\nThis chapter describes learning algorithms that automatically construct and \\nlearn from such explanations. In the remainder of this section we define more \\nprecisely the analytical learning problem. The next section presents a particular \\nexplanation-based learning algorithm called PROLOG-EBG. Subsequent sections \\nthen examine the general properties of this algorithm and its relationship to in- \\nductive learning algorithms discussed in other chapters. The final section describes \\nthe application of explanation-based learning to improving performance at large \\nstate-space search problems. In this chapter we consider the special case in which \\nexplanations are generated from prior knowledge that is perfectly correct, as it is \\nfor us humans in the above chess example. In Chapter 12 we consider the more \\ngeneral case of \\nlearning \\nwhen prior knowledge is only approximately correct. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 321}, page_content='11.1.1 Inductive and Analytical Learning Problems \\nThe essential difference between analytical and inductive learning methods is that \\nthey assume two different formulations of the learning problem: \\n0 In inductive learning, the learner is given a hypothesis space H from which \\nit must select an output hypothesis, and a set of training examples D = \\n{(xl, f (x~)), . . . (x,, f (x,))} where f (xi) is the target value for the instance \\nxi. The desired output of the learner is a hypothesis h from H that is con- \\nsistent with these training examples. \\n0 In analytical learning, the input to the learner includes the same hypothesis \\nspace H and training examples D as for inductive learning. In addition, \\nthe learner is provided an additional input: A domain theory B consisting \\nof background knowledge that can be used to explain observed training \\nexamples. The desired output of \\n,the \\nlearner is a hypothesis h from H that \\nis consistent with both the training examples D and the domain theory B. \\nTo illustrate, in our chess example each instance xi would describe a particular \\nchess position, and f (xi) would be True when xi is a position for which black \\nwill lose its queen within two moves, and False otherwise. We might define \\nthe hypothesis space H to consist of sets of Horn clauses (if-then rules) as in \\nChapter 10, where the predicates used by the rules refer to the positions or relative \\npositions of specific pieces on the board. The domain theory B would consist of a \\nformalization of the rules of chess, describing the legal moves, the fact that players \\nmust take turns, and the fact that the game is won when one player captures her \\nopponent\\'s king. \\nNote in analytical learning, the learner must output a hypothesis that is con- \\nsistent with both the training data and the domain theory. We say that hypothesis \\nh is consistent with domain theory B provided B does not entail the negation of \\nh \\n(i.e., B -h). This additional constraint that the output hypothesis must be \\nconsistent with B reduces the ambiguity faced by the learner when the data alone \\ncannot resolve among all hypotheses in H. The net effect, provided the domain \\ntheory is correct, is to increase the accuracy of the output hypothesis. \\nLet us introduce in detail a second example of an analytical learning prob- \\nlem--one that we will use for illustration throughout this chapter. Consider an \\ninstance space X in which each instance is a pair of physical objects. Each of the \\ntwo physical objects in the instance is described by the predicates Color, Volume, \\nOwner, Material, Type, and Density, and the relationship between the two objects \\nis described by the predicate On. Given this instance space, the task is to learn the \\ntarget concept \"pairs of physical objects, such that one can be stacked safely on \\nthe other,\" denoted by the predicate SafeToStack(x,y). Learning this target concept \\nmight be useful, for example, to a robot system that has the task of storing various \\nphysical objects within a limited workspace. The full definition of this analytical \\nlearning task is given in Table 1 1.1. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 322}, page_content='Given: \\nrn Instance space X: Each instance describes a pair of objects represented by the predicates Type, \\nColor, Volume, Owner, Material, Density, and On. \\nrn Hypothesis space H: Each hypothesis is a set of Horn clause rules. The head of each Horn \\nclause is a literal containing the target predicate SafeToStack. The body of each Horn clause \\nis a conjunction of literals based on the same predicates used to describe the instances, as \\nwell as the predicates LessThan, Equal, GreaterThan, and the functions plus, minus, and \\ntimes. For example, the following Horn clause is in the hypothesis space: \\nSa f eToStack(x, y) t Volume(x, vx) r\\\\ Volurne(y, vy) A LessThan(vx, vy) \\nrn Target concept: SafeToStack(x,y) \\nrn Training Examples: A typical positive example, SafeToStack(Obj1, ObjZ), is shown below: \\nOn(Objl.Obj2) Owner(0bj I, Fred) \\nType(0bj \\nI, Box) \\nOwner(Obj2, \\nLouise) \\nType(Obj2, \\nEndtable) \\nDensity(0bj 1 ,0.3) \\nColor(Obj1, Red) Material(Obj1, Cardboard) \\nColor(Obj2, \\nBlue) Material \\n(Obj2, \\nWood) \\nVolume(Objl,2) \\nDomain Theory B: \\nSafeToStack(x, y) c -Fragile(y) \\nSafeToStack(x, \\ny) c Lighter(x, y) \\nLighter@, \\ny) c Weight(x, wx) A Weight(y, wy) r\\\\ LessThan(wx, wy) \\nWeight(x, \\nw) c Volume(x, v) A Density(x,d) A Equal(w, times(v, d)) \\nWeight(x, 5) c Type(x, Endtable) \\nFragile(x) c Material (x, Glass) \\nDetermine: \\nrn A hypothesis from H consistent with the training examples and domain theory. \\nTABLE 11.1 \\nAn analytical learning problem: SafeToStack(x,y). \\nAs shown in Table 11.1, we have chosen a hypothesis space H in which \\neach hypothesis is a set of first-order if-then rules, or Horn clauses (throughout \\nthis chapter we follow the notation and terminology for first-order Horn clauses \\nsummarized in Table 10.3). For instance, the example Horn clause hypothesis \\nshown in the table asserts that it is SafeToStack any object x on any object y, if \\nthe Volume of x is LessThan the Volume of y (in this Horn clause the variables \\nvx and vy represent the volumes of x and y, respectively). Note the Horn clause \\nhypothesis can refer to any of the predicates used to describe the instances, as well \\nas several additional predicates and functions. A typical positive training example, \\nSafeToStack(Obj1, Obj2), is also shown in the table. \\nTo formulate this task as an analytical learning problem we must also provide \\na domain theory sufficient to explain why observed positive examples satisfy the \\ntarget concept. \\nIn \\nour earlier chess example, the domain theory corresponded to \\nknowledge of the legal moves in chess, from which we constructed explanations '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 323}, page_content='describing why black would lose its queen. In the current example, the domain \\ntheory must similarly explain why certain pairs of objects can be safely stacked \\non one another. The domain theory shown in the table includes assertions such \\nas \"it is safe to stack x on y if y is not Fragile,\" and \"an object x is Fragile if \\nthe Material from which x is made is Glass.\" Like the learned hypothesis, the \\ndomain theory is described by a collection of Horn clauses, enabling the system in \\nprinciple to incorporate any learned hypotheses into subsequent domain theories. \\nNotice that the domain theory refers to additional predicates such as Lighter and \\nFragile, which are not present in the descriptions of the training examples, but \\nwhich can be inferred from more primitive instance attributes such as Material, \\nDensity, and Volume, using other other rules in the domain theory. Finally, notice \\nthat the domain theory shown in the table is sufficient to prove that the positive \\nexample shown there satisfies the target concept SafeToStack. \\n11.2 LEARNING WITH PERFECT DOMAIN THEORIES: \\nPROLOG-EBG \\nAs stated earlier, in this chapter we consider explanation-based learning from \\ndomain theories that are perfect, that is, domain theories that are correct and \\ncomplete. A domain theory is said to be correct if each of its assertions is a \\ntruthful statement about the world. A domain theory is said to be complete with \\nrespect to a given target concept and instance space, if the domain theory covers \\nevery positive example in the instance space. Put another way, it is complete if \\nevery instance that satisfies the target concept can be proven by the domain theory \\nto satisfy it. Notice our definition of completeness does not require that the domain \\ntheory be able to prove that negative examples do not satisfy the target concept. \\nHowever, if we follow the usual PROLOG convention that unprovable assertions are \\nassumed to be false, then this definition of completeness includes full coverage \\nof both positive and negative examples by the domain theory. \\nThe reader may well ask at this point whether it is reasonable to assume that \\nsuch perfect domain theories are available to the learner. After all, if the learner \\nhad a perfect domain theory, why would it need to learn? There are two responses \\nto this question. \\nFirst, there are cases in which it is feasible to provide a perfect domain \\ntheory. Our earlier chess problem provides one such case, in which the legal \\nmoves of chess form a perfect domain theory from which the optimal chess \\nplaying strategy can (in principle) be inferred. Furthermore, although it is \\nquite easy to write down the legal moves of chess that constitute this domain \\ntheory, it is extremely difficult to write down the optimal chess-playing \\nstrategy. In such cases, we prefer to provide the domain theory to the learner \\nand rely on the learner to formulate a useful description of the target concept \\n(e.g., \\n\"board states in which I am about to lose my queen\") by examining \\nand generalizing from specific training examples. Section 11.4 describes the \\nsuccessful application of explanation-based learning with perfect domain '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 324}, page_content='theories to automatically improve performance at several search-intensive \\nplanning and optimization problems. \\n0 Second, in many other cases it is unreasonable to assume that a perfect \\ndomain theory is available. It is difficult to write a perfectly correct and \\ncomplete theory even for our relatively simple SafeToStack problem. A more \\nrealistic assumption is that plausible explanations based on imperfect domain \\ntheories must be used, rather than exact proofs based on perfect knowledge. \\nNevertheless, we can begin to understand the role of explanations in learning \\nby considering the ideal case of perfect domain theories. In Chapter 12 we \\nwill consider learning from imperfect domain theories. \\nThis section presents an algorithm called PROLOG-EBG (Kedar-Cabelli and \\nMcCarty \\n1987) that is representative of several explanation-based learning algo- \\nrithms. PROLOG-EBG is a sequential covering algorithm (see Chapter 10). In other \\nwords, it operates by learning a single Horn clause rule, removing the positive \\ntraining examples covered by this rule, then iterating this process on the remain- \\ning positive examples until no further positive examples remain uncovered. When \\ngiven a complete and correct domain theory, PROLOG-EBG is guaranteed to output \\na hypothesis (set of rules) that is itself correct and that covers the observed pos- \\nitive training examples. For any set of training examples, the hypothesis output \\nby PROLOG-EBG constitutes a set of logically sufficient conditions for the target \\nconcept, according to the domain theory. PROLOG-EBG \\nis \\na refinement of the EBG \\nalgorithm introduced by Mitchell et al. (1986) and is similar to the EGGS algo- \\nrithm described by \\nDeJong \\nand Mooney (1986). The PROLOG-EBG algorithm is \\nsummarized in Table 1 1.2. \\n11.2.1 An Illustrative Trace \\nTo illustrate, consider again the training example and domain theory shown in \\nTable 1 1.1. As summarized in Table 1 1.2, the PROLOG-EBG algorithm is a se- \\nquential covering algorithm that considers the training data incrementally. For \\neach new positive training example that is not yet covered by a learned Horn \\nclause, it forms a new Horn clause by: (1) explaining the new positive training \\nexample, (2) analyzing this explanation to determine an appropriate generaliza- \\ntion, and (3) refining the current hypothesis by adding a new Horn clause rule to \\ncover this positive example, as well as other similar instances. Below we examine \\neach of these three steps in turn. \\n11.2.1.1 EXPLAIN THE TRAINING EXAMPLE \\nThe first step in processing each novel training example is to construct an expla- \\nnation in terms of the domain theory, showing how this positive example satisfies \\nthe target concept. When the domain theory is correct and complete this expla- \\nnation constitutes a proof that the training example satisfies the target concept. \\nWhen dealing with imperfect prior knowledge, the notion of explanation must be \\nextended to allow for plausible, approximate arguments rather than perfect proofs. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 325}, page_content='PROWG-EBG(TargetConcept, TrainingExamples, DomainTheory) \\n0 LearnedRules c (1 \\n0 Pos c the positive examples from TrainingExamples \\n0 for each PositiveExample in Pos that is not covered by LearnedRules, do \\nI. Explain: \\nExplanation c an explanation (proof) in terms of the DomainTheory that PositiveEx- \\nample satisfies the TargetConcept \\n2. Analyze: \\nSufJicientConditions t the most general set of features of PositiveExample sufficient \\nto \\nsatisfy the TargetConcept according to the Explanation. \\n3. Rejine: \\n0 LearnedRules c LearnedRules + NewHornClause, where NewHornCIause is of \\nthe form \\nTargetConcept c SufJicientConditions \\n0 Return LearnedRules \\nTABLE 11.2 \\nThe explanation-based learning algorithm PROLOG-EBG. For each positive example that is not yet \\ncovered by the set of learned Horn clauses (LearnedRules), a new Horn clause is created. This \\nnew Horn clause is created by (1) explaining the training example in terms of the domain theory, \\n(2) analyzing this explanation to determine the relevant features of the example, then (3) constructing \\na new Horn clause that concludes the target concept when this set of features is satisfied. \\nThe explanation for the current training example is shown in Figure 11.2. \\nNote the bottom of this figure depicts in graphical form the positive training \\nexample SafeToStack ( Objl , 0bj2 ) from Table 1 1.1. The top of the figure depicts \\nthe explanation constructed for this training example. Notice the explanation, or \\nproof, states that it is SafeToStack Objl on 0bj2 because Objl is Lighter than \\nObj2. Furthermore, Objl is known to be Lighter, because its Weight can be \\ninferred from its Density and Volume, and because the Weight of 0bj2 can be \\ninferred from the default weight of an Endtable. The specific Horn clauses that \\nunderlie this explanation are shown in the domain theory of Table 1 1.1. Notice that \\nthe explanation mentions only a small fraction of the known attributes of Objl \\nand 0bj2 (i.e., those attributes corresponding to the shaded region in the figure). \\nWhile only a single explanation is possible for the training example and \\ndomain theory shown here, in general there may be multiple possible explanations. \\nIn such cases, any or all of the explanations may be used. While each may give \\nrise to a somewhat different generalization of the training example, all will be \\njustified by the given domain theory. In the case of PROLOG-EBG, the explanation \\nis generated using a backward chaining search as performed by PROLOG. PROLOG- \\nEBG, like PROLOG, halts once it finds the first valid proof. \\n11.2.1.2 ANALYZE THE EXPLANATION \\nThe key question faced in generalizing the training example is \"of the many fea- \\ntures that happen to be true of the current training example, which ones are gen- '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 326}, page_content='Explanation: \\nTraining Example: \\nFIGURE 11.2 \\nExplanation of a training example. The network at the bottom depicts graphically the training ex- \\nample SafeToStack(Obj1, Obj2) described in Table 11.1. The top portion of the figure depicts the \\nexplanation of how this example satisfies the target concept, SafeToStack. The shaded region of \\nthe training example indicates the example attributes used in the explanation. The other, irrelevant, \\nexample attributes will be dropped from the generalized hypothesis formed from this analysis. \\nerally relevant to the target concept?\\' The explanation constructed by the learner \\nprovides a direct answer to this question: precisely those features mentioned in the \\nexplanation. For example, the explanation of Figure 11.2 refers to the Density of \\nObjl, but not to its Owner. Therefore, the hypothesis for SafeToStack(x,y) should \\ninclude Density(x, 0.3), but not Owner(x, Fred). By collecting just the features \\nmentioned in the leaf nodes of the explanation in Figure 11.2 and substituting \\nvariables x and y for Objl and Obj2, we can form a general rule that is justified \\nby the domain theory: \\nSafeToStack(x, y) t Volume(x, 2) A Density(x, 0.3) A Type(y, Endtable) \\nThe body of the above rule includes each leaf node in the proof tree, except for \\nthe leaf nodes \"Equal(0.6, times(2,0.3)\" and \"LessThan(0.6,5).\" We omit these \\ntwo because they are by definition always satisfied, independent of x and y. \\nAlong with this learned rule, the program can also provide its justification: \\nThe explanation of the training example forms a proof for the correctness of this \\nrule. Although this explanation was formed to cover the observed training exam- \\nple, the same explanation will apply to any instance that matches this general rule. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 327}, page_content='The above rule constitutes a significant generalization of the training ex- \\nample, because it omits many properties of the example \\n(e.g., \\nthe Color of the \\ntwo objects) that are irrelevant to the target concept. However, an even more \\ngeneral rule can be obtained by more careful analysis of the explanation. PROLOG- \\nEBG computes the most general rule that can be justified by the explanation, by \\ncomputing the weakest preimage of the explanation, defined as follows: \\nDefinition: The weakest preimage of a conclusion C with respect to a proof P is \\nthe most general set of initial assertions A, such that A entails C according to P. \\nFor example, the weakest preimage of the target concept SafeToStack(x,y), \\nwith respect to the explanation from Table 11.1, is given by the body of the \\nfollowing rule. This is the most general rule that can be justified by the explanation \\nof Figure 1 1.2: \\nSafeToStack(x, \\ny) t Volume(x, vx) A Density(x, dx)~ \\nEqual(wx, \\ntimes(vx, \\ndx)) A LessThan(wx, 5)~ \\nType(y, \\nEndtable) \\nNotice this more general rule does not require the specific values for Volume \\nand Density that were required by the first rule. Instead, it states a more general \\nconstraint on the values of these attributes. \\nPROLOG-EBG \\ncomputes the weakest preimage of the target concept with re- \\nspect to the explanation, using a general procedure called regression (Waldinger \\n1977). The regression procedure operates on a domain theory represented by an \\narbitrary set of Horn clauses. It works iteratively backward through the explana- \\ntion, first computing the weakest preimage of the target concept with respect to \\nthe final proof step in the explanation, then computing the weakest preimage of \\nthe resulting expressions with respect to the preceding step, and so on. The pro- \\ncedure terminates when it has iterated over all steps \\n:in \\nthe explanation, yielding \\nthe weakest precondition of the target concept with respect to the literals at the \\nleaf nodes of the explanation. \\nA trace of this regression process is illustrated in Figure 11.3. In this fig- \\nure, the explanation from Figure 11.2 is redrawn in standard (nonitalic) font. The \\nfrontier of regressed expressions created at each step by the regression proce- \\ndure is shown underlined in italics. The process begins at the root of the tree, \\nwith the frontier initialized to the general target concept \\nSafeToStack(x,y). \\nThe \\nfirst step is to compute the weakest preimage of this frontier expression with \\nrespect to the final (top-most) inference rule in the explanation. The rule in \\nthis case is \\nSafeToStack(x, \\ny) t Lighter(x, y), so the resulting weakest preim- \\nage is \\nLighter@, \\ny). The process now continues by regressing the new frontier, \\n{Lighter(x, \\ny)], \\nthrough the next Horn clause in the explanation, resulting in the \\nregressed expressions \\n(Weight(x, \\nwx), \\nLessThan(wx, \\nwy), \\nWeight(y, \\nwy)}. This \\nindicates that the explanation will hold for any x and y such that the weight wx \\nof x is less than the weight wy of y. The regression of this frontier back to \\nthe leaf nodes of the explanation continues in this step-by-step fashion, finally '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 328}, page_content='FIGURE 11.3 \\nComputing the weakest preimage of SafeToStack(0 bj 1, Obj2) with respect to the explanation. The \\ntarget concept is regressed from the root (conclusion) of the explanation, down to the leaves. At each \\nstep (indicated by the dashed lines) the current frontier set of literals (underlined in italics) is regressed \\nbackward over one \\nrule \\nin the explanation. When this process is completed, the conjunction of \\nresulting literals constitutes the weakest preimage of the target concept with respect to the explanation. \\nThis weakest preimage is shown by \\nthe \\nitalicized literals at the bottom of the figure. \\nresulting in a set of generalized literals for the leaf nodes of the tree. This final \\nset of literals, shown at the bottom of Figure 11.3, forms the body of the final \\nrule. \\nThe heart of the regression procedure is the algorithm that at each step re- \\ngresses the current frontier of expressions through a single Horn clause from the \\ndomain theory. This algorithm is described and illustrated in Table 11.3. The illus- \\ntrated example in this table corresponds to the bottommost single regression step \\nof Figure 11.3. As shown in the table, the REGRESS algorithm operates by finding \\na substitution that unifies the head of the Horn clause rule with the corresponding \\nliteral in the frontier, replacing this expression in the frontier by the rule body, \\nthen applying a unifying substitution to the entire frontier. \\nThe final Horn clause rule output by PROLOG-EBG is formulated as follows: \\nThe clause body is defined to be the weakest preconditions calculated by the above \\nprocedure. The clause head is the target concept itself, with each substitution from \\neach regression step \\n(i.e., \\nthe substitution Oh[ in Table 11.3) applied to it. This \\nsubstitution is necessary in order to keep consistent variable names between the \\nhead and body \\nof \\nthe created clause, and to specialize the clause head when the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 329}, page_content='R~~~~ss(Frontier, Rule, Literal, &i) \\nFrontier: Set of literals to be regressed through Rule \\nRule: A Horn clause \\nLiteral: A literal in Frontier that is inferred by Rule in the explanation \\nOki: The substitution that unijies the head of Rule to the corresponding literal in the explanation \\nReturns the set of literals forming the weakest \\npreimage \\nof Frontier \\nwith \\nrespect to Rule \\nhead t head of Rule \\nbody t body of Rule \\nBkl t the most general unifier of head with Literal such that there exists a substitution Bli \\nfor which \\nOri (Bkl (head)) = Bhi (head) \\nReturn Okl (Frontier - head + body) \\nExample (the bottommost regression step in Figure 11.3): \\nh?~~~ss(Frontier, Rule, Literd, @hi) where \\nFrontier = {Volume(x, us), Density(x, dx), Equal(wx, times(vx,dx)), LessThan(wx, wy), \\nWeight(y, \\nwy)) \\nRule = Weight(z, 5) c Type(z, Endtable) \\nLiteral = Weight(y, wy) \\n6ki = {z/Obj21 \\nhead c Weight (z, 5) \\nbody c Type(z, Endtable) \\nBhl e {z/y, wy/5], where Bri = (ylObj2) \\nReturn {Volume(x, us), Density(x, dx), Equal (wx, times(vx, dx)), LessThan(wx, 5). \\nType(y, \\nEndtable)] \\nTABLE 11.3 \\nAlgorithm for regressing a set of literals through a single Horn clause. The set of literals given \\nby Frontier is regressed through Rule. Literal is the member of Frontier inferred by Rule in \\nthe explanation. The substitution Bki gives the binding of variables from the head of Rule to the \\ncorresponding literal in the explanation. The algorithm first computes a substitution Bhl that unifies \\nthe Rule head to Literal, in a way that is consistent with the substitution Bki. It then applies this \\nsubstitution Oh[ to construct the preimage of Frontier with respect to Rule. The symbols \"+\" and \\n\"-\" in the algorithm denote set union and set difference. The notation {zly] denotes the substitution \\nof y in place of z. An example trace is given. \\nexplanation applies to only a special case of the target concept. As noted earlier, \\nfor the current example the final rule is \\nSafeToStack(x, y) t Volume(x, vx) A Density(x, dx)~ \\nEqual(wx, \\ntimes(vx, \\ndx)) A LessThan(wx, 5)~ \\nType(y, \\nEndtable) \\n11.2.1.3 REFINE THE CURRENT HYPOTHESIS \\nThe current hypothesis at each stage consists of the set of Horn clauses learned \\nthus far. At each stage, the sequential covering algorithm picks a new positive '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 330}, page_content=\"example that is not yet covered by the current Horn clauses, explains this new \\nexample, and formulates a new rule'according to the procedure described above. \\nNotice only positive examples are covered in the algorithm as we have defined \\nit, and the learned set of Horn clause rules predicts only positive examples. A \\nnew instance is classified as negative if the current rules fail to predict that it is \\npositive. This is in keeping with the standard negation-as-failure approach used \\nin Horn clause inference systems such as PROLOG. \\n11.3 REMARKS ON EXPLANATION-BASED LEARNING \\nAs we saw in the above example, PROLOG-EBG conducts a detailed analysis of \\nindividual training examples to determine how best to generalize from the specific \\nexample to a general Horn clause hypothesis. The following are the key properties \\nof this algorithm. \\n0 Unlike inductive methods, PROLOG-EBG produces justified general hypothe- \\nses by using prior knowledge to analyze individual examples. \\n0 The explanation of how the example satisfies the target concept determines \\nwhich example attributes are relevant: those mentioned by the explanation. \\n0 The further analysis of the explanation, regressing the target concept to de- \\ntermine its weakest preimage with respect to the explanation, allows deriving \\nmore general constraints on the values of the relevant features. \\n0 Each learned Horn clause corresponds to a sufficient condition for satisfy- \\ning the target concept. The set of learned Horn clauses covers the positive \\ntraining examples encountered by the learner, as well as other instances that \\nshare the same explanations. \\n0 The generality of the learned Horn clauses will depend on the formulation \\nof the domain theory and on the sequence in which training examples are \\nconsidered. \\n0 PROLOG-EBG implicitly assumes that the domain theory is correct and com- \\nplete. If the domain theory is incorrect or incomplete, the resulting learned \\nconcept may also be incorrect. \\nThere are several related perspectives on explanation-based learning that \\nhelp to understand its capabilities and limitations. \\n0 EBL as theory-guided generalization of examples. EBL uses its given domain \\ntheory to generalize rationally from examples, distinguishing the relevant ex- \\nample attributes from the irrelevant, thereby allowing it to avoid the bounds \\non sample complexity that apply to purely inductive learning. This is the \\nperspective implicit in the above description of the PROLOG-EBG algorithm. \\n0 EBL as example-guided reformulation of theories. The PROLOG-EBG algo- \\nrithm can be viewed as a method for reformulating the domain theory into a \\nmore operational form. In particular, the original domain theory is reformu- \\nlated by creating rules that (a) follow deductively from the domain theory, \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 331}, page_content='and (b) classify the observed training examples in a single inference step. \\nThus, the learned rules can be seen as a reformulation of the domain theory \\ninto a set of special-case rules capable of classifying instances of the target \\nconcept in a single inference step. \\n0 EBL as \"just\" restating what the learner already \"knows. \" In one sense, the \\nlearner in our SafeToStack example begins with full knowledge of the Safe- \\nToStack concept. That is, if its initial domain theory is sufficient to explain \\nany observed training examples, then it is also sufficient to predict their \\nclassification in advance. In what sense, then, does this qualify as learning? \\nOne answer is that in many tasks the difference between what one knows \\nin principle and what one can efficiently compute in practice may be great, \\nand in such cases this \\nkind \\nof \"knowledge reformulation\" can be an impor- \\ntant form of learning. In playing chess, for example, the rules of the game \\nconstitute a perfect domain theory, sufficient in principle to play perfect \\nchess. Despite this fact, people still require considerable experience to learn \\nhow to play chess well. This is precisely a situation in which a complete, \\nperfect domain theory is already known to the (human) learner, and further \\nlearning is \"simply\" a matter of reformulating this knowledge into a form \\nin which it can be used more effectively to select appropriate moves. A be- \\nginning course in Newtonian physics exhibits the same property-the basic \\nlaws of physics are easily stated, but students nevertheless spend a large \\npart of a semester \\nworking \\nout the consequences so they have this knowl- \\nedge in more operational form and need not derive every problem solution \\nfrom first principles come the final exam. PROLOG-EBG performs this type \\nof reformulation of knowledge-its learned rules map directly from observ- \\nable instance features to the classification relative to the target concept, in a \\nway that is consistent with the underlying domain theory. Whereas it may \\nrequire many inference steps and considerable search to classify an arbi- \\ntrary instance using the original domain theory, the learned rules classify \\nthe observed instances in a single inference step. \\nThus, in its pure form EBL involves reformulating the domain theory to \\nproduce general rules that classify examples in a single inference step. This \\nkind \\nof knowledge reformulation is sometimes referred to as knowledge compilation, \\nindicating that the transformation is an efficiency improving one that does not \\nalter the correctness of the system\\'s knowledge. \\n11.3.1 Discovering New Features \\nOne interesting capability of PROLOG-EBG is its ability to formulate new features \\nthat are not explicit in the description of the training examples, but that are needed \\nto describe the general rule underlying the training example. This capability is \\nillustrated by the algorithm trace and the learned rule in the previous section. In \\nparticular, the learned rule asserts that the essential constraint on the Volume and \\nDensity of x is that their product is less than 5. In fact, the training examples '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 332}, page_content='contain no description of such a product, or of the value it should take on. Instead, \\nthis constraint is formulated automatically by the learner. \\nNotice this learned \"feature\" is similar in kind to the types of features repre- \\nsented by the hidden units of neural networks; that is, this feature is one of a very \\nlarge set of potential features that can be computed from the available instance \\nattributes. Like the \\nBACKPROPAGATION \\nalgorithm, PROLOG-EBG automatically for- \\nmulates such features in its attempt to fit the training data. However, unlike the \\nstatistical process that derives hidden unit features in neural networks from many \\ntraining examples, PROLOG-EBG employs an analytical process to derive new fea- \\ntures based on analysis of single training examples. Above, PROLOG-EBG derives \\nthe feature Volume . Density > 5 analytically from the particular instantiation \\nof the domain theory used to explain a single training example. For example, \\nthe notion that the product of Volume and Density is important arises from the \\ndomain theory rule that defines Weight. The notion that this product should be \\nless than 5 arises from two other domain theory rules that assert that Obj 1 should \\nbe Lighter than the Endtable, and that the Weight of the Endtable is 5. Thus, \\nit is the particular composition and instantiation of these primitive terms from the \\ndomain theory that gives rise to defining this new feature. \\nThe issue of automatically learning useful features to augment the instance \\nrepresentation is an important issue for machine learning. The analytical derivation \\nof new features in explanation-based learning and the inductive derivation of new \\nfeatures in the hidden layer of neural networks provide two distinct approaches. \\nBecause they rely on different sources of information (statistical regularities over \\nmany examples versus analysis of single examples using the domain theory), it \\nmay be useful to explore new methods that combine both sources. \\n11.3.2 Deductive Learning \\nIn its pure form, PROLOG-EBG is a deductive, rather than inductive, learning pro- \\ncess. That is, by calculating the weakest preimage of the explanation it produces \\na hypothesis h that follows deductively from the domain theory B, while covering \\nthe training data D. To be more precise, PROLOG-EBG outputs a hypothesis h that \\nsatisfies the following two constraints: \\nwhere the training data D consists of a set of training examples in which xi is the \\nith training instance and f (xi) is its target value (f is the target function). Notice \\nthe first of these constraints is simply a formalization of the usual requirement in \\nmachine learning, that the hypothesis h correctly predict the target value f (xi) for \\neach instance xi in the training data.t Of course there will, in general, be many \\nt~ere we include PROLOG-S~Y~ negation-by-failure in our definition of entailment (F), so that ex- \\namples are entailed to be negative examples if they cannot be proven to be positive. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 333}, page_content=\"alternative hypotheses that satisfy this first constraint. The second constraint de- \\nscribes the impact of the domain theory in PROLOG-EBL: The output hypothesis is \\nfurther constrained so that it must follow from the domain theory and the data. This \\nsecond constraint reduces the ambiguity faced by the learner when it must choose \\na hypothesis. Thus, the impact of the domain theory is to reduce the effective size \\nof the hypothesis space and hence reduce the sample complexity of learning. \\nUsing similar notation, we can state the type of knowledge that is required \\nby PROLOG-EBG for its domain theory. In particular, PROLOG-EBG assumes the \\ndomain theory B entails the classifications of the instances in the training data: \\nThis constraint on the domain theory B assures that an explanation can be con- \\nstructed for each positive example. \\nIt is interesting to compare the PROLOG-EBG learning setting to the setting \\nfor inductive logic programming (ILP) discussed in Chapter 10. In that chapter, \\nwe discussed a generalization of the usual inductive learning task, in which back- \\nground knowledge B' is provided to the learner. We will use B' rather than B to \\ndenote the background knowledge used by ILP, because it does not typically sat- \\nisfy the constraint given by Equation (1 1.3). ILP is an inductive learning system, \\nwhereas PROLOG-EBG \\nis \\ndeductive. ILP uses its background knowledge B' to en- \\nlarge the set of hypotheses to be considered, whereas PROLOG-EBG uses its domain \\ntheory B to reduce the set of acceptable hypotheses. As stated in Equation \\n(10.2), \\nILP \\nsystems output a hypothesis h that satisfies the following constraint: \\nNote the relationship between this expression and the constraints on h imposed \\nby PROLOG-EBG (given by Equations (1 1.1) and (1 1.2)). This ILP constraint on \\nh is a weakened form of the constraint given by Equation (1 1.1)-the ILP con- \\nstraint requires only that (B' A h /\\\\xi) k f (xi), whereas the PROLOG-EBG constraint \\nrequires the more strict (h  xi) k f (xi). Note also that ILP imposes no constraint \\ncorresponding to the PROLOG-EBG constraint of Equation (1 1.2). \\n11.3.3 Inductive Bias in Explanation-Based Learning \\nRecall from Chapter 2 that the inductive bias of a learning algorithm is a set \\nof assertions that, together with the training examples, deductively entail sub- \\nsequent predictions made by the learner. The importance of inductive bias is \\nthat it characterizes how the learner generalizes beyond the observed training \\nexamples. \\nWhat is the inductive bias of PROLOG-EBG? In PROLOG-EBG the output hy- \\npothesis h follows deductively from DAB, as described by Equation (1 1.2). There- \\nfore, the domain theory B is a set of assertions which, together with the training \\nexamples, entail the output hypothesis. Given that predictions of the learner follow \\nfrom this hypothesis h, it appears that the inductive bias of PROLOG-EBG is simply \\nthe domain theory \\nB \\ninput to the learner. In fact, this is the case except for one \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 334}, page_content='additional detail that must be considered: There are many alternative sets of Horn \\nclauses entailed by the domain theory. The remaining component of the inductive \\nbias is therefore the basis by which PROLOG-EBG chooses among these alternative \\nsets of Horn clauses. As we saw above, PROLOG-EBG employs a sequential cover- \\ning algorithm that continues to formulate additional Horn clauses until all positive \\ntraining examples have been covered. Furthermore, each individual Horn clause \\nis the most general clause (weakest preimage) licensed by the explanation of the \\ncurrent training example. Therefore, among the sets of Horn clauses entailed by \\nthe domain theory, we can characterize the bias of PROLOG-EBG as a preference \\nfor small sets of maximally general Horn clauses. In fact, the greedy algorithm of \\nPROLOG-EBG \\nis \\nonly a heuristic approximation to the exhaustive search algorithm \\nthat would be required to find the truly shortest set of maximally general Horn \\nclauses. Nevertheless, the inductive bias of PROLOG-EBG can be approximately \\ncharacterized in this fashion. \\nApproximate inductive bias of PROLOG-EBG: The domain theory B, plus a pref- \\nerence for small sets of maximally general Horn clauses. \\nThe most important point here is that the inductive bias of PROLOG-EBG- \\nthe policy by which it generalizes beyond the training data-is largely determined \\nby the input domain theory. This lies in stark contrast to most of the other learning \\nalgorithms we have discussed \\n(e.g., \\nneural networks, decision tree learning), in \\nwhich the inductive bias is a fixed property of the learning algorithm, typically \\ndetermined by the syntax of its hypothesis representation. Why is it important \\nthat the inductive bias be an input parameter rather than a fixed property of the \\nlearner? Because, as we have discussed in Chapter 2 and elsewhere, there is \\nno universally effective inductive bias and because bias-free learning is futile. \\nTherefore, any attempt to develop a general-purpose learning method must at \\nminimum allow the inductive bias to vary with the learning problem at hand. \\nOn a more practical level, in many tasks it is quite natural to input \\ndomain- \\nspecific knowledge \\n(e.g., \\nthe knowledge about Weight in the SafeToStack ex- \\nample) to influence how the learner will generalize beyond the training data. \\nIn contrast, it is less natural to \"implement\" an appropriate bias by restricting \\nthe syntactic form of the hypotheses \\n(e.g., \\nprefer short decision trees). Finally, \\nif we consider the larger issue of how an autonomous agent may improve its \\nlearning capabilities over time, then it is attractive to have a learning algorithm \\nwhose generalization capabilities improve as it acquires more knowledge of its \\ndomain. \\n11.3.4 Knowledge Level Learning \\nAs pointed out in Equation (1 1.2), the hypothesis h output by PROLOG-EBG follows \\ndeductively from the domain theory B and training data D. In fact, by examining \\nthe PROLOG-EBG algorithm it is easy to see that h follows directly from B alone, \\nindependent of D. One way to see this is to imagine an algorithm that we might '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 335}, page_content='call LEMMA-ENUMERATOR. The LEMMA-ENUMERATOR algorithm simply enumerates \\nall proof trees that conclude the target concept based on assertions in the domain \\ntheory B. For each such proof tree, LEMMA-ENUMERATOR calculates the weakest \\npreimage and constructs a Horn clause, in the same fashion as PROLOG-EBG. The \\nonly difference between LEMMA-ENUMERATOR and PROLOG-EBG is that \\nLEMMA- \\nENUMERATOR ignores the training data and enumerates all proof trees. \\nNotice LEMMA-ENUMERATOR will output a \\nsuperset \\nof the Horn clauses output \\nby PROLOG-EBG. Given this fact, several questions arise. First, if its hypotheses \\nfollow from the domain theory alone, then what is the role of training data in \\nPROLOG-EBG? The answer is that training examples focus the PROLOG-EBG al- \\ngorithm on generating rules that cover the distribution of instances that occur in \\npractice. In our original chess example, for instance, the set of all possible lemmas \\nis huge, whereas the set of chess positions that occur in normal play is only a \\nsmall fraction of those that are syntactically possible. Therefore, by focusing only \\non training examples encountered in practice, the program is likely to develop a \\nsmaller, more relevant set of rules than if it attempted to enumerate all possible \\nlemmas about chess. \\nThe second question that arises is whether PROLOG-EBG can ever learn a \\nhypothesis that goes beyond the knowledge that is already implicit in the domain \\ntheory. Put another way, will it ever learn to classify an instance that could not \\nbe classified by the original domain theory (assuming a theorem prover with \\nunbounded computational resources)? Unfortunately, it will not. If B F h, then \\nany classification entailed by h will also be entailed by B. Is this an inherent \\nlimitation of analytical or deductive learning methods? No, it is not, as illustrated \\nby the following example. \\nTo produce an instance of deductive learning in which the learned hypothesis \\nh entails conclusions that are not entailed by B, we must create an example where \\nB y h but where D A B F h (recall the constraint given by Equation (11.2)). \\nOne interesting case is when B contains assertions such as \"If x satisfies the \\ntarget concept, then so will \\ng(x).\" \\nTaken alone, this assertion does not entail the \\nclassification of any instances. However, once we observe a positive example, it \\nallows generalizing deductively to other unseen instances. For example, consider \\nlearning the PlayTennis target concept, describing the days on which our friend \\nRoss would like to play tennis. Imagine that each day is described only by the \\nsingle attribute Humidity, and the domain theory B includes the single assertion \\n\"If Ross likes to play tennis when the humidity is \\nx, \\nthen he will also like to play \\ntennis when the humidity is lower than x,\" which can be stated more formally as \\n(Vx) IF ((PlayTennis = Yes) t (Humidity = x)) \\nTHEN ((PlayTennis = Yes) t (Humidity 5 x)) \\nNote that this domain theory does not entail any conclusions regarding which \\ninstances are positive or negative instances of PlayTennis. However, once the \\nlearner observes a positive example day for which Humidity = .30, the domain \\ntheory together with this positive example entails the following general \\nhypothe- '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 336}, page_content='CHAPTER 11 ANALYTICAL LEARNING 325 \\nsis h: \\n(PlayTennis = Yes) t- (Humidity 5 .30) \\nTo summarize, this example illustrates a situation where B I+ h, but where \\nB A D I- h. The learned hypothesis in this case entails predictions that are not \\nentailed by the domain theory alone. The phrase knowledge-level learning is some- \\ntimes used to refer to this type of learning, in which the learned hypothesis entails \\npredictions that go beyond those entailed by the domain theory. The set of all \\npredictions entailed by a set of assertions Y is often called the deductive closure \\nof Y. The key distinction here is that in knowledge-level learning the deductive \\nclosure of B is a proper subset of the deductive closure of B + h. \\nA second example of knowledge-level analytical learning is provided by con- \\nsidering a type of assertions known as determinations, which have been explored \\nin detail by Russell (1989) and others. Determinations assert that some attribute of \\nthe instance is fully determined by certain other attributes, without specifying the \\nexact nature of the dependence. For example, consider learning the target concept \\n\"people who speak Portuguese,\" and imagine we are given as a domain theory the \\nsingle determination assertion \"the language spoken by a person is determined by \\ntheir nationality.\" Taken alone, this domain theory does not enable us to classify \\nany instances as positive or negative. However, if we observe that \"Joe, a 23- \\nyear-old left-handed Brazilian, speaks Portuguese,\" then we can conclude from \\nthis positive example and the domain theory that \"all Brazilians speak Portuguese.\" \\nBoth of these examples illustrate how deductive learning can produce output \\nhypotheses that are not entailed by the domain theory alone. In both of these cases, \\nthe output hypothesis h satisfies B A D I- h, but does not satisfy B I- h. In both \\ncases, the learner deduces a justified hypothesis that does not follow from either \\nthe domain theory alone or the training data alone. \\n11.4 EXPLANATION-BASED LEARNING OF SEARCH CONTROL \\nKNOWLEDGE \\nAs noted above, the practical applicability of the PROLOG-EBG algorithm is re- \\nstricted by its requirement that the domain theory be correct and complete. One \\nimportant class of learning problems where this requirement is easily satisfied is \\nlearning to speed up complex search programs. In fact, the largest scale attempts to \\napply explanation-based learning have addressed the problem of learning to con- \\ntrol search, or what is sometimes called \"speedup\" learning. For example, playing \\ngames such as chess involves searching through a vast space of possible moves \\nand board positions to find the best move. Many practical scheduling and optimiza- \\ntion problems are easily formulated as large search problems, in which the task is \\nto find some move toward the goal state. In such problems the definitions of the \\nlegal search operators, together with the definition of the search objective, provide \\na complete and correct domain theory for learning search control knowledge. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 337}, page_content='Exactly how should we formulate the problem of learning search control so \\nthat we can apply explanation-based learning? Consider a general search problem \\nwhere S is the set of possible search states, 0 is a set of legal search operators that \\ntransform \\none search state into another, and G is a predicate defined over S that \\nindicates which states are goal states. The problem in general is to find a sequence \\nof operators that will transform an arbitrary initial state si to some final state sf \\nthat satisfies the goal predicate G. One way to formulate the learning problem is to \\nhave our system learn a separate target concept for each of the operators in 0. In \\nparticular, for each operator o in 0 it might attempt to learn the target concept \"the \\nset of states for which o leads toward a goal state.\" Of course the exact choice \\nof which target concepts to learn depends on the internal structure of problem \\nsolver that must use this learned knowledge. For example, if the problem solver \\nis a means-ends planning system that works by establishing and solving subgoals, \\nthen we might instead wish to learn target concepts such as \\n\"the \\nset of planning \\nstates in which \\nsubgoals \\nof type A should be solved before subgoals of type B.\" \\nOne system that employs explanation-based learning to improve its search \\nis PRODIGY (Carbonell et al. 1990). PRODIGY is a domain-independent planning \\nsystem that accepts the definition of a problem domain in terms of the state \\nspace S and operators 0. It then solves problems of the form \"find a sequence \\nof operators that leads from initial state si to a state that satisfies goal predicate \\nG.\" PRODIGY uses a means-ends planner that decomposes problems into subgoals, \\nsolves them, then combines their solutions into a solution for the full problem. \\nThus, during its search for problem solutions PRODIGY repeatedly faces questions \\nsuch as \"Which \\nsubgoal \\nshould be solved \\nnext?\\'and \\n\"Which operator should \\nbe considered for solving this subgoal?\\' \\nMinton \\n(1988) describes the integration \\nof explanation-based learning into PRODIGY by defining a set of target concepts \\nappropriate for these kinds of control decisions that it repeatedly confronts. For \\nexample, one target concept is \"the set of states in which \\nsubgoal A should be \\nsolved before \\nsubgoal B.\" An example of a rule learned by PRODIGY for this target \\nconcept in a simple block-stacking problem domain is \\nIF One subgoal to be solved is On@, y), and \\nOne \\nsubgoal \\nto be solved is \\nOn(y, z) \\nTHEN Solve the subgoal On(y, z) before On(x, y) \\nTo understand this rule, consider again the simple block stacking problem illus- \\ntrated in Figure 9.3. In the problem illustrated by that figure, the goal is to stack \\nthe blocks so that they spell the word \"universal.\" PRODIGY would decompose this \\nproblem into several \\nsubgoals \\nto be achieved, including \\nOn(U, \\nN), \\nOn(N, \\nI), etc. \\nNotice the above rule matches the \\nsubgoals \\nOn(U, \\nN) and \\nOn(N, \\nI), and recom- \\nmends solving the subproblem \\nOn(N, \\nI) before solving \\nOn(U, \\nN). The justifica- \\ntion for this rule (and the explanation used by \\nPRODIGY \\nto learn the rule) is that \\nif we solve the \\nsubgoals \\nin the reverse sequence, we will encounter a conflict in \\nwhich we must undo the solution to the \\nOn(U, \\nN) \\nsubgoal \\nin order to achieve the \\nother \\nsubgoal \\nOn(N, \\nI). PRODIGY learns by first encountering such a conflict, then '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 338}, page_content=\"explaining to itself the reason for this conflict and creating a rule such as the one \\nabove. The net effect is that PRODIGY uses domain-independent knowledge about \\npossible \\nsubgoal \\nconflicts, together with domain-specific knowledge of specific \\noperators \\n(e.g., \\nthe fact that the robot can pick up only one block at a time), to \\nlearn useful domain-specific planning rules such as the one illustrated above. \\nThe use of explanation-based learning to acquire control knowledge for \\nPRODIGY has been demonstrated in a variety of problem domains including the \\nsimple block-stacking problem above, as well as more complex scheduling and \\nplanning problems. \\nMinton \\n(1988) reports experiments in three problem domains, \\nin which the learned control rules improve problem-solving efficiency by a factor \\nof two to four. \\nFurthermore, \\nthe performance of these learned rules is comparable \\nto that of handwritten rules across these three problem domains. \\nMinton \\nalso de- \\nscribes a number of extensions to the basic explanation-based learning procedure \\nthat improve its effectiveness for learning control knowledge. These include meth- \\nods for simplifying learned rules and for removing learned rules whose benefits \\nare smaller than their cost. \\nA second example of a general problem-solving architecture that incorpo- \\nrates a form of explanation-based learning is the SOAR system (Laird et al. 1986; \\nNewel1 \\n1990). SOAR supports a broad variety of problem-solving strategies that \\nsubsumes PRODIGY'S means-ends planning strategy. Like PRODIGY, however, SOAR \\nlearns by explaining situations in which its current search strategy leads to ineffi- \\nciencies. When it encounters a search choice for which it does not have a definite \\nanswer \\n(e.g., \\nwhich operator to apply next) SOAR reflects on this search impasse, \\nusing weak methods such as generate-and-test to determine the correct course of \\naction. The reasoning used to resolve this impasse can be interpreted as an expla- \\nnation for how to resolve similar impasses in the future. SOAR uses a variant of \\nexplanation-based learning called chunking to extract the general conditions un- \\nder which the same explanation applies. SOAR has been applied in a great number \\nof problem domains and has also been proposed as a psychologically plausible \\nmodel of human learning processes (see \\nNewel1 \\n1990). \\nPRODIGY and SOAR demonstrate that explanation-based learning methods can \\nbe successfully applied to acquire search control knowledge in a variety of problem \\ndomains. Nevertheless, many or most heuristic search programs still use numerical \\nevaluation functions similar to the one described in Chapter 1, rather than rules \\nacquired by explanation-based learning. What is the reason for this? In fact, there \\nare significant practical problems with applying EBL to learning search control. \\nFirst, in many cases the number of control rules that must be learned is very large \\n(e.g., \\nmany thousands of rules). As the system learns more and more control rules \\nto \\nimprove its search, it must pay a larger and larger cost at each step to match this \\nset of rules against the current search state. Note this problem is not specific to \\nexplanation-based learning; it will occur for any system that represents its learned \\nknowledge by a growing set of rules. Efficient algorithms for matching rules can \\nalleviate this problem, but not eliminate it completely. \\nMinton \\n(1988) discusses \\nstrategies for empirically estimating the computational cost and benefit of each \\nrule, learning rules only when the estimated benefits outweigh the estimated costs \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 339}, page_content='and deleting rules later found to have negative utility. He describes how using \\nthis \\nkind \\nof utility analysis to determine what should be learned and what should \\nbe forgotten significantly enhances the effectiveness of explanation-based learning \\nin PRODIGY. For example, in a series of robot \\nblock-stacking \\nproblems, PRODIGY \\nencountered 328 opportunities for learning a new rule, but chose to exploit only 69 \\nof these, and eventually reduced the learned rules to a set of 19, once low-utility \\nrules were eliminated. \\nTambe \\net al. (1990) and Doorenbos (1993) discuss how to \\nidentify types of rules that will be particularly costly to match, as well as methods \\nfor re-expressing such rules in more efficient forms and methods for optimizing \\nrule-matching algorithms. Doorenbos (1993) describes how these methods enabled \\nSOAR to efficiently match a set of 100,000 learned rules in one problem domain, \\nwithout a significant increase in the cost of matching rules per state. \\nA second practical problem with applying explanation-based learning to \\nlearning search control is that in many cases it is intractable even to construct \\nthe explanations for the desired target concept. For example, in chess we might \\nwish to learn a target concept such as \"states for which operator A leads toward \\nthe optimal solution.\" Unfortunately, to prove or explain why A leads toward the \\noptimal solution requires explaining that every alternative operator leads to a less \\noptimal outcome. This typically requires effort exponential in the search depth. \\nChien \\n(1993) and Tadepalli (1990) explore methods for \"lazy\" or \"incremental\" \\nexplanation, in which heuristics are used to produce partial and approximate, but \\ntractable, explanations. Rules are extracted from these imperfect explanations as \\nthough the explanations were perfect. Of course these learned rules may be in- \\ncorrect due to the incomplete explanations. The system accommodates this by \\nmonitoring the performance of the rule on subsequent cases. If the rule subse- \\nquently makes an error, then the original explanation is incrementally elaborated \\nto cover the new case, and a more refined rule is extracted from this incrementally \\nimproved explanation. \\nMany additional research efforts have explored the use of explanation-based \\nlearning for improving the efficiency of search-based problem solvers (for exam- \\nple, Mitchell 1981; Silver 1983; Shavlik 1990; Mahadevan et al. 1993; Gervasio \\nand DeJong 1994; DeJong 1994). Bennett and DeJong (1996) explore \\nexplanation- \\nbased learning for robot planning problems where the system has an imperfect \\ndomain theory that describes its world and actions. Dietterich and \\nFlann \\n(1995) \\nexplore the integration of explanation-based learning with reinforcement learning \\nmethods discussed in Chapter 13. Mitchell and \\nThrun \\n(1993) describe the appli- \\ncation of an explanation-based neural network learning method (see the EBNN \\nalgorithm discussed in Chapter 12) to reinforcement learning problems. \\n11.5 SUMMARY AND FURTHER READING \\nThe main points of this chapter include: \\nIn contrast to purely inductive learning methods that seek a hypothesis \\nto \\nfit the training data, purely analytical learning methods seek a hypothesis '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 340}, page_content='that fits the learner\\'s prior knowledge and covers the training examples. \\nHumans often make use of prior knowledge to guide the formation of new \\nhypotheses. This chapter examines purely analytical learning methods. The \\nnext chapter examines combined inductive-analytical learning. \\na Explanation-based learning is a form of analytical learning in which the \\nlearner processes each novel training example by (1) explaining the observed \\ntarget value for this example in terms of the domain theory, (2) analyzing this \\nexplanation to determine the general conditions under which the explanation \\nholds, and (3) refining its hypothesis to incorporate these general conditions. \\na PROLOG-EBG is an explanation-based learning algorithm that uses first-order \\nHorn clauses to represent both its domain theory and its learned hypothe- \\nses. In PROLOG-EBG an explanation is a \\nPROLOG \\nproof, and the hypothesis \\nextracted from the explanation is the weakest preimage of this proof. As a \\nresult, the hypotheses output by PROLOG-EBG follow deductively from its \\ndomain theory. \\na Analytical learning methods such as PROLOG-EBG construct useful interme- \\ndiate features as a side effect of analyzing individual training examples. This \\nanalytical approach to feature generation complements the statistically based \\ngeneration of intermediate features (eg., hidden unit features) in inductive \\nmethods such as BACKPROPAGATION. \\na Although PROLOG-EBG does not produce hypotheses that extend the deduc- \\ntive closure of its domain theory, other deductive learning procedures can. \\nFor example, a domain theory containing determination assertions \\n(e.g., \\n\"na- \\ntionality determines language\") can be used together with observed data to \\ndeductively infer hypotheses that go beyond the deductive closure of the \\ndomain theory. \\na One important class of problems for which a correct and complete domain \\ntheory can be found is the class of large state-space search problems. Systems \\nsuch as PRODIGY and SOAR have demonstrated the utility of explanation- \\nbased learning methods for automatically acquiring effective search control \\nknowledge that speeds up problem solving in subsequent cases. \\na Despite the apparent usefulness of explanation-based learning methods in \\nhumans, purely deductive implementations such as PROLOG-EBG suffer the \\ndisadvantage that the output hypothesis is only as correct as the domain \\ntheory. In the next chapter we examine approaches that combine inductive \\nand analytical learning methods in order to learn effectively from imperfect \\ndomain theories and limited training data. \\nThe roots of analytical learning methods can be traced to early work by \\nFikes et al. (1972) on learning macro-operators through analysis of operators \\nin \\nABSTRIPS \\nand to somewhat later work by \\nSoloway \\n(1977) on the use of \\nexplicit prior knowledge in learning. Explanation-based learning methods similar \\nto those discussed in this chapter first appeared in a number of systems developed \\nduring the early \\n1980s, \\nincluding \\nDeJong \\n(1981); Mitchell (1981); Winston et al. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 341}, page_content='(1983); and Silver (1983). DeJong and Mooney (1986) and Mitchell et al. (1986) \\nprovided general descriptions of the explanation-based learning paradigm, which \\nhelped spur a burst of research on this topic during the late 1980s. A collection of \\nresearch on explanation-based learning performed at the University of Illinois is \\ndescribed by DeJong \\n(1993), \\nincluding algorithms that modify the structure of the \\nexplanation in order to correctly generalize iterative and temporal explanations. \\nMore recent research has focused on extending explanation-based methods to \\naccommodate imperfect domain theories and to incorporate inductive together \\nwith analytical learning (see Chapter 12). An edited collection exploring the role \\nof goals and prior knowledge in human and machine learning is provided by Ram \\nand Leake (1995), and a recent overview of explanation-based learning is given \\nby DeJong (1997). \\nThe most serious attempts to employ explanation-based learning with perfect \\ndomain theories have been in the area of learning search control, or \"speedup\" \\nlearning. The SOAR system described by Laird et al. (1986) and the PRODIGY \\nsystem described by Carbonell et al. (1990) are among the most developed sys- \\ntems that use explanation-based learning methods for learning in problem solv- \\ning. Rosenbloom and Laird (1986) discuss the close relationship between SOAR\\'S \\nlearning method (called \"chunking\") and other explanation-based learning meth- \\nods. More recently, Dietterich and Flann (1995) have explored the combination \\nof explanation-based learning with reinforcement learning methods for learning \\nsearch control. \\nWhile our primary purpose here is to study machine learning algorithms, it \\nis interesting to note that experimental studies of human learning provide support \\nfor the conjecture that human learning is based on explanations. For example, \\nAhn et al. (1987) and Qin et al. (1992) summarize evidence supporting the con- \\njecture that humans employ explanation-based learning processes. \\nWisniewski \\nand \\nMedin \\n(1995) describe experimental studies of human learning that suggest a rich \\ninterplay between prior knowledge and observed data to influence the learning \\nprocess. Kotovsky and Baillargeon (1994) describe experiments that suggest even \\n1 1-month old infants build on prior knowledge as they learn. \\nThe analysis performed in explanation-based learning is similar to certain \\nkinds of program optimization methods used for PROLOG programs, such as par- \\ntial evaluation; van \\nHarmelen \\nand Bundy (1988) provide one discussion of the \\nrelationship. \\nEXERCISES \\n11.1. Consider the problem of learning the target concept \"pairs of people who live in \\nthe same house,\" denoted by the predicate HouseMates(x, y). Below is a positive \\nexample of the concept. \\nHouseMates(Joe, Sue) \\nPerson( Joe) \\nPerson(Sue) \\nSex(Joe, \\nMale) \\nSex(Sue, \\nFemale) \\nHair Color (Joe, Black) \\nHaircolor \\n(Sue, Brown) '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 342}, page_content='Height (Joe, Short) Height(Sue, Short) \\nNationality(Joe, \\nUS) \\nNationality(Sue, \\nUS) \\nMother(Joe, \\nMary) \\nMother(Sue, \\nMary) \\nAge (Joe, 8) Age(Sue, 6) \\nThe following domain theory is helpful for acquiring the HouseMates \\nconcept: \\nHouseMates(x, y) t InSameFamily(x, y) \\nHouseMates(x, \\ny) t FraternityBrothers(x, y) \\nInSameFamily(x, \\ny) t Married(x, y) \\nInSameFamily \\n(x, y) t Youngster(x) A Youngster (y) A SameMother (x, y) \\nSameMother(x, \\ny) t Mother(x, z) A Mother(y, z) \\nYoungster(x) t Age(x, a) A LessThan(a, 10) \\nApply the PROLOG-EBG algorithm to the task of generalizing from the above \\ninstance, using the above domain theory. In particular, \\n(a) Show a hand-trace of the PROLOG-EBG algorithm applied to this problem; that \\nis, show the explanation generated for the training instance, show the result of \\nregressing the target concept through this explanation, and show the resulting \\nHorn \\nclause rule. \\n(b) Suppose that the target concept is \"people who live with Joe\" instead of \"pairs \\nof people who live together.\" Write down this target concept in terms of the \\nabove formalism. Assuming the same training instance and domain theory as \\nbefore, what Horn clause rule will PROLOG-EBG produce for this new target \\nconcept? \\nAs noted in Section 11.3.1, PROLOG-EBG can construct useful new features that are \\nnot explicit features of the instances, but that are defined in terms of the explicit \\nfeatures and that are useful for describing the appropriate generalization. These \\nfeatures are derived as a side effect of analyzing the training example explanation. A \\nsecond method for deriving useful features is the BACKPROPAGATION algorithm for \\nmultilayer neural networks, in which new features are learned by the hidden units \\nbased on the statistical properties of a large number of examples. Can you suggest \\na way in which one might combine these analytical and inductive approaches to \\ngenerating new features? (Warning: This is an open research problem.) \\nREFERENCES \\nAhn, W., Mooney, R. J., Brewer, W. F., & DeJong, G. F. (1987). Schema acquisition from one \\nexample: Psychological evidence for explanation-based learning. Ninth Annual Conference of \\nthe Cognitive Science Society (pp. 50-57). Hillsdale, NJ: Lawrence Erlbaum Associates. \\nBennett, S. W., & DeJong, G. F. (1996). Real-world robotics: Learning to plan for robust execution. \\nMachine kaming, 23, 121. \\nCarbonell, J., Knoblock, C., & Minton, S. (1990). PRODIGY: An integrated architecture for planning \\nand learning. In K. VanLehn (Ed.), Architectures for Intelligence. Hillsdale, NJ: Lawrence \\nErlbaum Associates. \\nChien, S. (1993). NONMON: Learning with recoverable simplifications. In G. DeJong (Ed.), Znvesti- \\ngating explanation-based \\nlearning (pp. 41M34). Boston, MA: Kluwer Academic Publishers. \\nDavies, T. R., and Russell, S. J. (1987). A logical approach to reasoning by analogy. Proceedings of \\nthe 10th International Joint Conference on ArtiJcial Intelligence (pp. 264-270). San Mateo, \\nCA: \\nMorgan Kaufmann. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 343}, page_content='DeJong, G. (1981). Generalizations based on explanations. Proceedings of the Seventh International \\nJoint Conference on ArtiJicial Intelligence (pp. 67-70). \\nDeJong, G., & Mooney, R. (1986). Explanation-based learning: An alternative view. Machine Learn- \\ning, 1(2), 145-176. \\nDeJong, G. (Ed.). (1993). Investigating explanation-based learning. Boston, MA: Kluwer Academic \\nPublishers. \\nDeJong, G. (1994). Learning to plan in continuous domains. ArtiJicial Intelligence, 64(1), 71-141. \\nDeJong, G. (1997). Explanation-based learning. In A. Tucker (Ed.), The Computer Science and \\nEngineering Handbook (pp. 499-520). Boca Raton, FL: CRC Press. \\nDietterich, T. G., Flann, N. S. (1995). Explanation-based learning and reinforcement learning: A \\nunified view. Proceedings of the 12th International Conference on Machine Learning (pp. \\n176-184). \\nSan Mateo, CA: Morgan Kaufmann. \\nDoorenbos, \\nR. E. (1993). Matching 100,000 learned rules. Proceedings of the Eleventh National \\nConference on ArtiJicial Intelligence (pp. 290-296). AAAI Press/MIT Press. \\nFikes, \\nR., Hart, P., & Nisson, N. (1972). Learning and executing generalized robot plans. ArtiJicial \\nIntelligence, 3(4), 251-288. \\nFisher, D., \\nSubrarnanian, D., & Tadepalli, P. (1992). An overview of current research on knowl- \\nedge compilation and speedup learning. Proceedings of the Second International Workshop on \\nKnowledge Compilation and Speedup Learning. \\nFlann, N. S., & Dietterich, T. G. (1989). A study of explanation-based methods for inductive learning. \\nMachine Learning, 4, 187-226. \\nGervasio, \\nM. T., & DeJong, G. F. (1994). An incremental learning approach to completable planning. \\nProceedings of the Eleventh International Conference on Machine Learning, New Brunswick, \\nNJ. San Mateo, CA: Morgan Kaufmann. \\nvan Harmelen, F., & Bundy, A. (1988). Explanation-based generalisation = partial evaluation. Arti- \\nficial Intelligence, 36(3), 401-412. \\nKedar-Cabelli, S., & McCarty, T. (1987). Explanation-based generalization as resolution theorem \\nproving. Proceedings of the Fourth International Workshop on Machine Learning (pp. 383- \\n389). San Francisco: Morgan Kaufmann. \\nKotovsky, L., & Baillargeon, R. (1994). Calibration-based reasoning about collision events in 11- \\nmonth-old infants. Cognition, 51, 107-129. \\nLaird, J. E., Rosenbloom, P. S., & Newell, A. (1986). Chunking in SOAR: The anatomy of a general \\nlearning mechanism. Machine Learning, 1, 11. \\nMahadevan, S., Mitchell, T., \\nMostow, D. J., Steinberg, L., & Tadepalli, P. (1993). An apprentice- \\nbased approach to knowledge acquisition. In \\nS. \\nMahadevan, T. Mitchell, D. J. Mostow, L. \\nSteinberg, & P. Tadepalli (Eds.), ArtiiJicial Intelligence, 64(1), 1-52. \\nMinton, \\nS. (1988). Learning search control knowledge: An explanation-based approach. Boston, MA: \\nKluwer Academic Publishers. \\nMiton, \\nS., \\nCarbonell, \\nJ., Knoblock, C., \\nKuokka, \\nD., \\nEtzioni, O., & Gil, Y. (1989). Explanation-based \\nleaming: \\nA problem solving perspective. ArtiJicial Intelligence, 40, 63-1 18. \\nMinton, \\nS. (1990). Quantitative results concerning the utility of explanation-based leaming. ArtiJicial \\nIntelligence, 42, 363-391. \\nMitchell, T. M. (1981). Toward combining empirical and analytical methods for inferring heuristics \\n(Technical Report LCSR-TR-27), Rutgers Computer Science Department. (Also reprinted in \\nA. \\nElithorn & R. Banerji (Eds), ArtiJicial and Human Intelligence. North-Holland, 1984.) \\nMitchell, T. M. (1983). Learning and problem-solving. Proceedings of the Eighth International Joint \\nConference on \\nArtiiJicial \\nIntelligence. San Francisco: Morgan Kaufmann. \\nMitchell, T. M., Keller, R., & Kedar-Cabelli, S. (1986). Explanation-based generalization: A unifying \\nview. Machine Learning, 1(1), 47-80. \\nMitchell, T. M. (1990). Becoming increasingly reactive. Proceedings of the Eighth National Confer- \\nence on \\nArtQicial \\nIntelligence. Medo Park, CA: AAAI Press. \\nMitchell, T. M., & Thrun, S. B. (1993). Explanation-based neural network learning for robot control. \\nIn S. Hanson et al. (Eds.), Advances in neural infomtionprocessing systems 5 (pp. 287-2941. \\nSan Mateo, CA: Morgan-Kaufmann Press. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 344}, page_content=\"CHAF'TER 11 ANALYTICAL LEARNING 333 \\nNewell, A. (1990). Unified theories of cognition. Cambridge, MA: Harvard University Press. \\nQin, Y., Mitchell, T., & Simon, H. (1992). Using explanation-based generalization to simulate hu- \\nman learning from examples and learning by doing. Proceedings of the Florida A1 Research \\nSymposium (pp. 235-239). \\nRam, A., & Leake, D. B. (Eds.). (1995). Goal-driven learning. Cambridge, MA: MIT Press. \\nRosenblwm, \\nP., & Laird, J. (1986). Mapping explanation-based generalization onto SOAR. Fifih \\nNational Conference on Artificial Intelligence (pp. 561-567). AAAI Press. \\nRussell, S. (1989). The use of knowledge in analogy and induction. San Francisco: Morgan Kaufmann. \\nShavlik, J. W. (1990). Acquiring recursive and iterative concepts with explanation-based learning. \\nMachine Learning, 5, 39. \\nSilver, B. (1983). Learning equation solving methods from worked examples. Proceedings of the \\nI983 International Workshop on Machine Learning (pp. 99-104). CS Department, University \\nof Illinois at Urbana-Champaign. \\nSilver, B. (1986). Precondition analysis: Learning control information. In R. Michalski et al. (Eds.), \\nMachine Learning: An AI approach (pp. 647470). San Mateo, CA. Morgan Kaufmann. \\nSoloway, E. (1977). Knowledge directed learning using multiple levels of description (Ph.D. thesis). \\nUniversity of Massachusetts, Arnherst. \\nTadepalli, P. (1990). Tractable learning and planning in games (Technical report ML-TR-3 1) (Ph.D. \\ndissertation). Rutgers University Computer Science Department. \\nTambe, \\nM., \\nNewell, \\nA., & Rosenbloom, P. S. (1990). The problem of expensive chunks and its \\nsolution by restricting expressiveness. Machine Learning, 5(4), 299-348. \\nWaldinger, R. (1977). Achieving several goals simultaneously. In E. \\nElcock & D. Michie Pds.), \\nMachine Intelligence 8. London: Ellis Horwood Ltd. \\nWinston, P., \\nBinford, T., Katz, B., & Lowry, M. (1983). Learning physical descriptions from func- \\ntional definitions, examples, and precedents. Proceedings of the National Conference on Arti- \\njcial Intelligence (pp. 433-439). san Mateo, CA: Morgan Kaufmann. \\nWisniewski, \\nE. J., & Medin, D. L. (1995). Harpoons and long sticks: The interaction of theory \\nand similarity in rule induction. In A. Ram & D. B. Leake (Eds.), Goal-driven learning @p. \\n177-210). Cambridge, MA: MIT Press. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 345}, page_content='CHAPTER \\nCOMBINING \\nINDUCTIVE AND \\nANALYTICAL \\nLEARNING \\nPurely inductive learning methods formulate general hypotheses by finding empir- \\nical regularities over the training examples. Purely analytical methods use prior \\nknowledge to derive general hypotheses deductively., This chapter considers meth- \\nods that combine inductive and analytical mechanisms to obtain the benefits of both \\napproaches: better generalization accuracy when prior knowledge is available and re- \\nliance on observed training data to overcome shortcomings in prior knowledge. The \\nresulting combined methods outperform both purely inductive and purely analyti- \\ncal learning methods. This chapter considers inductive-analytical learning methods \\nbased on both symbolic and artificial neural network representations. \\n12.1 MOTIVATION \\nIn previous chapters we have seen two paradigms for machine learning: inductive \\nlearning and analytical learning. Inductive methods, such as decision tree induc- \\ntion and neural network BACKPROPAGATION, seek general hypotheses that fit the \\nobserved training data. Analytical methods, such as PROLOG-EBG, seek general \\nhypotheses that fit prior knowledge while covering the observed data. These two \\nlearning paradigms are based on fundamentally different justifications for learned \\nhypotheses and offer complementary advantages and disadvantages. Combining \\nthem offers the possibility of more powerful learning methods. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 346}, page_content='CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 335 \\nPurely analytical learning methods offer the advantage of generalizing more \\naccurately from less data by using prior knowledge to guide learning. However, \\nthey can be misled when given incorrect or insufficient prior knowledge. Purely \\ninductive methods offer the advantage that they require no explicit prior knowl- \\nedge and learn regularities based solely on the training data. However, they can \\nfail when given insufficient training data, and can be misled by the implicit in- \\nductive bias they must adopt in order to generalize beyond the observed data. \\nTable 12.1 summarizes these complementary advantages and pitfalls of induc- \\ntive and analytical learning methods. This chapter considers the question of how \\nto combine the two into a single algorithm that captures the best aspects of \\nboth. \\nThe difference between inductive and analytical learning methods can be \\nseen in the nature of the justiJications that can be given for their learned hypothe- \\nses. Hypotheses output by purely analytical learning methods such as PROLOG- \\nEBG carry a logical justification; the output hypothesis follows deductively from \\nthe domain theory and training examples. Hypotheses output by purely inductive \\nlearning methods such as BACKPROPAGATION carry a statistical justification; the \\noutput hypothesis follows from statistical arguments that the training sample is \\nsufficiently large that it is probably representative of the underlying distribution \\nof examples. This statistical justification for induction is clearly articulated in the \\nPAC-learning results discussed in Chapter 7. \\nGiven that analytical methods provide logically justified hypotheses and in- \\nductive methods provide statistically justified hypotheses, it is easy to see why \\ncombining them would be useful: Logical justifications are only as compelling as \\nthe assumptions, or prior knowledge, on which they are built. They are suspect or \\npowerless if prior knowledge is incorrect or unavailable. Statistical justifications \\nare only as compelling as the data and statistical assumptions on which they rest. \\nThey are suspect or powerless when assumptions about the underlying distribu- \\ntions cannot be trusted or when data is scarce. In short, the two approaches work \\nwell for different types of problems. By combining them we can hope to devise \\na more general learning approach that covers a more broad range of learning \\ntasks. \\nFigure 12.1 summarizes a spectrum of learning problems that varies by the \\navailability of prior knowledge and training data. At one extreme, a large volume \\nInductive learning Analytical learning \\nGoal: Hypothesis fits data Hypothesis fits domain theory \\nJustification: Statistical inference Deductive inference \\nAdvantagex \\nRequires little prior knowledge Learns from scarce data \\nPitfalls: Scarce data, incorrect bias Imperfect domain theory \\nTABLE 12.1 \\nComparison of purely analytical and purely inductive learning. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 347}, page_content='Inductive learning Analytical learning \\nplentiful \\ndata \\nNo prior knowledge \\nPerfect \\npriorknowledge \\nScarce data \\nFIGURE 12.1 \\nA spectrum of learning tasks. At the left extreme, no prior knowledge is available, and purely \\ninductive learning methods with high sample complexity are therefore necessary. At the rightmost \\nextreme, a perfect domain theory is available, enabling the use of purely analytical methods such as \\nPROLOG-EBG. Most practical problems lie somewhere between these two extremes. \\nof training data is available, but no prior knowledge. At the other extreme, strong \\nprior knowledge is available, but little training data. Most practical learning prob- \\nlems lie somewhere between these two extremes of the spectrum. For example, in \\nanalyzing a database of medical records to learn \"symptoms for which treatment \\nx is more effective than treatment y,\" one often begins with approximate prior \\nknowledge \\n(e.g., \\na qualitative model of the cause-effect mechanisms underlying \\nthe disease) that suggests the patient\\'s temperature is more likely to be relevant \\nthan the patient\\'s middle initial. Similarly, in analyzing a stock market database \\nto learn the target concept \"companies whose stock value will double over the \\nnext 10 months,\" one might have approximate knowledge of economic causes \\nand effects, suggesting that the gross revenue of the company is more likely to \\nbe relevant than the color of the company logo. In both of these settings, our \\nown prior knowledge is incomplete, but is clearly useful in helping discriminate \\nrelevant features from irrelevant. \\nThe question considered in this chapter is \"What kinds of learning algo- \\nrithms can we devise that make use of approximate prior knowledge, together \\nwith available data, to form general hypotheses?\\' Notice that even when using \\na purely inductive learning algorithm, one has the opportunity to make design \\nchoices based on prior knowledge of the particular learning task. For example, \\nwhen applying \\nBACKPROPAGATION \\nto a problem such as speech recognition, one \\nmust choose the encoding of input and output data, the error function to be \\nrnin- \\nimized during gradient descent, the number of hidden units, the topology of the \\nnetwork, the learning rate and momentum, etc. In making these choices, human \\ndesigners have the opportunity to embed task-specific knowledge into the learning \\nalgorithm. The result, however, is a purely inductive instantiation of \\nBACKPROPA- \\nGATION, specialized by the designer\\'s choices to the task of speech recognition. \\nOur interest here lies in something different. We are interested in systems that \\ntake prior knowledge as an explicit input to the learner, in the same sense that \\nthe training data is an explicit input, so that they remain general purpose algo- \\nrithms, even while taking advantage of domain-specific knowledge. In brief, our \\ninterest here lies in domain-independent algorithms that employ explicitly input \\ndomain-dependent knowledge. \\nWhat criteria should we use to compare alternative approaches to combining \\ninductive and analytical learning? Given that the learner will generally not know \\nthe quality of the domain theory or the training data in advance, we are interested '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 348}, page_content='CHAF\\'TER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 337 \\nin general methods that can operate robustly over the entire spectrum of problems \\nof Figure 12.1. Some specific properties we would like from such a learning \\nmethod include: \\na Given no domain theory, it should learn at least as effectively as purely \\ninductive methods. \\nGiven a perfect domain theory, it should learn at least as effectively as \\npurely analytical methods. \\na Given an imperfect domain theory and imperfect training data, it should \\ncombine the two to outperform either purely inductive or purely analytical \\nmethods. \\ne It should accommodate an unknown level of error in the training data. \\na It should accommodate an unknown level of error in the domain theory. \\nNotice this list of desirable properties is quite ambitious. For example, ac- \\ncommodating errors in the training data is problematic even for statistically based \\ninduction without at least some prior knowledge or assumption regarding the dis- \\ntribution of errors. Combining inductive and analytical learning is an area of active \\ncurrent research. While the above list is a fair summary of what we would like \\nour algorithms to accomplish, we do not yet have algorithms that satisfy all these \\nconstraints in a fully general fashion. \\nThe next section provides a more detailed discussion of the combined \\ninductive-analytical learning problem. Subsequent sections describe three differ- \\nent approaches to combining approximate prior knowledge with available training \\ndata to guide the learner\\'s search for an appropriate hypothesis. Each of these \\nthree approaches has been demonstrated to outperform purely inductive meth- \\nods in multiple task domains. For ease of comparison, we use a single example \\nproblem to illustrate all three approaches. \\n12.2 INDUCTIVE-ANALYTICAL APPROACHES TO LEARNING \\n12.2.1 The Learning Problem \\nTo summarize, the learning problem considered in this chapter is \\nGiven: \\n0 A set of training examples D, possibly containing errors \\n0 A domain theory B, possibly containing errors \\nA space of candidate hypotheses H \\nDetermine: \\nA hypothesis that best fits the training examples and domain theory \\nWhat precisely shall we mean by \"the hypothesis that best fits the training \\nexamples and domain \\ntheory?\\'In \\nparticular, shall we prefer hypotheses that fit '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 349}, page_content='the data a little better at the expense of fitting the theory less well, or vice versa? \\nWe can be more precise by defining measures of hypothesis error with respect \\nto the data and with respect to the domain theory, then phrasing the question in \\nterms of these errors. Recall from Chapter 5 that errorD(h) is defined to be the \\nproportion of examples from D that are misclassified by h. Let us define the error \\nerror~(h) of h with respect to a domain theory B to be the probability that h \\nwill disagree with B on the classification of a randomly drawn instance. We can \\nattempt to characterize the desired output hypothesis in terms of these errors. For \\nexample, we could require the hypothesis that minimizes some combined measure \\nof these errors, such as \\nargmin kDerrorD (h) + kBerrorB (h) \\nh€H \\nWhile this appears reasonable at first glance, it is not clear what values to assign \\nto k~ and kg to specify the relative importance of fitting the data versus fitting the \\ntheory. If we have a very poor theory and a great deal of reliable data, it will be \\nbest to weight error~(h) more heavily. Given a strong theory and a small sample \\nof very noisy data, the best results would be obtained by weighting errorB(h) \\nmore heavily. Of course if the learner does not know in advance the quality of \\nthe domain theory or training data, it will be unclear how it should weight these \\ntwo error components. \\nAn alternative perspective on the question of how to weight prior knowl- \\nedge and data is the Bayesian perspective. Recall from Chapter 6 that Bayes \\ntheorem describes how to compute the posterior probability P(h1D) of hypothe- \\nsis h given observed training data D. In particular, Bayes theorem computes this \\nposterior probability based on the observed data D, together with prior knowledge \\nin the form of P(h), P(D), and P(Dlh). Thus we can think of P(h), P(D), and \\nP(Dlh) as a form of background knowledge or domain theory, and we can think \\nof Bayes theorem as a method for weighting this domain theory, together with \\nthe observed data D, to assign a posterior probability P(hlD) to h. The Bayesian \\nview is that one should simply choose the hypothesis whose posterior probability \\nis greatest, and that Bayes theorem provides the proper method for weighting \\nthe contribution of this prior knowledge and observed data. Unfortunately, Bayes \\ntheorem implicitly assumes pe$ect knowledge about the probability distributions \\nP(h), P(D), and P(Dlh). When these quantities are only imperfectly known, \\nBayes theorem alone does not prescribe how to combine them with the observed \\ndata. (One possible approach in such cases is to assume prior probability distri- \\nbutions over P(h), P(D), and P(D1h) themselves, then calculate the expected \\nvalue of the posterior P (h 1 D) . However, this requires additional knowledge about \\nthe priors over P(h), P(D), and P(Dlh), so it does not really solve the general \\nproblem.) \\nWe will revisit the question of what we mean by \"best\" fit to the hypothesis \\nand data as we examine specific algorithms. For now, we will simply say that \\nthe learning problem is to minimize some combined measure of the error of the \\nhypothesis over the data and the domain theory. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 350}, page_content='CAAPrW 12 COMBINJNG INDUCTIVE AND ANALYTICAL LEARNING 339 \\n12.2.2 Hypothesis Space Search \\nHow can the domain theory and training data best be combined to constrain the \\nsearch for an acceptable hypothesis? This remains an open question in machine \\nlearning. This chapter surveys a variety of approaches that have been proposed, \\nmany of which consist of extensions to inductive methods we have already studied \\n(e.g., \\nBACKPROPAGATION, \\nFOIL). \\nOne way to understand the range of possible approaches is to return to our \\nview of learning as a task of searching through the space of alternative hypotheses. \\nWe can characterize most learning methods as search algorithms by describing \\nthe hypothesis space H they search, the initial hypothesis ho at which they begin \\ntheir search, the set of search operators 0 that define individual search steps, and \\nthe goal criterion G that specifies the search objective. In this chapter we explore \\nthree different methods for using prior knowledge to alter the search performed \\nby purely inductive methods. \\nUse prior knowledge to derive an initial hypothesis from which to begin the \\nsearch. In this approach the domain theory B is used to construct an ini- \\ntial hypothesis ho that is consistent with B. A standard inductive method \\nis then applied, starting with the initial hypothesis ho. For example, the \\nKBANN system described below learns artificial neural networks in this \\nway. It uses prior knowledge to design the interconnections and weights \\nfor an initial network, so that this initial network is perfectly consistent \\nwith the given domain theory. This initial network hypothesis is then re- \\nfined inductively using the \\nBACKPROPAGATION \\nalgorithm and available data. \\nBeginning the search at a hypothesis consistent with the domain theory \\nmakes it more likely that the final output hypothesis will better fit this \\ntheory. \\nUse prior knowledge to alter the objective of the hypothesis space search. \\nIn this approach, the goal criterion G is modified to require that the out- \\nput hypothesis fits the domain theory as well as the training examples. For \\nexample, the EBNN system described below learns neural networks in this \\nway. Whereas inductive learning of neural networks performs gradient de- \\nscent search to minimize the squared error of the network over the training \\ndata, EBNN performs gradient descent to optimize a different criterion. This \\nmodified criterion includes an additional term that measures the error of the \\nlearned network relative to the domain theory. \\n0 Use prior knowledge to alter the available search steps. In this approach, the \\nset of search operators 0 is altered by the domain theory. For example, the \\nFOCL system described below learns sets of Horn clauses in this way. It is \\nbased on the inductive system FOIL, which conducts a greedy search through \\nthe space of possible Horn clauses, at each step revising its current hypoth- \\nesis by adding a single new literal. FOCL uses the domain theory to expand \\nthe set of alternatives available when revising the hypothesis, allowing the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 351}, page_content='addition of multiple literals in a single search step when warranted by the \\ndomain theory. In this way, FOCL allows single-step moves through the \\nhypothesis space that would correspond to many steps using the original \\ninductive algorithm. These \"macro-moves\" can dramatically alter the course \\nof the search, so that the final hypothesis found consistent with the data is \\ndifferent from the one that would be found using only the inductive search \\nsteps. \\nThe following sections describe each of these approaches in turn. \\n12.3 USING PRIOR KNOWLEDGE TO INITIALIZE THE \\nHYPOTHESIS \\nOne approach to using prior knowledge is to initialize the hypothesis to perfectly fit \\nthe domain theory, then inductively refine this initial hypothesis as needed to fit the \\ntraining data. This approach is used by the KBANN (Knowledge-Based Artificial \\nNeural Network) algorithm to learn artificial neural networks. In KBANN an initial \\nnetwork is first constructed so that for every possible instance, the classification \\nassigned by the network is identical to that assigned by the domain theory. The \\nBACKPROPAGATION \\nalgorithm is then employed to adjust the weights of this initial \\nnetwork as needed to fit the training examples. \\nIt is easy to see the motivation for this technique: if the domain theory is \\ncorrect, the initial hypothesis will correctly classify all the training examples and \\nthere will be no need to revise it. However, if the initial hypothesis is found \\nto imperfectly classify the training examples, then it will be refined inductively \\nto improve its fit to the training examples. Recall that in the purely inductive \\nBACKPROPAGATION \\nalgorithm, weights are typically initialized to small random \\nvalues. The intuition behind KBANN is that even if the domain theory is only \\napproximately correct, initializing the network to fit this domain theory will give a \\nbetter starting approximation to the target function than initializing the network to \\nrandom initial weights. This should lead, in turn, to better generalization accuracy \\nfor the final hypothesis. \\nThis initialize-the-hypothesis approach to using the domain theory has been \\nexplored by several researchers, including Shavlik and \\nTowel1 \\n(1989), \\nTowel1 \\nand Shavlik \\n(1994), \\nFu (1989, \\n1993), \\nand Pratt \\n(1993a, \\n1993b). \\nWe will use \\nthe KBANN algorithm described in Shavlik and \\nTowel1 \\n(1989) to illustrate this \\napproach. \\n12.3.1 The KBANN Algorithm \\nThe KBANN algorithm exemplifies the initialize-the-hypothesis approach to using \\ndomain theories. It assumes a domain theory represented by a set of proposi- \\ntional, nonrecursive Horn clauses. A Horn clause is propositional if it contains no \\nvariables. The input and output of KBANN are as follows: '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 352}, page_content='- \\nKBANN(Domain-Theory, Training_Examples) \\nDomain-Theory: Set of propositional, nonrecursive Horn clauses. \\nTrainingJxamples: \\nSet of (input output) pairs of the \\ntargetfunction. \\nAnalytical step: Create an initial network equivalent to the domain theory. \\n1. For each instance attribute create a network input. \\n2. For each Horn clause in the Domain-Theory, create a network unit as follows: \\n0 Connect the inputs of this unit to the attributes tested by the clause antecedents. \\nFor each non-negated antecedent of the clause, assign a weight of W to the correspond- \\ning sigmoid unit input. \\nFor each negated antecedent of the clause, assign a weight of - W to the corresponding \\nsigmoid unit input. \\n0 Set the threshold weight wo for this unit to -(n - .5)W, where n is the number of \\nnon-negated antecedents of the clause. \\n3. Add additional connections among the network units, connecting each network unit at depth \\ni from the input layer to all network units at depth i + 1. Assign random near-zero weights to \\nthese additional connections. \\nInductive step: Refine the initial network. \\n4. Apply the BACKPROPAGATION algorithm to adjust the initial network weights to fit the \\nTraining-Examples. \\nTABLE 12.2 \\nThe KBANN algorithm. The domain theory is translated into an equivalent neural network (steps \\n1-3), which is inductively refined using the BACKPROPAGATION algorithm (step 4). A typical value \\nfor the constant W is 4.0. \\nGiven: \\n0 A set of training examples \\n0 A domain theory consisting of nonrecursive, propositional Horn clauses \\nDetermine: \\n0 An artificial neural network that fits the training examples, biased by the \\ndomain theory \\nThe two stages of the KBANN algorithm are first to create an artificial neural \\nnetwork that perfectly fits the domain theory and second to use the BACKPROPA- \\nCATION algorithm to refine this initial network to fit the training examples. The \\ndetails of this algorithm, including the algorithm for creating the initial network, \\nare given in Table 12.2 and illustrated in Section 12.3.2. \\n12.3.2 An Illustrative Example \\nTo illustrate the operation of KBANN, consider the simple learning problem sum- \\nmarized in Table 12.3, adapted from Towel1 and Shavlik (1989). Here each in- \\nstance describes a physical object in terms of the material from which it is made, \\nwhether it is light, etc. The task is to \\nlearn \\nthe target concept Cup defined over \\nsuch physical objects. Table 12.3 describes a set of training examples and a do- \\nmain theory for the Cup target concept. Notice the domain theory defines a Cup '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 353}, page_content='Domain theory: \\nCup t Stable, Lzpable, OpenVessel \\nStable t BottomIsFlat \\nLijiable t Graspable, Light \\nGraspable t HasHandle \\nOpenVessel t HasConcavity, ConcavityPointsUp \\nTraining examples: \\nBottomIsFlat \\nConcavitjPointsUp \\nExpensive \\nFragile \\nHandleOnTop \\nHandleOnSide \\nHasConcavity \\nHasHandle \\nLight \\nMadeOfCeramic \\nMadeOfPaper \\nMadeOfstyrofoam \\ncups \\nJJJJ \\nJJJJ \\nJ \\nJ \\nJ \\nJ \\nJ 4 \\nJJJJ \\nJ \\nJ \\nJJJJ \\nJ \\nJ \\nJ \\nJ \\nNon-Cups \\n2/44 J \\nJ \\nJJ \\nJ \\nJ \\nJ \\nJ \\nJ \\nJ \\nJ \\nJ \\nJ \\nJ \\nJJJJ \\nJ \\nJ \\nJ \\nJJJ \\nJ \\nJ \\nJJ \\nJ \\nJ \\nJ \\nTABLE 12.3 \\nThe Cup learning task. An approximate domain theory and a set of training examples for the target \\nconcept Cup. \\nas an object that is Stable, Liftable, and an OpenVessel. The domain theory also \\ndefines each of these three attributes in terms of more primitive attributes, \\ntenni- \\nnating in the primitive, operational attributes that describe the instances. Note the \\ndomain theory is not perfectly consistent with the training examples. For example, \\nthe domain theory fails to classify the second and third training examples as pos- \\nitive examples. Nevertheless, the domain theory forms a useful approximation to \\nthe target concept. KBANN uses the domain theory and training examples together \\nto learn the target concept more accurately than it could from either alone. \\nIn the first stage of the KBANN algorithm (steps 1-3 in the algorithm), an \\ninitial network is constructed that is consistent with the domain theory. For exam- \\nple, the network constructed from the Cup domain theory is shown in Figure 12.2. \\nIn general the network is constructed by creating a sigmoid threshold unit for each \\nHorn clause in the domain theory. KBANN follows the convention that a sigmoid \\noutput value greater than 0.5 is interpreted as True and a value below 0.5 as False. \\nEach unit is therefore constructed so that its output will be greater than 0.5 just \\nin those cases where the corresponding Horn clause applies. For each antecedent \\nto the Horn clause, an input is created to the corresponding sigmoid unit. The \\nweights of the sigmoid unit are then set so that it computes the logical AND of \\nits inputs. In particular, for each input corresponding to a non-negated antecedent, '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 354}, page_content='CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 343 \\nExpensive \\nRottomlsFlat \\nMadeofceramic \\nMadeofstyrofoam \\nMadeOfPaper \\nHasHandle \\nHandleOnTop \\nHandleonside \\nLight \\nHasconcavity \\nConcavityPointsUp \\nFragile \\nStable \\nLifable \\nFIGURE 12.2 \\nA neural network equivalent to the domain theory. This network, created in the first stage of the \\nKBANN algorithm, produces output classifications identical to those of the given domain theory \\nclauses. Dark lines indicate connections with weight W and correspond to antecedents of clauses \\nfrom the domain theory. Light lines indicate connections with weights of approximately zero. \\nthe weight is set to some positive constant W. For each input corresponding to a \\nnegated antecedent, the weight is set to - W. The threshold weight of the unit, wo \\nis then set to -(n- .5) W, where n is the number of non-negated antecedents. When \\nunit input values are 1 or \\n0, \\nthis assures that their weighted sum plus wo will be \\npositive (and the sigmoid output will therefore be greater than 0.5) if and only if \\nall clause antecedents are satisfied. Note for sigmoid units at the second and sub- \\nsequent layers, unit inputs will not necessarily be 1 and \\n0 \\nand the above argument \\nmay not apply. However, if a sufficiently large value is chosen for W, this KBANN \\nalgorithm can correctly encode the domain theory for arbitrarily deep networks. \\nTowell \\nand Shavlik (1994) report using W = 4.0 in many of their experiments. \\nEach sigmoid unit input is connected to the appropriate network input or to \\nthe output of another sigmoid unit, to mirror the graph of dependencies among \\nthe corresponding attributes in the domain theory. As a final step many additional \\ninputs are added to each threshold unit, with their weights set approximately to \\nzero. The role of these additional connections is to enable the network to induc- \\ntively learn additional dependencies beyond those suggested by the given domain \\ntheory. The solid lines in the network of Figure 12.2 indicate unit inputs with \\nweights of W, whereas the lightly shaded lines indicate connections with initial \\nweights near zero. It is easy to verify that for sufficiently large values of W this \\nnetwork will output values identical to the predictions of the domain theory. \\nThe second stage of KBANN (step 4 in the algorithm of Table 12.2) uses \\nthe training examples and the BACWROPAGATION algorithm to refine the initial \\nnetwork weights. Of course if the domain theory and training examples contain \\nno errors, the initial network will already fit the training data. In the Cup ex- \\nample, however, the domain theory and training data are inconsistent, and this \\nstep therefore alters the initial network weights. The resulting trained network \\nis summarized in Figure 12.3, with dark solid lines indicating the largest posi- \\ntive weights, dashed lines indicating the largest negative weights, and light lines '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 355}, page_content='344 MACHINE LEARNING \\nExpensive a;.- ,,*: ~ .- -...* \". , ,--\" . \\nBottodsFlat \\' \\'*\\' Stable \\nMadeOfCeramic \\nMadeofstyrofoam \\nMadeOfPaper \\nHasHandle \\nLifrable \\nHandleOnTop \\nHandleOnSide \\nLight \\n\\\\ cup \\nOpen-Vessel HasConcaviiy \\nConcavityPointsUp ,i ,,.,,... *. \\nFragile \". \"\"- \\nLarge negative weight \\n\"- \" Negligible weight \\nFIGURE 12.3 \\nResult of inductively refining the initial network. KBANN uses the training examples to modify \\nthe network weights derived from the domain theory. Notice the new dependency of Lifable on \\nMadeOfStyrofoam and HandleOnTop. \\nindicating negligible weights. Although the initial network rnisclassifies several \\ntraining examples from Table 12.3, the refined network of Figure 12.3 perfectly \\nclassifies all of these training examples. \\nIt is interesting to compare the final, inductively refined network weights to \\nthe initial weights derived from the domain theory. As can be seen in Figure 12.3, \\nsignificant new dependencies were discovered during the inductive step, including \\nthe dependency of the Liftable unit on the feature MadeOfStyrofoam. It is impor- \\ntant to keep in mind that while the unit labeled Liftable was initially defined \\nby the given Horn clause for Liftable, the subsequent weight changes performed \\nby \\nBACKPROPAGATION \\nmay have dramatically \\nchanged,the \\nmeaning of this hidden \\nunit. After training of the network, this unit may take on a very different meaning \\nunrelated to the initial notion of Liftable. \\n12.3.3 Remarks \\nTo summarize, KBANN analytically creates a network equivalent to the given \\ndomain theory, then inductively refines this initial hypothesis to better fit the \\ntraining data. In doing so, it modifies the network weights as needed to overcome \\ninconsistencies between the domain theory and observed data. \\nThe chief benefit of KBANN over purely inductive \\nBACKPROPAGATION \\n(be- \\nginning with random initial weights) is that it typically generalizes more accurately \\nthan \\nBACKPROPAGATION \\nwhen given an approximately correct domain theory, es- \\npecially when training data is scarce. KBANN and other initialize-the-hypothesis \\napproaches have been demonstrated to outperform purely inductive systems in \\nseveral practical problems. For example, \\nTowel1 \\net al. (1990) describe the appli- \\ncation of KBANN to a molecular genetics problem. Here the task was to learn to '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 356}, page_content='CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 345 \\nrecognize DNA segments called promoter regions, which influence gene activity. \\nIn this experiment KBANN was given an initial domain theory obtained from a \\nmolecular geneticist, and a set of 53 positive and 53 negative training examples \\nof promoter regions. Performance was evaluated using a leave-one-out strategy \\nin which the system was run 106 different times. On each iteration KBANN was \\ntrained using 105 of the 106 examples and tested on the remaining example. The \\nresults of these 106 experiments were accumulated to provide an estimate of the \\ntrue error rate. KBANN obtained an error rate of 41106, compared to an error rate \\nof 81106 using standard BACKPROPAGATION. A variant of the KBANN approach was \\napplied by Fu \\n(1993), \\nwho reports an error rate of 21106 on the same data. Thus, \\nthe impact of prior knowledge in these experiments was to reduce significantly \\nthe error rate. The training data for this experiment is available at World Wide \\nWeb site http:llwww.ics.uci.edu/~mlearn/MLRepository.html. \\nBoth Fu (1993) and Towel1 et al. (1990) report that Horn clauses extracted \\nfrom the final trained network provided a refined domain theory that better fit \\nthe observed data. Although it is sometimes possible to map from the learned \\nnetwork weights back to a refined set of Horn clauses, in the general case this \\nis problematic because some weight settings have no direct Horn clause analog. \\nCraven and Shavlik (1994) and Craven (1996) describe alternative methods for \\nextracting symbolic rules from learned networks. \\nTo understand the significance of KBANN it is useful to consider how its \\nhypothesis search differs from that of the purely inductive \\nBACKPROPAGATION al- \\ngorithm. The hypothesis space search conducted by both algorithms is depicted \\nschematically in Figure 12.4. As shown there, the key difference is the initial \\nhypothesis from which weight tuning is performed. In the case that multiple hy- \\npotheses (weight vectors) can be found that fit the data-a condition that will be \\nespecially likely when training data is scarce-KBANN is likely to converge to a \\nhypothesis that generalizes beyond the data in a way that is more similar to the \\ndomain theory predictions. On the other hand, the particular hypothesis to which \\nBACKPROPAGATION \\nconverges will more likely be a hypothesis with small weights, \\ncorresponding roughly to a generalization bias of smoothly interpolating between \\ntraining examples. In brief, KBANN uses a domain-specific theory to bias gen- \\neralization, whereas \\nBACKPROPAGATION \\nuses a domain-independent syntactic bias \\ntoward small weight values. Note in this summary we have ignored the effect of \\nlocal minima on the search. \\nLimitations of KBANN include the fact that it can accommodate only propo- \\nsitional domain theories; that is, collections of variable-free Horn clauses. It is also \\npossible for KBANN to be misled when given highly inaccurate domain theories, \\nso that its generalization accuracy can deteriorate below the level of \\nBACKPROPA- \\nGATION. Nevertheless, it and related algorithms have been shown to be useful for \\nseveral practical problems. \\nKBANN illustrates the initialize-the-hypothesis approach to combining ana- \\nlytical and inductive learning. Other examples of this approach include Fu (1993); \\nGallant (1988); Bradshaw et al. (1989); Yang and Bhargava (1990); \\nLacher \\net al. \\n(1991). These approaches vary in the exact technique for constructing the initial '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 357}, page_content='Hypothesis Space \\nHypotheses that \\nfit training data \\nequally well \\nInitial hypothesis \\nfor KBANN \\n\\\\ \\ni -Initial hypothesis \\nfor BACKPROPAGATIOI\\\\~ \\nFIGURE 12.4 \\nHypothesis space search in KBANN. KBANN initializes the network to fit the domain theory, whereas \\nBACKPROPAGATION \\ninitializes the network to small random weights. Both then refine the weights \\niteratively using the same gradient descent rule. When multiple hypotheses can be found that fit the \\ntraining data (shaded region), KBANN and \\nBACKPROPAGATION \\nare likely to find different hypotheses \\ndue to their different starting points. \\nnetwork, the application of BACKPROPAGATION to weight tuning, and in methods for \\nextracting symbolic descriptions from the refined network. Pratt \\n(1993a, \\n1993b) \\ndescribes an initialize-the-hypothesis approach in which the prior knowledge is \\nprovided by a previously learned neural network for a related task, rather than \\na manually provided symbolic domain theory. Methods for training the values \\nof Bayesian belief networks, as discussed in Section 6.11, can also be viewed \\nas using prior knowledge to initialize the hypothesis.. Here the prior knowledge \\ncorresponds to a set of conditional independence \\nassumptions \\nthat determine the \\ngraph structure of the Bayes net, whose conditional probability tables are then \\ninduced from the training data. \\n12.4 USING PRIOR KNOWLEDGE TO ALTER THE SEARCH \\nOBJECTIVE \\nThe above approach begins the gradient descent search with a hypothesis that \\nperfectly fits the domain theory, then perturbs this hypothesis as needed to maxi- \\nmize the fit to the training data. An alternative way of using prior knowledge is \\nto incorporate it into the error criterion minimized by gradient descent, so that the \\nnetwork must fit a combined function of the training data and domain theory. In \\nthis section, we consider using prior knowledge in this fashion. In particular, we \\nconsider prior knowledge in the form of known derivatives of the target function. \\nCertain types of prior knowledge can be expressed quite naturally in this form. \\nFor example, in training a neural network to recognize handwritten characters we '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 358}, page_content='CHAF\\'TER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNiNG 347 \\ncan specify certain derivatives of the target function in order to express our prior \\nknowledge that \"the identity of the character is independent of small translations \\nand rotations of the image.\" \\nBelow we describe the \\nTANGENTPROP \\nalgorithm, which trains a neural net- \\nwork to fit both training values and training derivatives. Section 12.4.4 then de- \\nscribes how these training derivatives can be obtained from a domain theory \\nsimilar to the one used in the Cup example of Section 12.3. In particular, it \\ndiscusses how the EBNN algorithm constructs explanations of individual train- \\ning examples in order to extract training derivatives for use by \\nTANGENTPROP. \\nTANGENTPROP \\nand EBNN have been demonstrated to outperform purely inductive \\nmethods in a variety of domains, including character and object recognition, and \\nrobot perception and control tasks. \\n12.4.1 The TANGENTPROP Algorithm \\nTANGENTPROP (Simard et al. 1992) accommodates domain knowledge expressed \\nas derivatives of the target function with respect to transformations of its inputs. \\nConsider a learning task involving an instance space X and target function f. Up \\nto now we have assumed that each training example consists of a pair (xi, f (xi)) \\nthat describes some instance xi and its training value f (xi). The TANGENTPROP \\nalgorithm assumes various training derivatives of the target function are also \\nprovided. For example, if each instance xi is described by a single real value, \\nthen each training example may be of the form (xi, f (xi), q lx, ). Here lx, \\ndenotes the derivative of the target function f with respect to x, evaluated at the \\npoint x = xi. \\nTo develop an intuition for the benefits of providing training derivatives as \\nwell as training values during learning, consider the simple learning task depicted \\nin Figure 12.5. The task is to learn the target function f shown in the leftmost plot \\nof the figure, based on the three training examples shown: (xl, f (xl)), (x2, f (x2)), \\nand (xg, f (xg)). Given these three training examples, the BACKPROPAGATION algo- \\nrithm can be expected to hypothesize a smooth function, such as the function g \\ndepicted in the middle plot of the figure. The rightmost plot shows the effect of \\nFIGURE 12.5 \\nFitting values and derivatives with TANGENTPROP. Let f be the target function for which three ex- \\namples (XI, f (xi)), (x2, f (x2)), and (x3, f (x3)) are known. Based on these points the learner might \\ngenerate the hypothesis g. If the derivatives are also known, the learner can generalize more accu- \\nrately h. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 359}, page_content='providing training derivatives, or slopes, as additional information for each train- \\ning example \\n(e.g., \\n(XI, f (XI), I,, )). By fitting both the training values f (xi) \\nand these training derivatives \\nPI,, \\nthe learner has a better chance to correctly \\ngeneralize from the sparse training data. To summarize, the impact of including \\nthe training derivatives is to override the usual syntactic inductive bias of \\nBACK- \\nPROPAGATION that favors a smooth interpolation between points, replacing it by \\nexplicit input information about required derivatives. The resulting hypothesis h \\nshown in the rightmost plot of the figure provides a much more accurate estimate \\nof the true target function f. \\nIn the above example, we considered only simple kinds of derivatives of \\nthe target function. In fact, \\nTANGENTPROP \\ncan accept training derivatives with \\nrespect to various transformations of the input x. Consider, for example, the task \\nof learning to recognize handwritten characters. In particular, assume the input \\nx corresponds to an image containing a single handwritten character, and the \\ntask is to correctly classify the character. In this task, we might be interested in \\ninforming the learner that \"the target function is invariant to small rotations of \\nthe character within the image.\" In order to express this prior knowledge to the \\nlearner, we first define a transformation \\ns(a, \\nx), which rotates the image x by a! \\ndegrees. Now we can express our assertion about rotational invariance by stating \\nthat for each training instance \\nxi, \\nthe derivative of the target function with respect \\nto this transformation is zero \\n(i.e., \\nthat rotating the input image does not alter the \\nvalue of the target function). In other words, we can assert the following training \\nderivative for every training instance \\nxi \\naf ($(a, xi)) =o aa \\nwhere f is the target function and s(a, xi) is the image resulting from applying \\nthe transformation \\ns \\nto the image \\nxi. \\nHow are such training derivatives used by \\nTANGENTPROP \\nto constrain the \\nweights of the neural network? In \\nTANGENTPROP \\nthese training derivatives are \\nincorporated into the error function that is minimized by gradient descent. Recall \\nfrom Chapter 4 that the BACKPROPAGATION algorithm performs gradient descent to \\nattempt to minimize the sum of squared errors \\nwhere \\nxi \\ndenotes the ith training instance, f denotes the true target function, and \\nf denotes the function represented by the learned neural network. \\nIn \\nTANGENTPROP \\nan additional term is added to the error function to penal- \\nize discrepancies between the \\ntrainin4 \\nderivatives and the actual derivatives of \\nthe learned neural network function f. In general, TANGENTPROP accepts multi- \\nple transformations \\n(e.g., \\nwe might wish to assert both rotational invariance and \\ntranslational invariance of the character identity). Each transformation must be \\nof the form \\nsj(a, \\nx) where a! is a continuous parameter, where sj is differen- \\ntiable, and where \\nsj(O, \\nx) = x (e.g., for rotation of zero degrees the transforma- \\ntion is the identity function). For each such transformation, \\nsj(a!, \\nx), \\nTANGENT- '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 360}, page_content='CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 349 \\nPROP considers the squared error between the specified training derivative and \\nthe actual derivative of the learned neural network. The modified error func- \\ntion is \\nwhere p is a constant provided by the user to determine the relative importance \\nof fitting training values versus fitting training derivatives. Notice the first term \\nin this definition of E is the original squared error of the network versus training \\nvalues, and the second term is the squared error in the network versus training \\nderivatives. \\nSimard et al. (1992) give the gradient descent rule for minimizing this ex- \\ntended error function E. It can be derived in a fashion analogous to the derivation \\ngiven in Chapter 4 for the simpler \\nBACKPROPAGATION \\nrule. \\n12.4.2 An Illustrative Example \\nSimard et al. (1992) present results comparing the generalization accuracy of TAN- \\nGENTPROP and purely inductive BACKPROPAGATION for the problem of recognizing \\nhandwritten characters. More specifically, the task in this case is to label images \\ncontaining a single digit between \\n0 \\nand 9. In one experiment, both \\nTANGENT- \\nPROP and \\nBACKPROPAGATION \\nwere trained using training sets of varying size, then \\nevaluated based on their performance over a separate test set of 160 examples. \\nThe prior knowledge given to \\nTANGENTPROP \\nwas the fact that the classification \\nof the digit is invariant of vertical and horizontal translation of the image \\n(i.e., \\nthat the derivative of the target function was 0 with respect to these transforma- \\ntions). The results, shown in Table 12.4, demonstrate the ability of \\nTANGENTPROP \\nusing this prior knowledge to generalize more accurately than purely inductive \\nBACKPROPAGATION. \\nTraining \\nset size \\n10 \\n20 \\n40 \\n80 \\n160 \\n320 \\nPercent error on test set \\nTANGENTPROP I BACKPROPAGATION \\nTABLE 12.4 \\nGeneralization accuracy for TANGENTPROP and BACKPROPAGATION, for handwritten digit recognition. \\nTANGENTPROP generalizes more accurately due to its prior knowledge that the identity of the digit \\nis invariant of translation. These results are from Sirnard et al. (1992). '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 361}, page_content=\"12.4.3 Remarks \\nTo summarize, TANGENTPROP uses prior knowledge in the form of desired deriva- \\ntives of the target function with respect to transformations of its inputs. It combines \\nthis prior knowledge with observed training data, by minimizing an objective func- \\ntion that measures both the network's error with respect to the training example \\nvalues (fitting the data) and its error with respect to the desired derivatives (fitting \\nthe prior knowledge). The value of p determines the degree to which the network \\nwill fit one or the other of these two components in the total error. The behavior \\nof the algorithm is sensitive to p, which must be chosen by the designer. \\nAlthough \\nTANGENTPROP \\nsucceeds in combining prior knowledge with train- \\ning data to guide learning of neural networks, it is not robust to errors in the prior \\nknowledge. Consider what will happen when prior knowledge is incorrect, that \\nis, when the training derivatives input to the learner do not correctly reflect the \\nderivatives of the true target function. In this case the algorithm will attempt to fit \\nincorrect derivatives. It may therefore generalize less accurately than if it ignored \\nthis prior knowledge altogether and used the purely inductive \\nBACKPROPAGATION \\nalgorithm. If we knew in advance the degree of error in the training derivatives, \\nwe might use this information to select the constant p that determines the relative \\nimportance of fitting training values and fitting training derivatives. However, this \\ninformation is unlikely to be known in advance. In the next section we discuss \\nthe EBNN algorithm, which automatically selects values for p on an example-by- \\nexample basis in order to address the possibility of incorrect prior knowledge. \\nIt is interesting to compare the search through hypothesis space (weight \\nspace) performed by \\nTANGENTPROP, \\nKBANN, and BACKPROPAGATION. \\nTANGENT- \\nPROP \\nincorporates prior knowledge to influence the hypothesis search by altering \\nthe objective function to be minimized by gradient descent. This corresponds to \\naltering the goal of the hypothesis space search, as illustrated in Figure 12.6. Like \\nBACKPROPAGATION \\n(but unlike KBANN), \\nTANGENTPROP \\nbegins the search with an \\ninitial network of small random weights. However, the gradient descent training \\nrule produces different weight updates than BACKPROPAGATION, resulting in a dif- \\nferent final hypothesis. As shown in the figure, the set of hypotheses that minimizes \\nthe \\nTANGENTPROP \\nobjective may differ from the set that minimizes the \\nBACKPROP- \\nAGATION objective. Importantly, if the training examples and prior knowledge are \\nboth correct, and the target function can be accurately represented by the ANN, \\nthen the set of weight vectors that satisfy the TANGENTPROP objective will be a \\nsubset of those satisfying the weaker \\nBACKPROPAGATION \\nobjective. The difference \\nbetween these two sets of final hypotheses is the set of incorrect hypotheses that \\nwill be considered by BACKPROPAGATION, but ruled out by \\nTANGENTPROP \\ndue to \\nits prior knowledge. \\nNote one alternative to fitting the training derivatives of the target function \\nis to simply synthesize additional training examples near the observed training \\nexamples, using the known training derivatives to estimate training values for \\nthese nearby instances. For example, one could take a training image in the above \\ncharacter recognition task, translate it a small amount, and assert that the \\ntrans- \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 362}, page_content='Hypothesis Space \\nHypotheses that Hypotheses that \\nmaximize fit to maximizefit to data data and prior \\nknowledge \\nTANGENTPROP \\nSearch BACKPROPAGATION \\nSearch \\nFIGURE 12.6 \\nHypothesis space search in TANGENTPROP. TANGENTPROP initializes the network to small random \\nweights, just as in BACKPROPAGATION. However, it uses a different error function to drive the gradient \\ndescent search. The error used by TANGENTPROP includes both the error in predicting training values \\nand in predicting the training derivatives provided as prior knowledge. \\nlated image belonged to the same class as the original example. We might expect \\nthat fitting these synthesized examples using \\nBACKPROPAGATION \\nwould produce \\nresults similar to fitting the original training examples and derivatives using \\nTAN- \\nGENTPROP. \\nSimard et al. (1992) report experiments showing similar generalization \\nerror in the two cases, but report that \\nTANGENTPROP \\nconverges considerably more \\nefficiently. It is interesting to note that the ALVINN system, which learns to steer \\nan autonomous vehicle (see Chapter 4), uses a very similar approach to synthesize \\nadditional training examples. It uses prior knowledge of how the desired steer- \\ning direction changes with horizontal translation of the camera image to create \\nmultiple synthetic training examples to augment each observed training example. \\n12.4.4 The EBNN Algorithm \\nThe EBNN (Explanation-Based Neural Network learning) algorithm (Mitchell and \\nThrun \\n1993a; \\nThrun \\n1996) builds on the \\nTANGENTPROP \\nalgorithm in two significant \\nways. First, instead of relying on the user to provide training derivatives, EBNN \\ncomputes training derivatives itself for each observed training example. These \\ntraining derivatives are calculated by explaining each training example in terms \\nof a given domain theory, then extracting training derivatives from this explana- \\ntion. Second, EBNN addresses the issue of how to weight the relative importance \\nof the inductive and analytical components of learning (i.e., how to select the \\nparameter p in Equation [12.1]). The value of p is chosen independently for each \\ntraining example, based on a heuristic that considers how accurately the domain \\ntheory predicts the training value for this particular example. Thus, the analytical \\ncomponent of learning is emphasized for those training examples that are correctly '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 363}, page_content='explained by the domain theory and de-emphasized for training examples that are \\npoorly explained. \\nThe inputs to EBNN include (1) a set of training examples of the form \\n(xi, f (xi)) with no training derivatives provided, and (2) a domain theory analo- \\ngous to that used in explanation-based learning (Chapter 11) and in KBANN, but \\nrepresented by a set of previously trained neural networks rather than a set of \\nHorn clauses. The output of EBNN is a new neural network that approximates the \\ntarget function f. This learned network is trained to fit both the training examples \\n(xi, f (xi)) and training derivatives of f extracted from the domain theory. Fitting \\nthe training examples (xi, f (xi)) constitutes the inductive component of learning, \\nwhereas fitting the training derivatives extracted from the domain theory provides \\nthe analytical component. \\nTo illustrate the type of domain theory used by EBNN, consider Figure 12.7. \\nThe top portion of this figure depicts an EBNN domain theory for the target func- \\ntion Cup, with each rectangular block representing a distinct neural network in the \\ndomain theory. Notice in this example there is one network for each of the Horn \\nclauses in the symbolic domain theory of Table 12.3. For example, the network \\nlabeled Graspable takes as input the description of an instance and produces as \\noutput a value indicating whether the object is graspable (EBNN typically repre- \\nsents true propositions by the value 0.8 and false propositions by the value 0.2). \\nThis network is analogous to the Horn clause for Graspable given in Table 12.3. \\nSome networks take the outputs of other networks as their inputs \\n(e.g., \\nthe \\nright- \\nmost network labeled Cup takes its inputs from the outputs of the Stable, Lifable, \\nand OpenVessel networks). Thus, the networks that make up the domain theory \\ncan be chained together to infer the target function value for the input instance, \\njust as Horn clauses might be chained together for this purpose. In general, these \\ndomain theory networks may be provided to the learner by some external source, \\nor they may be the result of previous learning by the same system. EBNN makes \\nuse of these domain theory networks to learn the \\nnew,target \\nfunction. It does not \\nalter the domain theory networks during this process. \\nThe goal of EBNN is to learn a new neural network to describe the target \\nfunction. We will refer to this new network as the target network. In the example of \\nFigure 12.7, the target network Cup,,,,,, shown at the bottom of the figure takes \\nas input the description of an arbitrary instance and outputs a value indicating \\nwhether the object is a Cup. \\nEBNN learns the target network by invoking the TANGENTPROP algorithm \\ndescribed in the previous section. Recall that \\nTANGENTPROP \\ntrains a network to fit \\nboth training values and training derivatives. EBNN passes along to \\nTANGENTPROP \\nthe training values (xi, f (xi)) that it receives as input. In addition, EBNN provides \\nTANGENTPROP \\nwith derivatives that it calculates from the domain theory. To see \\nhow EBNN calculates these training derivatives, consider again Figure 12.7. The \\ntop portion of this figure shows the domain theory prediction of the target function \\nvalue for a particular training instance, xi. EBNN calculates the derivative of this \\nprediction with respect to each feature of the input instance. For the example in the \\nfigure, the instance xi is described by features such as MadeOf Styrof oam = 0.2 '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 364}, page_content='CHAFER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 353 \\nExplanation of \\ntraining example \\nin terms of \\ndomain theory: \\nBonomlsFku = T - \\nConcavilyPoinrsUp = T- \\nExpensive = T- \\nFragile = T - \\nHandIeOnTop = F - \\nHandleOdide = T - \\nHasConcovity =T - \\nHosHandle = T - \\nLight =T- \\nM&ofcemic = T- \\nMadeOfPoper = F - \\nModeofstyrofoam =F \\n7- \\nTarget network: BO~O~ISFIU~ \\nConcaviryPointsUp \\nExpensive \\nFmgile \\nHandIeOnTop \\nHandleOnSide \\nHosConcavity \\nCUP \\nHasHandle \\nLight \\nMadeofceramic \\nMadeofpaper \\nMadeofstyrofoarn \\nTraining \\nderivatives \\nFIGURE 12.7 \\nExplanation of a training example in EBNN. The explanation consists of a prediction of the target \\nfunction value by the domain \\ntheory \\nnetworks (top). Training derivatives are extracted from this \\nexplanation in order to train the separate target network (bottom). Each rectangular block represents \\na distinct \\nmultilayer \\nneural network. \\n(i.e., False), and the domain theory prediction is that Cup = 0.8 (i.e., True). \\nEBNN calculates the partial derivative of this prediction with respect to each \\ninstance feature, yielding the set of derivatives \\nacup acup \\naBottomIsFlat \\' aConcavityPointsUp \" \" aMadeOf acup Styrof oam 1 \\nThis set of derivatives is the gradient of the domain theory prediction function with \\nrespect to the input instance. The subscript refers to the fact that these derivatives '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 365}, page_content=\"354 MACHINE LEARNING \\nhold when x = xi. In the more general case where the target function has multiple \\noutput units, the gradient is computed for each of these outputs. This matrix of \\ngradients is called the Jacobian of the target function. \\nTo see the importance of these training derivatives in helping to learn the \\ntarget network, consider the derivative \\n,E~~~i,,e. \\nIf the domain theory encodes \\nthe knowledge that the feature Expensive is irrelevant to the target function Cup, \\nthen the derivative ,E~~e~i,,e extracted from the explanation will have the value \\nzero. A derivative of zero corresponds to the assertion that a change in the fea- \\nture Expensive will have no impact on the predicted value of Cup. On the other \\nhand, a large positive or negative derivative corresponds to the assertion that the \\nfeature is highly relevant to determining the target value. Thus, the derivatives \\nextracted from the domain theory explanation provide important information for \\ndistinguishing relevant from irrelevant features. When these extracted derivatives \\nare provided as training derivatives to \\nTANGENTPROP \\nfor learning the target net- \\nwork Cup,,,,,,, they provide a useful bias for guiding generalization. The usual \\nsyntactic inductive bias of neural network learning is replaced in this case by the \\nbias exerted by the derivatives obtained from the domain theory. \\nAbove we described how the domain theory prediction can be used to gen- \\nerate a set of training derivatives. To be more precise, the full EBNN algorithm \\nis as follows. Given the training examples and domain theory, EBNN first cre- \\nates a new, fully connected feedforward network to represent the target function. \\nThis target network is initialized with small random weights, just as in \\nBACK- \\nPROPAGATION. Next, for each training example (xi, f (xi)) EBNN determines the \\ncorresponding training derivatives in a two-step process. First, it uses the domain \\ntheory to predict the value of the target function for instance xi. Let \\nA(xi) \\nde- \\nnote this domain theory prediction for instance \\nxi. \\nIn other words, \\nA(xi) \\nis the \\nfunction defined by the composition of the domain theory networks forming the \\nexplanation for xi. Second, the weights and activations of the domain theory net- \\nworks are analyzed to extract the derivatives of \\nA(xi) \\n'with respect to each of the \\ncomponents of xi \\n(i.e., \\nthe Jacobian of \\nA(x) \\nevaluated at \\nx = xi). Extracting these \\nderivatives follows a process very similar to calculating the 6 terms in the BACK- \\nPROPAGATION algorithm (see Exercise 12.5). Finally, EBNN uses a minor variant \\nof the \\nTANGENTPROP \\nalgorithm to train the target network to fit the following error \\nfunction \\nwhere \\nHere \\nxi \\ndenotes the ith training instance and \\nA(x) \\ndenotes the domain theory \\nprediction for input x. The superscript notation \\nxj \\ndenotes the jth component of \\nthe vector x \\n(i.e., \\nthe jth input node of the neural network). The coefficient c is \\na normalizing constant whose value is chosen to assure that for all i, 0 5 pi 5 1. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 366}, page_content='CHAPTER 12 COMBmG INDUCTIVE AND ANALYTICAL LEARNING 355 \\nAlthough the notation here appears a bit tedious, the idea is simple. The \\nerror given by Equation (12.2) has the same general form as the error function \\nin Equation (12.1) minimized by \\nTANGENTPROP. \\nThe \\nleftmost \\nterm measures the \\nusual sum of squared errors between the training value f (xi) and the value pre- \\ndicted by the target network \\nf\"(xi). \\nThe rightmost term measures the squared error \\nbetween the training derivatives extracted from the domain theory and the \\nactual derivatives of the target network e. Thus, the leftmost term contributes \\nthe inductive constraint that the hypothesis must fit the observed training data, \\nwhereas the rightmost term contributes the analytical constraint that it must fit \\nthe training derivatives extracted from the domain theory. Notice the derivative \\nin Equation (12.2) is just a special case of the expression af(sfz\") of Equa- \\ntion \\n(12.1), \\nfor which \\nsj(a, \\nxi) \\nis the transformation that replaces \\nx! \\nby \\nx/ + a. \\nThe precise weight-training rule used by EBNN is described by Thrun (1996). \\nThe relative importance of the inductive and analytical learning components \\nis determined in EBNN by the constant pi, defined in Equation (12.3). The value \\nof pi is determined by the discrepancy between the domain theory prediction \\nA(xi) \\nand the training value f (xi). The analytical component of learning is thus \\nweighted more heavily for training examples that are correctly predicted by the \\ndomain theory and is suppressed for examples that are not correctly predicted. \\nThis weighting heuristic assumes that the training derivatives extracted from the \\ndomain theory are more likely to be correct in cases where the training value is \\ncorrectly predicted by the domain theory. Although one can construct situations \\nin which this heuristic fails, in practice it has been found effective in several \\ndomains \\n(e.g., \\nsee Mitchell and Thrun \\n[1993a]; \\nThrun \\n[1996]). \\n12.4.5 Remarks \\nTo summarize, the EBNN algorithm uses a domain theory expressed as a set of \\npreviously learned neural networks, together with a set of training examples, to \\ntrain its output hypothesis (the target network). For each training example EBNN \\nuses its domain theory to explain the example, then extracts training derivatives \\nfrom this explanation. For each attribute of the instance, a training derivative is \\ncomputed that describes how the target function value is influenced by a small \\nchange to this attribute value, according to the domain theory. These training \\nderivatives are provided to a variant of \\nTANGENTPROP, \\nwhich fits the target network \\nto these derivatives and to the training example values. Fitting the derivatives \\nconstrains the learned network to fit dependencies given by the domain theory, \\nwhile fitting the training values constrains it to fit the observed data itself. The \\nweight pi placed on fitting the derivatives is determined independently for each \\ntraining example, based on how accurately the domain theory predicts the training \\nvalue for this example. \\nEBNN has been shown to be an effective method for learning from ap- \\nproximate domain theories in several domains. Thrun (1996) describes its ap- \\nplication to a variant of the Cup learning task discussed above and reports that '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 367}, page_content=\"EBNN generalizes more accurately than standard BACKPROPAGATION, especially \\nwhen training data is scarce. For example, after 30 training examples, EBNN \\nachieved a root-mean-squared error of 5.5 on a separate set of test data, compared \\nto an error of 12.0 for BACKPROPAGATION. Mitchell and Thrun \\n(1993a) \\ndescribe \\napplying EBNN to learning to control a simulated mobile robot, in which the do- \\nmain theory consists of neural networks that predict the effects of various robot \\nactions on the world state. Again, EBNN using an approximate, previously learned \\ndomain theory, outperformed BACKPROPAGATION. Here \\nBACKPROPAGATION \\nrequired \\napproximately 90 training episodes to reach the level of performance achieved \\nby EBNN after 25 training episodes. O'Sullivan et al. (1997) and Thrun (1996) \\ndescribe several other applications of EBNN to real-world robot perception and \\ncontrol tasks, in which the domain theory consists of networks that predict the \\neffect of actions for an indoor mobile robot using sonar, vision, and laser range \\nsensors. \\nEBNN bears an interesting relation to other explanation-based learning meth- \\nods, such as PROLOG-EBG described in Chapter 11. Recall from that chapter that \\nPROLOG-EBG also constructs explanations (predictions of example target values) \\nbased on a domain theory. In PROLOG-EBG the explanation is constructed from a \\ndomain theory consisting of Horn clauses, and the target hypothesis is refined by \\ncalculating the weakest conditions under which this explanation holds. Relevant \\ndependencies in the explanation are thus captured in the learned Horn clause hy- \\npothesis. EBNN constructs an analogous explanation, but it is based on a domain \\ntheory consisting of neural networks rather than Horn clauses. As in PROLOG-EBG, \\nrelevant dependencies are then extracted from the explanation and used to refine \\nthe target hypothesis. In the case of EBNN, these dependencies take the form \\nof derivatives because derivatives are the natural way to represent dependencies \\nin continuous functions such as neural networks. In contrast, the natural way to \\nrepresent dependencies in symbolic explanations or logical proofs is to describe \\nthe set of examples to which the proof applies. \\nThere are several differences in capabilities between EBNN and the sym- \\nbolic explanation-based methods of Chapter 11. The main difference is that EBNN \\naccommodates imperfect domain theories, whereas PROLOG-EBG does not. This \\ndifference follows from the fact that EBNN is built on the inductive mechanism \\nof fitting the observed training values and uses the domain theory only as an addi- \\ntional constraint on the learned hypothesis. A second important difference follows \\nfrom the fact that PROLOG-EBG learns a growing set of Horn clauses, whereas \\nEBNN learns a fixed-size neural network. As discussed in Chapter 11, one diffi- \\nculty in learning sets of Horn clauses is that the cost of classifying a new instance \\ngrows as learning proceeds and new Horn clauses are added. This problem is \\navoided in EBNN because the fixed-size target network requires constant time to \\nclassify new instances. However, the fixed-size neural network suffers the cor- \\nresponding disadvantage that it may be unable to represent sufficiently complex \\nfunctions, whereas a growing set of Horn clauses can represent increasingly com- \\nplex functions. Mitchell and Thrun \\n(1993b) \\nprovide a more detailed discussion of \\nthe relationship between EBNN and symbolic explanation-based learning methods. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 368}, page_content=\"CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 357 \\n12.5 USING PRIOR KNOWLEDGE TO AUGMENT SEARCH \\nOPERATORS \\nThe two previous sections examined two different roles for prior knowledge in \\nlearning: initializing the learner's hypothesis and altering the objective function \\nthat guides search through the hypothesis space. In this section we consider a \\nthird way of using prior knowledge to alter the hypothesis space search: using \\nit to alter the set of operators that define legal steps in the search through the \\nhypothesis space. This approach is followed by systems such as FOCL (Pazzani \\net al. 1991; Pazzani and Kibler 1992) and ML-SMART (Bergadano and Giordana \\n1990). Here we use FOCL to illustrate the approach. \\n12.5.1 The FOCL Algorithm \\nFOCL is an extension of the purely inductive FOIL system described in Chap- \\nter 10. Both FOIL and FOCL learn a set of first-order Horn clauses to cover the \\nobserved training examples. Both systems employ a sequential covering algorithm \\nthat learns a single Horn clause, removes the positive examples covered by this \\nnew Horn clause, and then iterates this procedure over the remaining training \\nexamples. In both systems, each new Horn clause is created by performing a \\ngeneral-to-specific search, beginning with the most general possible Horn clause \\n(i.e., \\na clause containing no preconditions). Several candidate specializations of \\nthe current clause are then generated, and the specialization with greatest infor- \\nmation gain relative to the training examples is chosen. This process is iterated, \\ngenerating further candidate specializations and selecting the best, until a Horn \\nclause with satisfactory performance is obtained. \\nThe difference between FOIL and FOCL lies in the way in which candidate \\nspecializations are generated during the general-to-specific search for a single Horn \\nclause. As described in Chapter 10, FOIL generates each candidate specialization \\nby adding a single new literal to the clause preconditions. FOCL uses this same \\nmethod for producing candidate specializations, but also generates additional spe- \\ncializations based on the domain theory. The solid edges in the search tree of Fig- \\nure 12.8 show the general-to-specific search steps considered in a typical search by \\nFOIL. The dashed edge in the search tree of Figure 12.8 denotes an additional can- \\ndidate specialization that is considered by FOCL and based on the domain theory. \\nAlthough FOCL and FOIL both learn first-order Horn clauses, we illustrate \\ntheir operation here using the simpler domain of propositional (variable-free) Horn \\nclauses. In particular, consider again the Cup target concept, training examples, \\nand domain theory from Figure 12.3. To describe the operation of FOCL, we must \\nfirst draw a distinction between two kinds of literals that appear in the domain \\ntheory and hypothesis representation. We will say a literal is operational if it is \\nallowed to be used in describing an output hypothesis. For example, in the Cup \\nexample of Figure 12.3 we allow output hypotheses to refer only to the 12 at- \\ntributes that describe the training examples \\n(e.g., HasHandle, HandleOnTop). \\nLiterals based on these 12 attributes are thus considered operational. In contrast, \\nliterals that \\noccur only as intermediate features in the domain theory, but not as \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 369}, page_content='Cup C \\n[2+,3-I I( i \\\\ \\nCup C Fragile ... \\nCup C BottamlsFlal, \\nI2++l Light, \\nHmConcnvity, \\nConcavifyPointsUp \\n[4+.2-I \\nCup C \\nHasConcavity, \\nConcavityPointsUp \\nHandleOnTop \\n[0+,2-I \\nCup C BonomlsFlat, \\nLight, \\nHasConcaviry, \\nConcavifyPoinfsUp, \\n1 ~andleon~o~ \\nW+&I \\nCup C BottomlsFlof, \\nLight, \\nHasConcavity, \\nFIGURE 12.8 \\nHypothesis space search in FOCL. To learn a single rule, FOCL searches from general to increasingly \\nspecific hypotheses. Two kinds of operators generate specializations of the current hypothesis. One \\nkind \\nadds a single new literal (solid \\nlines.in \\nthe figure). A second kind of operator specializes the \\nrule by adding a set of literals that constitute logically sufficient conditions for the target concept, \\naccording to the domain theory (dashed lines in the figure). FOCL selects among all these candidate \\nspecializations, based on their performance over the data. Therefore, imperfect domain theories will \\nimpact the hypothesis only if the evidence supports the theory. This example is based on the same \\ntraining data and domain theory as the earlier KBANN example. \\nprimitive attributes of the instances, are considered nonoperational. An example \\nof a nonoperational attribute in this case is the attribute Stable. \\nAt each point in its general-to-specific search, FOCL expands its current \\nhypothesis h using the following two operators: , \\n1. For each operational literal that is not part of h, create a specialization of h \\nby adding this single literal to the preconditio s. This is also the method used \\nby FOIL to generate candidate successors. he solid arrows in Figure 12.8 \\ndenote this type of specialization. \\nP '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 370}, page_content='CHAPTER 12 COMBINING INDUCTIVE AND ANALYTICAL LEARNING 359 \\n2. Create an operational, logically sufficient condition for the target concept \\naccording to the domain theory. Add this set of literals to the current precon- \\nditions of h. Finally, prune the preconditions of h by removing any literals \\nthat are unnecessary according to the training data. The dashed arrow in \\nFigure 12.8 denotes this type of specialization. \\nThe detailed procedure for the second operator above is as follows. FOCL \\nfirst selects one of the domain theory clauses whose head (postcondition) matches \\nthe target concept. If there are several such clauses, it selects the clause whose \\nbody (preconditions) have the highest information gain relative to the training \\nexamples of the target concept. For example, in the domain theory and training \\ndata of Figure 12.3, there is only one such clause: \\nCup t Stable, Lifable, Openvessel \\nThe preconditions of the selected clause form a logically sufficient condition for \\nthe target concept. Each nonoperational literal in these sufficient conditions is \\nnow replaced, again using the domain theory and substituting clause precondi- \\ntions for clause postconditions. For example, the domain theory clause Stable t \\nBottomIsFlat is used to substitute the operational BottomIsFlat for the unopera- \\ntional Stable. This process of \"unfolding\" the domain theory continues until the \\nsufficient conditions have been restated in terms of operational literals. If there \\nare several alternative domain theory clauses that produce different results, then \\nthe one with the greatest information gain is greedily selected at each step of \\nthe unfolding process. The reader can verify that the final operational sufficient \\ncondition given the data and domain theory in the current example is \\nBottomIsFlat , HasHandle, Light, HasConcavity , ConcavityPointsUp \\nAs a final step in generating the candidate specialization, this sufficient condition is \\npruned. For each literal in the expression, the literal is removed unless its removal \\nreduces classification accuracy over the training examples. This step is included \\nto recover from overspecialization in case the imperfect domain theory includes \\nirrelevant literals. In our current example, the above set of literals matches two \\npositive and two negative examples. Pruning (removing) the literal HasHandle re- \\nsults in improved performance. The final pruned, operational, sufficient conditions \\nare, therefore, \\nBottomZsFlat , Light, HasConcavity , ConcavityPointsUp \\nThis set of literals is now added to the preconditions of the current hypothesis. \\nNote this hypothesis is the result of the search step shown by the dashed arrow \\nin Figure 12.8. \\nOnce candidate specializations of the current hypothesis have been gener- \\nated, using both of the two operations above, the candidate with highest informa- \\ntion gain is selected. In the example shown in Figure 12.8 the candidate chosen \\nat the first level in the search tree is the one generated by the domain theory. The \\nsearch then proceeds by considering further specializations of the theory-suggested '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 371}, page_content='preconditions, thereby allowing the inductive component of learning to refine the \\npreconditions derived from the domain theory. In this example, the domain theory \\naffects the search only at the first search level. However, this will not always be \\nthe case. Should the empirical support be stronger for some other candidate at the \\nfirst level, theory-suggested literals may still be added at subsequent steps in the \\nsearch. To summarize, FOCL learns Horn clauses of the form \\nwhere c is the target concept, oi is an initial conjunction of operational literals \\nadded one at a time by the first syntactic operator, ob is a conjunction of oper- \\national literals added in a single step based on the domain theory, and of is a \\nfinal conjunction of operational literals added one at a time by the first syntactic \\noperator. Any of these three sets of literals may be empty. \\nThe above discussion illustrates the use of a propositional domain theory \\nto create candidate specializations of the hypothesis during the general-to-specific \\nsearch for a single Horn clause. The algorithm is easily extended to first-order \\nrepresentations \\n(i.e., \\nrepresentations including variables). Chapter 10 discusses in \\ndetail the algorithm used by FOIL to generate first-order Horn clauses, including \\nthe extension of the first of the two search operators described above to first-order \\nrepresentations. To extend the second operator to accommodate first-order domain \\ntheories, variable substitutions must be considered when unfolding the domain \\ntheory. This can be accomplished using a procedure related to the regression \\nprocedure described in Table 1 1.3. \\n12.5.2 Remarks \\nFOCL uses the domain theory to increase the number of candidate specializations \\nconsidered at each step of the search for a single Horn clause. Figure 12.9 com- \\npares the hypothesis space search performed by FOCL to that performed by the \\npurely inductive FOIL algorithm on which it is based. \\nFOCL\\'s \\ntheory-suggested \\nspecializations correspond to \"macro\" steps in \\nFOIL\\'S \\nsearch, in which several \\nliterals are added in a single step. This process can be viewed as promoting a \\nhypothesis that might be considered later in the search to one that will be con- \\nsidered immediately. If the domain theory is correct, the training data will bear \\nout the superiority of this candidate over the others and it will be selected. If the \\ndomain theory is incorrect, the empirical evaluation of all the candidates should \\ndirect the search down an alternative path. \\nTo summarize, FOCL uses both a syntactic generation of candidate special- \\nizations and a domain theory driven generation of candidate specializations at each \\nstep in the search. The algorithm chooses among these candidates based solely on \\ntheir empirical support over the training data. Thus, the domain theory is used in \\na fashion that biases the learner, but leaves final search choices to be made based \\non performance over the training data. The bias introduced by the domain theory \\nis a preference in favor of Horn clauses most similar to operational, logically \\nsufficient conditions entailed by the domain theory. This bias is combined with '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 372}, page_content='Hypothesis Space \\n1 \\nHypotheses thatfit \\ntraining data \\nequally well \\nFIGURE 12.9 \\nHypothesis space search in FOCL. FOCL augments the set of search operators used by FOIL. Whereas \\nFOIL considers adding a single new literal at each step, FOIL also considers adding multiple literals \\nderived from the domain theory. \\nthe bias of the purely inductive FOIL program, which is a preference for shorter \\nhypotheses. \\nFOCL has been shown to generalize more accurately than the purely induc- \\ntive FOIL algorithm in a number of application domains in which an imperfect do- \\nmain theory is available. For example, Pazzani and Kibler (1992) explore learning \\nthe concept \"legal chessboard positions.\" Given 60 training examples describing \\n30 legal and 30 illegal endgame board positions, FOIL achieved an accuracy of \\n86% over an independent set of test examples. FOCL was given the same 60 train- \\ning examples, along with an approximate domain theory with an accuracy of 76%. \\nFOCL produced a hypothesis with generalization accuracy of 94%-less than half \\nthe error rate of FOIL. Similar results have been obtained in other domains. For \\nexample, given 500 training examples of telephone network problems and their \\ndiagnoses from the telephone company NYNEX, FOIL achieved an accuracy of \\n90%, \\nwhereas FOCL reached an accuracy of 98% when given the same training \\ndata along with a 95% accurate domain theory. \\n12.6 STATE OF THE ART \\nThe methods presented in this chapter are only a sample of the possible approaches \\nto combining analytical and inductive learning. While each of these methods has \\nbeen demonstrated to outperform purely inductive learning methods in selected \\ndomains, none of these has been thoroughly tested or proven across a large variety \\nof problem domains. The topic of combining inductive and analytical learning \\nremains a very active research area. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 373}, page_content='12.7 SUMMARY AND FURTHER READING \\nThe main points of this chapter include: \\n0 Approximate prior knowledge, or domain theories, are available in many \\npractical learning problems. Purely inductive methods such as decision tree \\ninduction and neural network \\nBACKPROPAGATION \\nfail to utilize such domain \\ntheories, and therefore perform poorly when data is scarce. Purely analyti- \\ncal learning methods such as PROLOG-EBG utilize such domain theories, but \\nproduce incorrect hypotheses when given imperfect prior knowledge. Meth- \\nods that blend inductive and analytical learning can gain the benefits of both \\napproaches: reduced sample complexity and the ability to overrule incorrect \\nprior knowledge. \\nOne way to view algorithms for combining inductive and analytical learning \\nis to consider how the domain theory affects the hypothesis space search. \\nIn this chapter we examined methods that use imperfect domain theories to \\n(1) create the initial hypothesis in the search, (2) expand the set of search \\noperators that generate revisions to the current hypothesis, and (3) alter the \\nobjective of the search. \\nA system that uses the domain theory to initialize the hypothesis is KBANN. \\nThis algorithm uses a domain theory encoded as propositional rules to ana- \\nlytically construct an artificial neural network that is equivalent to the domain \\ntheory. This network is then inductively refined using the \\nBACKPROPAGATION \\nalgorithm, to improve its performance over the training data. The result is \\na network biased by the original domain theory, whose weights are refined \\ninductively based on the training data. \\nTANGENTPROP uses prior knowledge represented by desired derivatives of \\nthe target function. In some domains, such as image processing, this is \\na natural way to express prior knowledge. \\nTANGENTPROP \\nincorporates this \\nknowledge by altering the objective function minimized by gradient descent \\nsearch through the space of possible hypotheses. \\nEBNN uses the domain theory to alter the objective in searching the hy- \\npothesis space of possible weights for an artificial neural network. It uses \\na domain theory consisting of previously learned neural networks to per- \\nform a neural network analog to symbolic explanation-based learning. As in \\nsymbolic explanation-based learning, the domain theory is used to explain \\nindividual examples, yielding information about the relevance of different \\nexample features. With this neural network representation, however, infor- \\nmation about relevance is expressed in the form of derivatives of the target \\nfunction value with respect to instance features. The network hypothesis is \\ntrained using a variant of the \\nTANGENTPROP \\nalgorithm, in which the error to \\nbe minimized includes both the error in network output values and the error \\nin network derivatives obtained from explanations. \\nFOCL uses the domain theory to expand the set of candidates considered at \\neach step in the search. It uses an approximate domain theory represented '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 374}, page_content='by first order Horn clauses to learn a set of Horn clauses that approximate \\nthe target function. FOCL employs a sequential covering algorithm, learning \\neach Horn clause by a general-to-specific search. The domain theory is used \\nto augment the set of next more specific candidate hypotheses considered \\nat each step of this search. Candidate hypotheses are then evaluated based \\non their performance over the training data. In this way, FOCL combines \\nthe greedy, general-to-specific inductive search strategy of FOIL with the \\nrule-chaining, analytical reasoning of analytical methods. \\n0 The question of how to best blend prior knowledge with new observations \\nremains one of the key open questions in machine learning. \\nThere are many more examples of algorithms that attempt to combine induc- \\ntive and analytical learning. For example, methods for learning Bayesian belief \\nnetworks discussed in Chapter 6 provide one alternative to the approaches dis- \\ncussed here. The references at the end of this chapter provide additional examples \\nand sources for further reading. \\nEXERCISES \\n12.1. Consider learning the target concept GoodCreditRisk defined over instances de- \\nscribed by the four attributes HasStudentLoan, HasSavingsAccount, Isstudent, \\nOwnsCar. Give the initial network created by KBANN for the following domain \\ntheory, including all network connections and weights. \\nGoodCreditRisk t Employed, LowDebt \\nEmployed t -1sStudent \\nLowDebt t -HasStudentLoan, HasSavingsAccount \\n12.2. KBANN converts a set of propositional Horn clauses into an initial neural network. \\nConsider the class of n-of-m clauses, which are Horn clauses containing m literals \\nin the preconditions (antecedents), and an associated parameter n where n m. \\nThe preconditions of an n-of-m Horn clause are considered to be satisfied if at least \\nn of its m preconditions are satisfied. For example, the clause \\nStudent t LiveslnDorm, Young, Studies; n = 2 \\nasserts that one is a Student if at least two of these three preconditions are satisfied. \\nGive an algorithm similar to that used by KBANN, that accepts a set of \\npropositional n-of-m clauses and constructs a neural network consistent with the \\ndomain theory. \\n12.3. Consider extending KBANN to accept a domain theory consisting of first-order \\nrather than propositional Horn clauses \\n(i.e., \\nHorn clauses containing variables, as in \\nChapter 10). Either give an algorithm for constructing a neural network equivalent \\nto a set of Horn clauses, or discuss the difficulties that prevent this. \\n12.4. This exercise asks you to derive a gradient descent rule analogous to that used by \\nTANGENTPROP. \\nConsider the instance space X consisting of the real numbers, and \\nconsider the hypothesis space H consisting of quadratic functions of x. That is, '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 375}, page_content=\"each hypothesis h(x) is of the form \\n(a) Derive a gradient descent rule that minimizes the same criterion as BACKPROP- \\nAGATION; that is, the sum of squared errors between the hypothesis and target \\nvalues of the training data. \\n(b) Derive a second gradient descent rule that minimizes the same criterion as \\nTANGENTPROP. \\nConsider only the single transformation s(a, x) = x + a. \\n12.5. EBNN extracts training derivatives from explanations by examining the weights \\nand activations of the neural networks that make up the explanation. Consider the \\nsimple example in which the explanation is formed by a single sigmoid unit with \\nn inputs. Derive a procedure for extracting the derivative 91,=,~ where xi is a \\nparticular training instance input to the unit, f(x) is the sigmoid unit output, and \\nxi denotes the jth input to the sigmoid unit. You may wish to use the notation x! \\nto refer to the jth component of xi. Hint: The derivation is similar to the derivation \\nof the \\nBACKPROPAGATION \\ntraining rule. \\n12.6. Consider again the search trace of FOCL shown in Figure 12.8. Suppose that the \\nhypothesis selected at the first level in the search is changed to \\nCup t -.HasHandle \\nDescribe the second-level candidate hypotheses that will be generated by FOCL as \\nsuccessors to this hypothesis. You need only include those hypotheses generated \\nby \\nFOCL's \\nsecond search operator, which uses its domain theory. Don't forget to \\npost-prune the sufficient conditions. Use the training data from Table 12.3. \\n12.7. This chapter discussed three approaches to using prior knowledge to impact the \\nsearch through the space of possible hypotheses. Discuss your ideas for how these \\nthree approaches could be integrated. Can you propose a specific algorithm that \\nintegrates at least two of these three for some specific hypothesis representation? \\nWhat advantages and disadvantages would you anticipate from this integration? \\n12.8. Consider again the question from Section 12.2.1, regarding what criterion to use \\nfor choosing among hypotheses when both data and prior knowledge are available. \\nGive your own viewpoint on this issue. \\nREFERENCES \\nAbu-Mostafa, Y. S. (1989). Learning from hints in neural networks. Journal of Complexity, 6(2), \\n192-198. \\nBergadano, F., & Giordana, A. (1990). Guiding induction with domain theories. In R. Michalski et \\nal. (Eds.), Machine learning: An art$cial intelligence approach 3 (pp. 474-492). San Mateo, \\nCA: Morgan Kaufmann. \\nBradshaw, G., Fozzard, R., & Cice, L. (1989). A connectionist expert system that really works. In \\nAdvances in neural information processing. San Mateo, CA: Morgan Kaufmam. \\nCaruana, R. (1996). Algorithms and applications for multitask learning. Proceedings of the 13th \\nInternational Conference on Machine Learning. San Francisco: Morgan Kaufmann. \\nCooper, G. C., & Herskovits, E. (1992). A Bayesian method for the induction of probabilistic networks \\nfrom data. Machine Learning, 9, 309-347. \\nCraven, M. W. (1996). Extracting comprehensible modelsfrom trained neural networks (PhD thesis) \\n(UW Technical Report CS-TR-96-1326). Department of Computer Sciences, University of \\nWisconsin-Madison. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 376}, page_content=\"Craven, M. W., & Shavlik, J. W. (1994). Using sampling and queries to extract rules from trained \\nneural networks. Proceedings of the 11th International Conference on Machine Learning (pp. \\n3745). \\nSan Mateo, CA: Morgan Kaufmann. \\nFu, L. M. (1989). Integration of neural heuristics into knowledge-based inference. Connection Science, \\n1(3), 325-339. \\nFu, L. M. (1993). Knowledge-based connectionism for revising domain theories. IEEE Transactions \\non Systems, Man, and Cybernetics, 23(1), 173-182. \\nGallant, S. I. (1988). Connectionist expert systems. CACM, 31(2), 152-169. \\nKoppel, M., Feldman, R., & Segre, A. (1994). Bias-driven revision of logical domain theories. Journal \\nof Artificial Intelligence, 1, 159-208. http:llwww.cs.washington.edulresearch/jairhome.html. \\nLacher, R., Hmska, S., & Kuncicky, D. (1991). Backpropagation learning in expert networks (Dept. \\nof Computer Science Technical Report TR91-015). Florida State University, Tallahassee. \\nMach, \\nR., & Shavlik, J. (1993). Using knowledge-based neural networks to improve algorithms: \\nRefining the Chou-Fasman algorithm for protein folding. Machine Learning, 11(3), 195-215. \\nMitchell, T. M., & Thrun, S. B. (1993a). Explanation-based neural network learning for robot control. \\nIn S. Hanson, J. \\nCowan, & C. Giles (Eds.), Advances in neural infomtionprocessing systems \\n5 (pp. 287-294). San Mateo, CA: Morgan-Kaufmann Press. \\nMitchell, T. M., & Thrun, S. B. (1993b). Explanation-based learning: A comparison of symbolic and \\nneural network approaches. Tenth International Conference on Machine Learning, Amherst, \\nMA. \\nMooney, R. (1993). Induction over the unexplained: Using overly-general domain theories to aid \\nconcept learning. Machine Learning, lO(1). \\nO'Sullivan, \\nJ., Mitchell, T., & Thrun, S. (1997). Explanation-based learning for mobile robot per- \\nception. In K. Ikeuchi & M. Veloso (Eds.), Symbolic Visual Learning (pp. 295-324). \\nOurston, D., & Mooney, R. J. (1994). Theory refinement combining analytical and empirical methods. \\nArti2cial Intelligence, 66(2). \\nPazzani, M. J., & Brunk, C. (1993). Finding accurate frontiers: A knowledge-intensive approach to \\nrelational learning. Proceedings of the I993 National Conference on Artificial Intelligence (pp. \\n328-334). AAAI Press. \\nPazzani, M. J., Brunk, C. A., & Silverstein, G. (1991). A knowledge-intensive approach to learning \\nrelational concepts. Proceedings of the Eighth International Workshop on Machine Learning \\n(pp. 432436). San Mateo, CA: Morgan Kaufmann. \\nPazzani, M. J., & Kibler, D. (1992). The utility of knowledge in inductive learning. MachineLearning, \\n9(1), 57-94. \\nPratt, L. Y. (1993a). Transferring previously learned BACKPROPAGATION neural networks to new \\nlearning tasks (Ph.D. thesis). Department of Computer Science, Rutgers University, New \\nJersey. (Also Rutgers Computer Science Technical Report ML-TR-37.) \\nPratt, L. Y. (1993b). Discriminability-based transfer among neural networks. In J. E. Moody et \\nal. (Eds.), Advances in Nerual Infomtion Processing Systems 5. San Mateo, CA: Morgan \\nKaufmann. \\nRosenbloom, \\nP. S., & Aasman, J. (1990). Knowledge level and inductive uses of chunking (ebl). \\nProceedings of the Eighth National Conference on Artificial Intelligence (pp. 821-827). AAAI \\nPress. \\nRussell, S., Binder, J., Koller, D., & Kanazawa, K. (1995). Local learning in probabilistic networks \\nwith hidden variables. Proceedings of the 14th International Joint Conference on Artificial \\nIntelligence, Montreal. Morgan Kaufmann. \\nShavlik, J., & Towell, G. (1989). An approach to combining explanation-based and neural learning \\nalgorithms. Connection Science, 1(3), 233-255. \\nSimard, \\nP. S., \\nVictoni, \\nB., \\nLeCun, Y., & Denker, J. (1992). Tangent prop-A formalism for specifying \\nselected \\ninvariances \\nin an adaptive network. In J. Moody et al. (Eds.), Advances in Neural \\nInforination \\nProcessing System 4. San Mateo, CA: Morgan Kaufmann. \\nSudharth, \\nS. \\nC., & Holden, A. D. C. (1991). Symbolic-neural systems and the use of hints for \\ndeveloping \\ncomplex \\nsystems. International Journal of Man-Machine Studies, 35(3), 291-3 11. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 377}, page_content='Thrun, S. (1996). Explanation based neural network learning: A lifelong learning approach. Boston: \\nKluwer Academic Publishers. \\nThrun, S., & Mitchell, T. M. (1993). Integrating inductive neural network learning and explanation- \\nbased learning. Proceedings of the 1993 International Joint Conference on Artificial Intelli- \\ngence. \\nThrun, S., & Mitchell, T. M. (1995). Learning one more thing. Proceedings of the 1995 International \\nJoint Conference on Artificial Intelligence, Montreal. \\nTowell, G., & Shavlik, J. (1989). An approach to combining explanation-based and neural learning \\nalgorithms. Connection Science, (I), 233-255. \\nTowell, G., & Shavlik, J. (1994). Knowledge-based artificial neural networks. Artificial Intelligence, \\n70(1-2), 119-165. \\nTowell, G., Shavlik, J., & Noordewier, M. (1990). Refinement of approximate domain theories by \\nknowledge-based neural networks. Proceedings of the Eighth National Conference on Artijcial \\nIntelligence (pp. 861-866). Cambridge, MA: AAAI, MIT Press. \\nYang, Q., & Bhargava, V. (1990). Building expert systems by a modified perceptron network with \\nrule-transfer algorithms (pp. 77-82). International Joint Conference on Neural Networks, IEEE. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 378}, page_content='CHAPTER \\nREINFORCEMENT \\nLEARNING \\nReinforcement learning addresses the question of how an autonomous agent that \\nsenses and acts in its environment can learn to choose optimal actions to achieve its \\ngoals. This very generic problem covers tasks such as learning to control a mobile \\nrobot, learning to optimize operations in factories, and learning to play board games. \\nEach time the agent performs an action in its environment, a trainer may provide a \\nreward or penalty to indicate the desirability of the resulting state. For example, when \\ntraining an agent to play a game the trainer might provide a positive reward when the \\ngame is won, negative reward when it is lost, and zero reward in all other states. The \\ntask of the agent is to learn from this indirect, delayed reward, to choose sequences \\nof actions that produce the greatest cumulative reward. This chapter focuses on \\nan algorithm called Q learning that can acquire optimal control strategies from \\ndelayed rewards, even when the agent has no prior knowledge of the effects of \\nits actions on the environment. Reinforcement learning algorithms are related to \\ndynamic programming algorithms frequently used to solve optimization problems. \\n13.1 INTRODUCTION \\nConsider building a learning robot. The robot, or agent, has a set of sensors to \\nobserve the state of its environment, and a set of actions it can perform to alter \\nthis state. For example, a mobile robot may have sensors such as a camera and \\nsonars, and actions such as \"move forward\" and \"turn.\" Its task is to learn a control \\nstrategy, or policy, for choosing actions that achieve its goals. For example, the \\nrobot may have a goal of docking onto its battery charger whenever its battery \\nlevel is low. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 379}, page_content=\"This chapter is concerned with how such agents can learn successful control \\npolicies by experimenting in their environment. We assume that the goals of the \\nagent can be defined by a reward function that assigns a numerical value-an \\nimmediate payoff-to each distinct action the agent may take from each distinct \\nstate. For example, the goal of docking to the battery charger can be captured by \\nassigning a positive reward \\n(e.g., \\n+loo) \\nto state-action transitions that immediately \\nresult in a connection to the charger and a reward of zero to every other state-action \\ntransition. This reward function may be built into the robot, or known only to an \\nexternal teacher who provides the reward value for each action performed by the \\nrobot. The task of the robot is to perform sequences of actions, observe their conse- \\nquences, and learn a control policy. The control policy we desire is one that, from \\nany initial state, chooses actions that maximize the reward accumulated over time \\nby the agent. This general setting for robot learning is summarized in Figure 13.1. \\nAs is apparent from Figure 13.1, the problem of learning a control policy to \\nmaximize cumulative reward is very general and covers many problems beyond \\nrobot learning tasks. In general the problem is one of learning to control sequential \\nprocesses. This includes, for example, manufacturing optimization problems in \\nwhich a sequence of manufacturing actions must be chosen, and the reward to \\nbe maximized is the value of the goods produced minus the costs involved. It \\nincludes sequential scheduling problems such as choosing which taxis to send \\nfor passengers in a large city, where the reward to be maximized is a function \\nof the wait time of the passengers and the total fuel costs of the taxi fleet. In \\ngeneral, we are interested in any type of agent that must learn to choose actions \\nthat alter the state of its environment and where a cumulative reward function \\nis used to define the quality of any given action sequence. Within this class of \\nproblems we will consider specific settings, including settings in which the actions \\nhave deterministic or nondeterministic outcomes, and settings in which the agent \\nAgent \\nI Environment I \\nGoal: Learn to choose actions that maximize \\nr +yr +y2r + ... , where 0 gy<l 01 2 \\nFIGURE 13.1 \\nAn agent interacting with its environment. \\nThe agent exists in an environment described \\nby some set of possible states S. It can \\nperform any of a set of possible actions \\nA. Each time it performs an action a, in \\nsome state st the agent receives a real-valued \\nreward r, that indicates the immediate value \\nof this state-action transition. This produces \\na sequence of states si, actions ai, and \\nimmediate rewards ri as shown in the figure. \\nThe agent's task is to learn a control policy, \\nn : S + A, that maximizes the expected \\nsum of these rewards, with future rewards \\ndiscounted exponentially by their delay. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 380}, page_content=\"has or does not have prior knowledge about the effects of its actions on the \\nenvironment. \\nNote we have touched on the problem of learning to control sequential \\nprocesses earlier in this book. In Section 11.4 we discussed explanation-based \\nlearning of rules to control search during problem solving. There the problem is \\nfor the agent to choose among alternative actions at each step in its search for some \\ngoal state. The techniques discussed here differ from those of Section 11.4, in that \\nhere we consider problems where the actions may have nondeterministic outcomes \\nand where the learner lacks a domain theory that describes the outcomes of its \\nactions. In Chapter 1 we discussed the problem of learning to choose actions while \\nplaying the game of checkers. There we sketched the design of a learning method \\nvery similar to those discussed in this chapter. In fact, one highly successful \\napplication of the reinforcement learning algorithms of this chapter is to a similar \\ngame-playing problem. Tesauro (1995) describes the TD-GAMMON program, which \\nhas used reinforcement learning to become a world-class backgammon player. This \\nprogram, after training on 1.5 million self-generated games, is now considered \\nnearly equal to the best human players in the world and has played competitively \\nagainst top-ranked players in international backgammon tournaments. \\nThe problem of learning a control policy to choose actions is similar in some \\nrespects to the function approximation problems discussed in other chapters. The \\ntarget function to be learned in this case is a control policy, n : S + A, that \\noutputs an appropriate action a from the set A, given the current state s from the \\nset S. However, this reinforcement learning problem differs from other function \\napproximation tasks in several important respects. \\n0 Delayed reward. The task of the agent is to learn a target function n that \\nmaps from the current state s to the optimal action a = n(s). In earlier \\nchapters we have always assumed that when learning some target function \\nsuch as n, each training example would be a pair of the form (s, n(s)). In \\nreinforcement learning, however, training information is not available in this \\nform. Instead, the trainer provides only a sequence of immediate reward val- \\nues as the agent executes its sequence of actions. The agent, therefore, faces \\nthe problem of temporal credit assignment: determining which of the actions \\nin its sequence are to be credited with producing the eventual rewards. \\n0 Exploration. In reinforcement learning, the agent influences the distribution \\nof training examples by the action sequence it chooses. This raises the ques- \\ntion of which experimentation strategy produces most effective learning. The \\nlearner faces a \\ntradeoff \\nin choosing whether to favor exploration of unknown \\nstates and actions (to gather new information), or exploitation of states and \\nactions that it has already learned will yield high reward (to maximize its \\ncumulative reward). \\n0 Partially observable states. Although it is convenient to assume that the \\nagent's sensors can perceive the entire state of the environment at each time \\nstep, in many practical situations sensors provide only partial information. \\nFor example, a robot with a forward-pointing camera cannot see what is \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 381}, page_content=\"behind it. In such cases, it may be necessary for the agent to consider its \\nprevious observations together with its current sensor data when choosing \\nactions, and the best policy may be one that chooses actions specifically to \\nimprove the observability of the environment. \\nLife-long learning. Unlike isolated function approximation tasks, robot learn- \\ning often requires that the robot learn several related tasks within the same \\nenvironment, using the same sensors. For example, a mobile robot may need \\nto learn how to dock on its battery charger, how to navigate through nar- \\nrow corridors, and how to pick up output from laser printers. This setting \\nraises the possibility of using previously obtained experience or knowledge \\nto reduce sample complexity when learning new tasks. \\n13.2 THE LEARNING TASK \\nIn this section we formulate the problem of learning sequential control strategies \\nmore precisely. Note there are many ways to do so. For example, we might assume \\nthe agent's actions are deterministic or that they are nondeterministic. We might \\nassume that the agent can predict the next state that will result from each action, or \\nthat it cannot. We might assume that the agent is trained by an expert who shows \\nit examples of optimal action sequences, or that it must train itself by performing \\nactions of its own choice. Here we define one quite general formulation of the \\nproblem, based on Markov decision processes. This formulation of the problem \\nfollows the problem illustrated in Figure 13.1. \\nIn a Markov decision process (MDP) the agent can perceive a set S of distinct \\nstates of its environment and has a set A of actions that it can perform. At each \\ndiscrete time step t, the agent senses the current state st, chooses a current action \\na,, and performs it. The environment responds by giving the agent a reward r, = \\nr (st, a,) and by producing the succeeding state s,+l = 6(s,, a,). Here the functions \\n6 and r are part of the environment and are not necessarily known to the agent. \\nIn an MDP, the functions 6(st, a,) and r(s,, a,) depend only on the current state \\nand action, and not on earlier states or actions. In this chapter we consider only \\nthe case in which S and A are finite. In general, 6 and r may be nondeterministic \\nfunctions, but we begin by considering only the deterministic case. \\nThe task of the agent is to learn a policy, n : S + A, for selecting its next \\naction a, based on the current observed state st; that is, n(s,) = a,. How shall we \\nspecify precisely which policy n we would like the agent to learn? One obvious \\napproach is to require the policy that produces the greatest possible cumulative \\nreward for the robot over time. To state this requirement more precisely, we define \\nthe cumulative value Vn(s,) achieved by following an arbitrary policy n from an \\narbitrary initial state st as follows: \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 382}, page_content='CHAPTER 13 REINFORCEMENT LEARNING 371 \\nwhere the sequence of rewards rt+i is generated by beginning at state s, and by \\nrepeatedly using the policy n to select actions as described above \\n(i.e., a, = n(st), \\na,+l = n(~,+~), etc.). Here 0 5 y < 1 is a constant that determines the relative \\nvalue of delayed versus immediate rewards. In particular, rewards received i time \\nsteps into the future are discounted exponentially by a factor of y \\'. Note if we set \\ny = 0, only the immediate reward is considered. As we set y closer to 1, future \\nrewards are given greater emphasis relative to the immediate reward. \\nThe quantity \\nVX(s) \\ndefined by Equation (13.1) is often called the discounted \\ncumulative reward achieved by policy n from initial state s. It is reasonable to \\ndiscount future rewards relative to immediate rewards because, in many cases, \\nwe prefer to obtain the reward sooner rather than later. However, other defini- \\ntions of total reward have also been explored. For example, \\njinite \\nhorizon reward, \\nc:=, rt+i, considers the undiscounted sum of rewards over a finite number h of . - \\nsteps. Another possibility is average reward, limb,, cF=~ rt+i, which consid- \\ners the average reward per time step over the entire lifetime of the agent. In \\nthis chapter we restrict ourselves to considering discounted reward as defined \\nby Equation (13.1). Mahadevan (1996) provides a discussion of reinforcement \\nlearning when the criterion to be optimized is average reward. \\nWe are now in a position to state precisely the agent\\'s learning task. We \\nrequire that the agent learn a policy n that maximizes \\nV\"(s) \\nfor all states s. \\nWe will call such a policy an optimal policy and denote it by T*. \\nn* r argmax V\" (s), (Vs) \\nX \\nTo simplify notation, we will refer to the value function v\"*(s) of such an optimal \\npolicy as \\nV*(s). \\nV*(s) \\ngives the maximum discounted cumulative reward that the \\nagent can obtain starting from state s; that is, the discounted cumulative reward \\nobtained by following the optimal policy beginning at state s. \\nTo illustrate these concepts, a simple grid-world environment is depicted \\nin the topmost diagram of Figure 13.2. The six grid squares in this diagram \\nrepresent six possible states, or locations, for the agent. Each arrow in the diagram \\nrepresents a possible action the agent can take to move from one state to another. \\nThe number associated with each arrow represents the immediate reward \\nr(s, \\na) \\nthe agent receives if it executes the corresponding state-action transition. Note \\nthe immediate reward in this particular environment is defined to be zero for \\nall state-action transitions except for those leading into the state labeled G. It is \\nconvenient to think of the state G as the goal state, because the only way the agent \\ncan receive reward, in this case, is by entering this state. Note in this particular \\nenvironment, the only action available to the agent once it enters the state G is \\nto remain in this state. For this reason, we call G an absorbing state. \\nOnce the states, actions, and immediate rewards are defined, and once we \\nchoose a value for the discount factor y, we can determine the optimal policy n* \\nand its value function \\nV*(s). In this case, let us choose y = 0.9. The diagram \\nat the bottom of the figure shows one optimal policy for this setting (there are \\nothers as well). Like any policy, this policy specifies exactly one action that the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 383}, page_content='r (s, a) (immediate reward) values \\nQ(s, \\na) values \\nV*(s) \\nvalues \\nOne optimal policy \\nFIGURE 13.2 \\nA simple deterministic world to illustrate the basic concepts of Q-learning. Each grid square represents \\na distinct state, each arrow a distinct action. The immediate reward function, r(s, a) gives reward 100 \\nfor actions entering the goal state G, and zero otherwise. Values of V*(s) and Q(s, a) follow from \\nr(s, a), and the discount factor y = 0.9. An optimal policy, corresponding to actions with maximal \\nQ values, is also shown. \\nagent will select in any given state. Not surprisingly, the optimal policy directs \\nthe agent along the shortest path toward the state G. \\nThe diagram at the right of Figure 13.2 shows the values of V* for each \\nstate. For example, consider the bottom right state in this diagram. The value of \\nV* for this state is 100 because the optimal policy in this state selects the \"move \\nup\" action that receives immediate reward 100. Thereafter, the agent will remain \\nin the absorbing state and receive no further rewards. Similarly, the value of V* \\nfor the bottom center state is 90. This is because the optimal policy will move \\nthe agent from this state to the right (generating an immediate reward of zero), \\nthen upward (generating an immediate reward of 100). Thus, the discounted future \\nreward from the bottom center state is \\no+y100+y20+Y30+...=90 '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 384}, page_content=\"CHAPTER 13 REINFORCEMENT LEARNING 373 \\nRecall that V* is defined to be the sum of discounted future rewards over the \\ninfinite future. In this particular environment, once the agent reaches the absorbing \\nstate G its infinite future will consist of remaining in this state and receiving \\nrewards of zero. \\n13.3 Q LEARNING \\nHow can an agent learn an optimal policy n* for an arbitrary environment? It is \\ndifficult to learn the function rt* : S + A directly, because the available training \\ndata does not provide training examples of the form (s, a). Instead, the only \\ntraining information available to the learner is the sequence of immediate rewards \\nr(si, ai) for i = 0, 1,2, . . . . As we shall see, given this kind of training information \\nit is easier to learn a numerical evaluation function defined over states and actions, \\nthen implement the optimal policy in terms of this evaluation function. \\nWhat evaluation function should the agent attempt to learn? One obvious \\nchoice is V*. The agent should prefer state sl over state s2 whenever V*(sl) > \\nV*(s2), because the cumulative future reward will be greater from sl. Of course \\nthe agent's policy must choose among actions, not among states. However, it can \\nuse V* in certain settings to choose among actions as well. The optimal action \\nin state s is the action a that maximizes the sum of the immediate reward r(s, a) \\nplus the value V* of the immediate successor state, discounted by y. \\nn*(s) = argmax[r(s, a) f y V*(G(s, a))] \\na \\n(recall that 6(s, a) denotes the state resulting from applying action a to state s.) \\nThus, the agent can acquire the optimal policy by learning V*, provided it has \\nperfect knowledge of the immediate reward function r and the state transition \\nfunction 6. When the agent knows the functions r and 6 used by the environment \\nto respond to its actions, it can then use Equation (13.3) to calculate the optimal \\naction for any state s. \\nUnfortunately, learning V* is a useful way to learn the optimal policy only \\nwhen the agent has perfect knowledge of 6 and r. This requires that it be able to \\nperfectly predict the immediate result \\n(i.e., \\nthe immediate reward and immediate \\nsuccessor) for every possible state-action transition. This assumption is compara- \\nble to the assumption of a perfect domain theory in explanation-based learning, \\ndiscussed in Chapter 11. In many practical problems, such as robot control, it \\nis impossible for the agent or its human programmer to predict in advance the \\nexact outcome of \\napplying \\nan arbitrary action to an arbitrary state. Imagine, for \\nexample, the difficulty in describing 6 for a robot arm shoveling dirt when the \\nresulting state includes the positions of the dirt particles. In cases where either \\n6 or r is unknown, learning V* is unfortunately of no use for selecting optimal \\nactions because the agent cannot evaluate Equation (13.3). What evaluation func- \\ntion should the agent use in this more general setting? The evaluation function Q, \\ndefined in the following section, provides one answer. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 385}, page_content=\"374 MACHINE LEARNING \\n13.3.1 The Q Function \\nLet us define the evaluation function Q(s, a) so that its value is the maximum dis- \\ncounted cumulative reward that can be achieved starting from state s and applying \\naction a as the first action. In other words, the value of Q is the reward received \\nimmediately upon executing action a from state s, plus the value (discounted by \\ny) of following the optimal policy thereafter. \\nQ(s, a) - r(s, a) + Y V*(6(s, a)) (1 3.4) \\nNote that Q(s, a) is exactly the quantity that is maximized in Equation (13.3) \\nin order to choose the optimal action a in state s. Therefore, we can rewrite \\nEquation (13.3) in terms of Q(s, a) as \\nn * (s) = argmax Q (s , a) (13.5) \\na \\nWhy is this rewrite important? Because it shows that if the agent learns the Q \\nfunction instead of the V* function, it will be able to select optimal actions even \\nwhen it has no knowledge of thefunctions r and 6. As Equation (13.5) makes clear, \\nit need only consider each available action a in its current state s and choose the \\naction that maximizes Q(s, a). \\nIt may at first seem surprising that one can choose globally optimal action \\nsequences by reacting repeatedly to the local values of Q for the current state. \\nThis means the agent can choose the optimal action without ever conducting a \\nlookahead search to explicitly consider what state results from the action. Part of \\nthe beauty of Q learning is that the evaluation function is defined to have precisely \\nthis property-the value of Q for the current state and action summarizes in a \\nsingle number all the information needed to determine the discounted cumulative \\nreward that will be gained in the future if action a is selected in state s. \\nTo illustrate, Figure 13.2 shows the Q values for every state and action in the \\nsimple grid world. Notice that the Q value for each state-action transition equals \\nthe r value for this transition plus the V* value for the resulting state discounted by \\ny. Note also that the optimal policy shown in the figure corresponds to selecting \\nactions with maximal Q values. \\n13.3.2 An Algorithm for Learning Q \\nLearning the Q function corresponds to learning the optimal policy. How can Q \\nbe learned? \\nThe key problem is finding a reliable way to estimate training values for \\nQ, given only a sequence of immediate rewards r spread out over time. This can \\nbe accomplished through iterative approximation. To see how, notice the close \\nrelationship between Q and V*, \\nV*(S) = max Q(s, a') \\na' \\nwhich allows rewriting Equation (13.4) as \\nQ(s, a) = r(s, a) + y max Q(W, a), a') a' \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 386}, page_content=\"CHAPTER 13 REINFORCEMENT LEARNJNG 375 \\nThis recursive definition of Q provides the basis for algorithms that iter- \\natively approximate Q (Watkins 1989). To describe the algorithm, we will use \\nthe symbol Q to refer to the learner's estimate, or hypothesis, of the actual Q \\nfunction. In this algorithm the learner represents its hypothesis Q by a large table \\nwith a separate entry for each state-action pair. The table entry for the pair (s, a) \\nstores the value for \\n~(s, \\na)-the learner's current hypothesis about the actual \\nbut unknown value \\nQ(s, \\na). The table can be initially filled with random values \\n(though it is easier to understand the algorithm if one assumes initial values of \\nzero). The agent repeatedly observes its current state s, chooses some action a, \\nexecutes this action, then observes the resulting reward r = r(s, a) and the new \\nstate \\ns' = 6(s, a). It then updates the table entry for ~(s, a) following each such \\ntransition, according to the rule: \\nQ(S, \\na) t r + y max &(st, a') \\na' \\n(13.7) \\nNote this training rule uses the agent's current Q values for the new state \\ns' \\nto refine its estimate of \\n~(s, \\na) for the previous state s. This training rule \\nis motivated by Equation \\n(13.6), \\nalthough the training rule concerns the agent's \\napproximation Q, whereas Equation (13.6) applies to the actual Q function. Note \\nalthough Equation (13.6) describes Q in terms of the functions \\n6(s, \\na) and \\nr(s, a), \\nthe agent does not need to know these general functions to apply the training \\nrule of Equation (13.7). Instead it executes the action in its environment and \\nthen observes the resulting new state \\ns' \\nand reward r. Thus, it can be viewed as \\nsampling these functions at the current values of \\ns \\nand a. \\nThe above Q learning algorithm for deterministic Markov decision processes \\nis described more precisely in Table 13.1. Using this algorithm the agent's estimate \\nQ converges in the limit to the actual Q function, provided the system can be \\nmodeled as a deterministic Markov decision process, the reward function r is \\nQ learning algorithm \\nFor each s, a initialize the table entry ~(s, a) to zero. \\nObserve the current state s \\nDo forever: \\nSelect an action a and execute it \\nReceive immediate reward r \\nObserve the new state s' \\nUpdate the table entry for ~(s, a) as follows: \\n~(s,a) cr + ymax&(s',af) \\na' \\nS CS' \\nTABLE 13.1 \\nQ learning algorithm, assuming deterministic rewards and actions. The discount factor y may be any \\nconstant such that 0 5 y < 1. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 387}, page_content='bounded, and actions are chosen so that every state-action pair is visited infinitely \\noften. \\n13.3.3 An Illustrative Example \\nTo illustrate the operation of the Q learning algorithm, consider a single action \\ntaken by an agent, and the corresponding refinement to Q shown in Figure 13.3. \\nIn this example, the agent moves one cell to the right in its grid world and receives \\nan immediate reward of zero for this transition. It then applies the training rule \\nof Equation (13.7) to refine its estimate Q for the state-action transition it just \\nexecuted. According to the training rule, the new Q estimate for this transition \\nis the sum of the received reward (zero) and the highest Q value associated with \\nthe resulting state \\n(loo), \\ndiscounted by y (.9). \\nEach time the agent moves forward from an old state to a new one, Q \\nlearning propagates Q estimates backward from the new state to the old. At the \\nsame time, the immediate reward received by the agent for the transition is used \\nto augment these propagated values of Q. \\nConsider applying this algorithm to the grid world and reward function \\nshown in Figure 13.2, for which the reward is zero everywhere, except when \\nentering the goal state. Since this world contains an absorbing goal state, we will \\nassume that training consists of a series of episodes. During each episode, the \\nagent begins at some randomly chosen state and is allowed to execute actions \\nuntil it reaches the absorbing goal state. When it does, the episode ends and \\nInitial state: S] Next state: S2 \\nFIGURE 13.3 \\nThe update to Q after executing a single^ action. The diagram on the left shows the initial state \\ns! of the robot (R) and several relevant Q values in its initial hypothesis. For example, the value \\nQ(s1, aright) = 72.9, where aright refers to the action that moves R to its right. When the robot \\nexecutes the action aright, it receives immediate reward r = 0 and transitions to state s2. It then \\nupdates its estimate i)(sl, aright) based on its Q estimates for the new state s2. Here y = 0.9. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 388}, page_content='CHAPTER 13 REINFORCEMENT LEARNING 377 \\nthe agent is transported to a new, randomly chosen, initial state for the next \\nepisode. \\nHow will the values of Q evolve as the Q learning algorithm is applied in \\nthis case? With all the Q values initialized to zero, the agent will make no changes \\nto any Q table entry until it happens to reach the goal state and receive a nonzero \\nreward. This will result in refining the Q value for the single transition leading \\ninto the goal state. On the next episode, if the agent passes through this state \\nadjacent to the goal state, its nonzero Q value will allow refining the value for \\nsome transition two steps from the goal, and so on. Given a sufficient number of \\ntraining episodes, the information will propagate from the transitions with nonzero \\nreward back through the entire state-action space available to the agent, resulting \\neventually in a Q table containing the Q values shown in Figure 13.2. \\nIn the next section we prove that under certain assumptions the Q learning \\nalgorithm of Table 13.1 will converge to the correct Q function. First consider \\ntwo general properties of this Q learning algorithm that hold for any deterministic \\nMDP in which the rewards are non-negative, assuming we initialize all Q values to \\nzero. The first property is that under these conditions the Q values never decrease \\nduring training. More formally, let \\nQ,(s, \\na) denote the learned \\n~(s, \\na) value after \\nthe nth iteration of the training procedure \\n(i.e., \\nafter the nth state-action transition \\ntaken by the agent). Then \\nA second general property that holds under these same conditions is that through- \\nout the training process every Q value wi:l remain in the interval between zero \\nand its true Q value. \\n13.3.4 Convergence \\nWill the algorithm of Table 13.1 converge toward a Q equal to the true Q function? \\nThe answer is yes, under certain conditions. First, we must assume the system is \\na deterministic MDP. Second, we must assume the immediate reward values are \\nbounded; that is, there exists some positive constant c such that for all states s \\nand actions a, \\nIr(s, \\na)l < c. Third, we assume the agent selects actions in such \\na fashion that it visits every possible state-action pair infinitely often. By this \\nthird condition we mean that if action a is a legal action from state s, then over \\ntime the agent must execute action a from state \\ns \\nrepeatedly and with nonzero \\nfrequency as the length of its action sequence approaches infinity. Note these \\nconditions are in some ways quite general and in others fairly restrictive. They \\ndescribe a more general setting than illustrated by the example in the previous \\nsection, because they allow for environments with arbitrary positive or negative \\nrewards, and for environments where any number of state-action transitions may \\nproduce nonzero rewards. The conditions are also restrictive in that they require \\nthe agent to visit every distinct state-action transition infinitely often. This is a \\nvery strong assumption in large (or continuous!) domains. We will discuss stronger '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 389}, page_content='convergence results later. However, the result described in this section provides \\nthe basic intuition for understanding why Q learning works. \\nThe key idea underlying the proof of convergence is that the table entry \\n~(s, a) with the largest error must have its error reduced by a factor of y whenever \\nit is updated. The reason is that its new value depends only in part on error-prone \\nQ estimates, with the remainder depending on the error-free observed immediate \\nreward r. \\nTheorem 13.1. Convergence of Q learning for deterministic Markov decision \\nprocesses. Consider a Q learning agent in a deterministic MDP with bounded re- \\nwards (Vs, a) lr(s, a)[ 5 c. The* Q learning agent uses the training rule of Equa- \\ntion (13.7), initializes its table Q(s, a) to arbitrary finite values, and uses a discount \\nfactor y such that 0 y < 1. Let Q,(s, a) denote the agent\\'s hypothesis ~(s, a) \\nfollowing the nth update. If each state-action pair is visited infinitely often, then \\nQ,(s, a) converges to Q(s, a) as n + oo, for all s, a. \\nProof. Since each state-action transition occurs infinitely often, consider consecutive \\nintervals during which each state-action transition occurs at least once. The proof \\nconsists of showing that the maximum error over all entries in the Q table is reduced \\nby at least a factor of y during each such interval. Q, is the agent\\'s table of estimated \\nQ values after n updates. Let An be the maximum error in Q,; that is \\nBelow we use s\\' to denote S(s, a). Now for any table entry (in@, a) that is updated \\non iteration \\nn + 1, the magnitude of the error in the revised estimate Q,+~(S, a) is \\nIQ,+I(S, a) - Q(s, all = I(r + y max Qn(s\\', a\\')) - (r + y m?x Q(d, a\\'))] \\na\\' a \\n= y I my Qn(st, a\\') - my Q(s1, a\\') I \\na a \\n5 y max I Qn(s1, a\\') - ~(s\\', a\\') I \\na\\' \\n5 Y my I Q, (s\", a\\') - QW, a\\') I \\ns ,a \\nI Qn+i (s, a) - Q(s, all 5 Y An \\nThe third line above follows from the second line because for any two functions fi \\nand f2 the following inequality holds \\nIn going from the third line to the fourth line above, note we introduce a new \\nvariable s\" over which the maximization is performed. This is legitimate because \\nthe maximum value will be at least as great when we allow this additional variable \\nto vary. Note that by introducing this variable we obtain an expression that matches \\nthe definition of A,. \\nThus, the updated Q,+~(S, a) for any s, a is at most y times the maximum \\nerror in the Q,, table, A,. The largest error in the initial table, Ao, is bounded because \\nvalues of ~~(s, a) and Q(s, a) are bounded for all s, a. Now after the first interval '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 390}, page_content='CHAPTER 13 REINFORCEMENT LEARNING 379 \\nduring which each s, a is visited, the largest error in the table will be at most yAo. \\nAfter k such intervals, the error will be at most ykAo. Since each state is visited \\ninfinitely often, the number of such intervals is infinite, and A, -+ 0 as n + oo. \\nThis proves the theorem. 0 \\n13.3.5 Experimentation Strategies \\nNotice the algorithm of Table 13.1 does not specify how actions are chosen by the \\nagent. One obvious strategy would be for the agent in state s to select the action a \\nthat maximizes ~(s, a), thereby exploiting its current approximation Q. However, \\nwith this strategy the agent runs the risk that it will overcommit to actions that \\nare found during early training to have high Q values, while failing to explore \\nother actions that have even higher values. In fact, the convergence theorem above \\nrequires that each state-action transition occur infinitely often. This will clearly \\nnot occur if the agent always selects actions that maximize its current &(s, a). For \\nthis reason, it is common in Q learning to use a probabilistic approach to selecting \\nactions. Actions with higher Q values are assigned higher probabilities, but every \\naction is assigned a nonzero probability. One way to assign such probabilities is \\nwhere P(ai 1s) is the probability of selecting action ai, given that the agent is in \\nstate s, and where k > 0 is a constant that determines how strongly the selection \\nfavors actions with high Q values. Larger values of k will assign higher proba- \\nbilities to actions with above average Q, causing the agent to exploit what it has \\nlearned and seek actions it believes will maximize its reward. In contrast, small \\nvalues of k will allow higher probabilities for other actions, leading the agent \\nto explore actions that do not currently have high Q values. In some cases, k is \\nvaried with the number of iterations so that the agent favors exploration during \\nearly stages of learning, then gradually shifts toward a strategy of exploitation. \\n13.3.6 Updating Sequence \\nOne important implication of the above convergence theorem is that Q learning \\nneed not train on optimal action sequences in order to converge to the optimal \\npolicy. In fact, it can learn the Q function (and hence the optimal policy) while \\ntraining from actions chosen completely at random at each step, as long as the \\nresulting training sequence visits every state-action transition infinitely often. This \\nfact suggests changing the sequence of training example transitions in order to \\nimprove training efficiency without endangering final convergence. To illustrate, \\nconsider again learning in an MDP with a single absorbing goal state, such as the \\none in Figure 13.1. Assume as before that we train the agent with a sequence of \\nepisodes. For each episode, the agent is placed in a random initial state and is \\nallowed to perform actions and to update its Q table until it reaches the absorbing \\ngoal state. A new training episode is then begun by removing the agent from the '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 391}, page_content=\"goal state and placing it at a new random initial state. As noted earlier, if we \\nbegin with all Q values initialized to zero, then after the first full episode only \\none entry in the agent's Q table will have been changed: the entry corresponding \\nto the final transition into the goal state. Note that if the agent happens to follow \\nthe same sequence of actions from the same random initial state in its second full \\nepisode, then a second table entry would be made nonzero, and so on. If we run \\nrepeated identical episodes in this fashion, the frontier of nonzero Q values will \\ncreep backward from the goal state at the rate of one new state-action transition \\nper episode. Now consider training on these same state-action transitions, but in \\nreverse chronological order for each episode. That is, we apply the same update \\nrule from Equation (13.7) for each transition considered, but perform these updates \\nin reverse order. In this case, after the first full episode the agent will have updated \\nits Q estimate for every transition along the path it took to the goal. This training \\nprocess will clearly converge in fewer iterations, although it requires that the agent \\nuse more memory to store the entire episode before beginning the training for that \\nepisode. \\nA second strategy for improving the rate of convergence is to store past \\nstate-action transitions, along with the immediate reward that was received, and \\nretrain on them periodically. Although at first it might seem a waste of effort to \\nretrain on the same transition, recall that the updated ~(s, a) value is determined \\nby the values ~(s', a) of the successor state s' = 6(s, a). Therefore, if subsequent \\ntraining changes one of the ~(s', a) values, then retraining on the transition (s, a) \\nmay result in an altered value for ~(s, a). In general, the degree to which we wish \\nto replay old transitions versus obtain new ones from the environment depends \\non the relative costs of these two operations in the specific problem domain. For \\nexample, in a robot domain with navigation actions that might take several seconds \\nto perform, the delay in collecting a new state-action transition from the external \\nworld might be several orders of magnitude more costly than internally replaying \\na previously observed transition. This difference can be very significant given that \\nQ learning can often require thousands of training iterations to converge. \\nNote throughout the above discussion we have kept our assumption that the \\nagent does not know the state-transition function 6(s, a) used by the environment \\nto create the successor state s' = S(s, a), or the function r(s, a) used to generate \\nrewards. If it does know these two functions, then many more efficient methods \\nare possible. For example, if performing external actions is expensive the agent \\nmay simply ignore the environment and instead simulate it internally, efficiently \\ngenerating simulated actions and assigning the appropriate simulated rewards. \\nSutton \\n(1991) describes the DYNA architecture that performs a number of simulated \\nactions after each step executed in the external world. Moore and Atkeson (1993) \\ndescribe an approach called prioritized sweeping that selects promising states to \\nupdate next, focusing on predecessor states when the current state is found to \\nhave a large update. Peng and Williams (1994) describe a similar approach. A \\nlarge number of efficient algorithms from the field of dynamic programming can \\nbe applied when the functions 6 and r are known. Kaelbling et al. (1996) survey \\na number of these. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 392}, page_content='CHAPTER 13 REINFORCEMENT LEARNING 381 \\n13.4 NONDETERMINISTIC REWARDS AND ACTIONS \\nAbove we considered Q learning in deterministic environments. Here we consider \\nthe nondeterministic case, in which the reward function r(s, a) and action transi- \\ntion function 6(s, a) may have probabilistic outcomes. For example, in Tesaur~\\'~ \\n(1995) backgammon playing program, action outcomes are inherently probabilis- \\ntic because each move involves a roll of the dice. Similarly, in robot problems \\nwith noisy sensors and effectors it is often appropriate to model actions and re- \\nwards as nondeterministic. In such cases, the functions 6(s, a) and r(s, a) can be \\nviewed as first producing a probability distribution over outcomes based on s and \\na, and then drawing an outcome at random according to this distribution. When \\nthese probability distributions depend solely on s and a (e.g., they do not depend \\non previous states or actions), then we call the system a nondeterministic Markov \\ndecision process. \\nIn this section we extend the Q learning algorithm for the deterministic \\ncase to handle nondeterministic MDPs. To accomplish this, we retrace the line \\nof argument that led to the algorithm for the deterministic case, revising it where \\nneeded. \\nIn the nondeterministic case we must first restate the objective of the learner \\nto take into account the fact that outcomes of actions are no longer \\ndeterministic. \\nThe obvious generalization is to redefine the value V\" of a policy n to be the ex- \\npected value (over these nondeterministic outcomes) of the discounted cumulative \\nreward received by applying this policy \\nwhere, as before, the sequence of rewards r,+i is generated by following policy \\nn beginning at state s. Note this is a generalization of Equation (13.1), which \\ncovered the deterministic case. \\nAs before, we define the optimal policy n* to be the policy n that maxi- \\nmizes V\"(s) for all states s. Next we generalize our earlier definition of Q from \\nEquation \\n(13.4), \\nagain by taking its expected value. \\nwhere P(slls, a) is the probability that taking action a in state s will produce the \\nnext state s\\'. Note we have used P(slls, a) here to rewrite the expected value of \\nV*(6(s, a)) in terms of the probabilities associated with the possible outcomes of \\nthe probabilistic 6. \\nAs before we can re-express Q recursively \\nQ(s, a) = E[r(s, a)] + y P(sfls, a) my Q(sl, a\\') (13.9) \\nS\\' \\na '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 393}, page_content=\"which is the generalization of the earlier Equation (13.6). To summarize, we have \\nsimply redefined Q(s, a) in the nondeterministic case to be the expected value of \\nits previously defined quantity for the deterministic case. \\nNow that we have generalized the definition of Q to accommodate the non- \\ndeterministic environment functions r and 6, a new training rule is needed. Our \\nearlier training rule derived for the deterministic case (Equation 13.7) fails to con- \\nverge in this nondeterministic setting. Consider, for example, a nondeterministic \\nreward function r(s, a) that produces different rewards each time the transition \\n(s, a} is repeated. In this case, the training rule will repeatedly alter the values of \\nQ(S, a), even if we initialize the Q table values to the correct Q function. In brief, \\nthis training rule does not converge. This difficulty can be overcome by modifying \\nthe training rule so that it takes a decaying weighted average of the current Q \\nvalue and the revised estimate. Writing Q, to denote the agent's estimate on the \\nnth iteration of the algorithm, the following revised training rule is sufficient to \\nassure convergence of Q to Q: \\nQ~(s, \\na) -+ (1 - un)Qn-l(s, a) + a,[r + y max Q,-~(S', a')] \\nat (13.10) \\nwhere \\na, = 1 \\n1 + visits, (s, a) \\nwhere s and a here are the state and action updated during the nth iteration, and \\nwhere visits,(s, a) is the total number of times this state-action pair has been \\nvisited up to and including the nth iteration. \\nThe key idea in this revised rule is that revisions to Q are made more \\ngradually than in the deterministic case. Notice if we were to set a, to 1 in \\nEquation (13.10) we would have exactly the training rule for the deterministic case. \\nWith smaller values of a, this term is now averaged in with the current ~(s, a) to \\nproduce the new updated value. Notice that the value of a, in Equation (13.11) \\ndecreases as n increases, so that updates become smaller as training progresses. \\nBy reducing a at an appropriate rate during training, we can achieve convergence \\nto the correct Q function. The choice of a, given above is one of many that \\nsatisfy the conditions for convergence, according to the following theorem due to \\nWatkins and \\nDayan (1992). \\nTheorem 13.2. Convergence of Q learning for nondeterministic Markov de- \\ncision processes. Consider a Q learning agent in a nondeterministic MDP with \\nbounded rewards (Vs, a)lr(s, a)l 5 c. The Q learning agent uses the training rule of \\nEquation (13.10), initializes its table ~(s, a) to arbitrary finite values, and uses a \\ndiscount factor y such that 0 5 y < 1. Let n(i, s, a) be the iteration corresponding \\nto the ith time that action a is applied to state s. If each state-action pair is visited \\ninfinitely often, 0 5 a,, < 1, and \\nthen for all s and a, &,(s, a) + Q(s, a) as n + 00, with probability 1. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 394}, page_content=\"While Q learning and related reinforcement learning algorithms can be \\nproven to converge under certain conditions, in practice systems that use Q learn- \\ning often require many thousands of training iterations to converge. For exam- \\nple, Tesauro's TD-GAMMON discussed earlier trained for 1.5 million backgammon \\ngames, each of which contained tens of state-action transitions. \\n13.5 TEMPORAL DIFFERENCE LEARNING \\nThe Q learning algorithm learns by iteratively reducing the discrepancy between \\nQ value estimates for adjacent state:,. In this sense, Q learning is a special case \\nof a general class of temporal diflerence algorithms that learn by reducing dis- \\ncrepancies between estimates made by the agent at different times. Whereas the \\ntraining rule of Equation (13.10) reduces the difference between the estimated Q \\nvalues of a state and its immediate successor, we could just as well design an algo- \\nrithm that reduces discrepancies between this state and more distant descendants \\nor ancestors. \\nTo explore this issue further, recall that our Q learning training rule calcu- \\nlates a training value for &(st, a,) in terms of the values for &(s,+l, at+l) where \\ns,+l is the result of applying action a, to the state st. Let Q(')(s,, a,) denote the \\ntraining value calculated by this one-step lookahead \\nOne alternative way to compute a training value for Q(s,, a,) is to base it on the \\nobserved rewards for two steps \\n2  st, a,) = rt + yr,+l + y max Q(s~+~, a) \\nor, in general, for n steps \\nQ(~)(s,,~,) = rt + yr,+l + ,-. + y(n-l)rt+n-l + ynmax&(s,+,,a) \\nSutton (1988) introduces a general method for blending these alternative \\ntraining estimates, called TD(h). The idea is to use a constant 0 5 h 5 1 to \\ncombine the estimates obtained from various lookahead distances in the following \\nfashion \\nAn equivalent recursive definition for Qh is \\nNote if we choose h = 0 we have our original training estimate Q('), which \\nconsiders only one-step discrepancies in the Q estimates. As h is increased, the al- \\ngorithm places increasing emphasis on discrepancies based on more distant \\nlooka- \\nheads. At the extreme value A. = 1, only the observed r,+i values are considered, \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 395}, page_content=\"with no contribution from the current Q estimate. Note when Q = Q, the training \\nvalues given by Qh will be identical for all values of \\nh \\nsuch that 0 5 h 5 I. \\nThe motivation for the TD(h) method is that in some settings training will \\nbe more efficient if more distant lookaheads are considered. For example, when \\nthe agent follows an optimal policy for choosing actions, then eh with h = 1 will \\nprovide a perfect estimate for the true Q value, regardless of any inaccuracies in \\nQ. On the other hand, if action sequences are chosen suboptimally, then the r,+i \\nobserved far into the future can be misleading. \\nPeng and Williams (1994) provide a further discussion and experimental \\nresults showing the superior performance of \\nQqn \\none problem domain. \\nDayan \\n(1992) shows that under certain assumptions a similar \\nTD(h) \\napproach applied \\nto learning the V* function converges correctly for any h such that 0 5 A 5 1. \\nTesauro (1995) uses a \\nTD(h) \\napproach in his TD-GAMMON program for playing \\nbackgammon. \\n13.6 GENERALIZING FROM EXAMPLES \\nPerhaps the most constraining assumption in our treatment of Q learning up to \\nthis point is that the target function is represented as an explicit lookup table, \\nwith a distinct table entry for every distinct input value \\n(i.e., \\nstate-action pair). \\nThus, the algorithms we discussed perform a kind of rote learning and make \\nno attempt to estimate the Q value for unseen state-action pairs by generalizing \\nfrom those that have been seen. This rote learning assumption is reflected in the \\nconvergence proof, which proves convergence only if every possible state-action \\npair is visited (infinitely often!). This is clearly an unrealistic assumption in large \\nor infinite spaces, or when the cost of executing actions is high. As a result, \\nmore practical systems often combine function approximation methods discussed \\nin other chapters with the Q learning training rules described here. \\nIt is easy to incorporate function approximation algorithms such as \\nBACK- \\nPROPAGATION into the Q learning algorithm, by substituting a neural network for \\nthe lookup table and using each \\n~(s, \\na) update as a training example. For example, \\nwe could encode the state \\ns \\nand action a as network inputs and train the network \\nto output the target values of Q given by the training rules of Equations (13.7) \\nand (13.10). An alternative that has sometimes been found to be more successful \\nin practice is to train a separate network for each action, using the state as input \\nand Q as output. Another common alternative -is to train one network with the \\nstate as input, but with one Q output for each action. Recall that in Chapter 1, we \\ndiscussed approximating an evaluation function over checkerboard states using a \\nlinear function and the LMS algorithm. \\nIn practice, a number of successful reinforcement learning systems have been \\ndeveloped by incorporating such function approximation algorithms in place of the \\nlookup table. Tesauro's successful TD-GAMMON program for playing backgammon \\nused a neural network and the \\nBACKPROPAGATION \\nalgorithm together with a \\nTD(A) \\ntraining rule. Zhang and Dietterich (1996) use a similar combination of \\nBACKPROP- \\nAGATION and TD(h) for job-shop scheduling tasks. Crites and Barto (1996) describe \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 396}, page_content=\"a neural network reinforcement learning approach for an elevator scheduling task. \\nThrun \\n(1996) reports a neural network based approach to Q learning to learn basic \\ncontrol procedures for a mobile robot with sonar and camera sensors. Mahadevan \\nand Connell (1991) describe a Q learning approach based on clustering states, \\napplied to a simple mobile robot control problem. \\nDespite the success of these systems, for other tasks reinforcement learning \\nfails to converge once a generalizing function approximator is introduced. Ex- \\namples of such problematic tasks are given by \\nBoyan \\nand Moore \\n(1995), \\nBaird \\n(1995), \\nand Gordon (1995). Note the convergence theorems discussed earlier in \\nthis chapter apply only when Q is represented by an explicit table. To see the \\ndifficulty, consider using a neural network rather than an explicit table to repre- \\nsent Q. Note if the learner updates the network to better fit the training Q value \\nfor a particular transition \\n(si, \\nai), \\nthe altered network weights may also change \\nthe Q estimates for arbitrary other transitions. Because these weight changes may \\nincrease the error in Q estimates for these other transitions, the argument prov- \\ning the original theorem no longer holds. Theoretical analyses of reinforcement \\nlearning with generalizing function approximators are given by Gordon (1995) \\nand Tsitsiklis (1994). Baird (1995) proposes gradient-based methods that circum- \\nvent this difficulty by directly minimizing the sum of squared discrepancies in \\nestimates between adjacent states (also called \\nBellman \\nresidual errors). \\n13.7 RELATIONSHIP TO DYNAMIC PROGRAMMING \\nReinforcement learning methods such as Q learning are closely related to a long \\nline of research on dynamic programming approaches to solving Markov decision \\nprocesses. This earlier work has typically assumed that the agent possesses perfect \\nknowledge of the functions \\nS(s, a) and r(s, a) that define the agent's environment. \\nTherefore, it has primarily addressed the question of how to compute the optimal \\npolicy using the least computational effort, assuming the environment could be \\nperfectly simulated and no direct interaction was required. The novel aspect of \\nQ learning is that it assumes the agent does not have knowledge of S(s, a) and \\nr(s, a), and that instead of moving about in an internal mental model of the state \\nspace, it must move about the real world and observe the consequences. In this \\nlatter case our primary concern is usually the number of real-world actions that the \\nagent must perform to converge to an acceptable policy, rather than the number of \\ncomputational cycles it must expend. The reason is that in many practical domains \\nsuch as manufacturing problems, the costs in time and in dollars of performing \\nactions in the external world dominate the computational costs. Systems that learn \\nby moving about the real environment and observing the results are typically called \\nonline systems, whereas those that learn solely by simulating actions within an \\ninternal model are called ofline systems. \\nThe close correspondence between these earlier approaches and the rein- \\nforcement learning problems discussed here is apparent by considering Bellman's \\nequation, which \\nforms \\nthe foundation for many dynamic programming approaches \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 397}, page_content=\"to solving MDPs. Bellman's equation is \\nNote the very close relationship between Bellman's equation and our earlier def- \\ninition of an optimal policy in Equation (13.2). \\nBellman \\n(1957) showed that the \\noptimal policy n* satisfies the above equation and that any policy n satisfying \\nthis equation is an optimal policy. Early work on dynamic programming includes \\nthe Bellman-Ford shortest path algorithm \\n(Bellman \\n1958; Ford and Fulkerson \\n1962), \\nwhich learns paths through a graph by repeatedly updating the estimated \\ndistance to the goal for each graph node, based on the distances for its neigh- \\nbors. In this algorithm the assumption that graph edges and the goal node are \\nknown is equivalent to our assumption that 6(s, a) and r(s, a) are known. Barto \\net al. (1995) discuss the close relationship between reinforcement learning and \\ndynamic programming. \\n13.8 SUMMARY AND FURTHER READING \\nThe key points discussed in this chapter include: \\n0 Reinforcement learning addresses the problem of learning control strategies \\nfor autonomous agents. It assumes that training information is available in \\nthe form of a real-valued reward signal given for each state-action transition. \\nThe goal of the agent is to learn an action policy that maximizes the total \\nreward it will receive from any starting state. \\n0 The reinforcement learning algorithms addressed in this chapter fit a problem \\nsetting known as a Markov decision process. In Markov decision processes, \\nthe outcome of applying any action to any state depends only on this ac- \\ntion and state (and not on preceding \\nactions:or \\nstates). Markov decision \\nprocesses cover a wide range of problems including many robot control, \\nfactory automation, and scheduling problems. \\n0 Q learning is one form of reinforcement learning in which the agent learns \\nan evaluation function over states and actions. In particular, the evaluation \\nfunction Q(s, a) is defined as the maximum expected, discounted, cumulative \\nreward the agent can achieve by applying action a to state s. The Q learning \\nalgorithm has the advantage that it can-be employed even when the learner \\nhas no prior knowledge of how its actions affect its environment. \\n0 Q learning can be proven to converge to the correct Q function under cer- \\ntain assumptions, when the learner's hypothesis ~(s, a) is represented by a \\nlookup table with a distinct entry for each (s, a) pair. It can be shown to \\nconverge in both deterministic and nondeterministic MDPs. In practice, Q \\nlearning can require many thousands of training iterations to converge in \\neven modest-sized problems. \\n0 Q learning is a member of a more general class of algorithms, called tem- \\nporal difference algorithms. In general, temporal difference algorithms learn \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 398}, page_content=\"CHAFER 13 REINFORCEMENT LEARNING 387 \\nby iteratively reducing the discrepancies between the estimates produced by \\nthe agent at different times. \\nReinforcement learning is closely related to dynamic programming ap- \\nproaches to Markov decision processes. The key difference is that histori- \\ncally these dynamic programming approaches have assumed that the agent \\npossesses knowledge of the state transition function \\n6(s, \\na) and reward func- \\ntion r (s , a). In contrast, reinforcement learning algorithms such as Q learning \\ntypically assume the learner lacks such knowledge. \\nThe common theme that underlies much of the work on reinforcement learn- \\ning is to iteratively reduce the discrepancy between evaluations of successive \\nstates. Some of the earliest work on such methods is due to Samuel (1959). His \\ncheckers learning program attempted to learn an evaluation function for checkers \\nby using evaluations of later states to generate training values for earlier states. \\nAround the same time, the Bellman-Ford, single-destination, shortest-path algo- \\nrithm was developed \\n(Bellman \\n1958; Ford and Fulkerson \\n1962), \\nwhich propagated \\ndistance-to-goal values from nodes to their neighbors. Research on optimal control \\nled to the solution of Markov decision processes using similar methods \\n(Bellman \\n1961; Blackwell 1965). Holland's (1986) bucket brigade method for learning clas- \\nsifier systems used a similar method for propagating credit in the face of delayed \\nrewards. \\nBarto \\net al. (1983) discussed an approach to temporal credit assignment \\nthat led to Sutton's paper (1988) defining the \\nTD(k) \\nmethod and proving its con- \\nvergence for k = 0. Dayan (1992) extended this result to arbitrary values of k. \\nWatkins \\n(1989) introduced Q learning to acquire optimal policies when the re- \\nward and action transition functions are unknown. Convergence proofs are known \\nfor several variations on these methods. In addition to the convergence proofs \\npresented in this chapter see, for example, (Baird 1995; Bertsekas 1987; \\nTsitsiklis \\n1994, Singh and Sutton 1996). \\nReinforcement learning remains an active research area. \\nMcCallum \\n(1995) \\nand Littman \\n(1996), \\nfor example, discuss the extension of reinforcement learning \\nto settings with hidden state variables that violate the Markov assumption. Much \\ncurrent research seeks to scale up these methods to larger, more practical prob- \\nlems. For example, Maclin and Shavlik (1996) describe an approach in which a \\nreinforcement learning agent can accept imperfect advice from a trainer, based on \\nan extension to the KBANN algorithm (Chapter 12). Lin (1992) examines the role \\nof teaching by providing suggested action sequences. Methods for scaling \\nUp \\nby \\nemploying a hierarchy of actions are suggested by Singh (1993) and \\nLin \\n(1993). \\nDietterich and \\nFlann \\n(1995) explore the integration of explanation-based methods \\nwith reinforcement learning, and Mitchell and \\nThrun \\n(1993) describe the appli- \\ncation of the EBNN algorithm (Chapter 12) to Q learning. Ring (1994) explores \\ncontinual learning by the agent over multiple tasks. \\nRecent surveys of reinforcement learning are given by Kaelbling et al. \\n(1996); \\nBarto (1992); Barto et al. (1995); Dean et al. (1993). \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 399}, page_content=\"EXERCISES \\n13.1. Give a second optimal policy for the problem illustrated in Figure 13.2. \\n13.2. Consider the deterministic grid world shown below with the absorbing goal-state \\nG. Here the immediate rewards are 10 for the labeled transitions and 0 for all \\nunlabeled transitions. \\n(a) Give the V* value for every state in this grid world. Give the \\nQ(s, \\na) value for \\nevery transition. Finally, show an optimal policy. Use y = 0.8. \\n(b) Suggest a change to the reward function r(s, a) that alters the Q(s, a) values, \\nbut does not alter the optimal policy. Suggest a change to \\nr(s, \\na) that alters \\nQ(s, \\na) but does not alter \\nV*(s, \\na). \\n(c) Now consider applying the Q learning algorithm to this grid world, assuming \\nthe table of Q values is initialized to zero. Assume the agent begins in the \\nbottom left grid square and then travels clockwise around the perimeter of \\nthe grid until it reaches the absorbing goal state, completing the first training \\nepisode. Describe which Q values are modified as a result of this episode, and \\ngive their revised values. Answer the question again assuming the agent now \\nperforms a second identical episode. Answer it again for a third episode. \\n13.3. Consider playing Tic-Tac-Toe against an opponent who plays randomly. In partic- \\nular, assume the opponent chooses with uniform probability any open space, unless \\nthere is a forced move (in which case it makes the obvious correct move). \\n(a) Formulate the problem of learning an optimal Tic-Tac-Toe strategy in this case \\nas a Q-learning task. What are the states, transitions, and rewards in this non- \\ndeterministic Markov decision process? \\n(b) Will your program succeed if the opponent plays optimally rather than ran- \\ndomly? \\n13.4. Note in many MDPs it is possible to find two policies nl and n2 such that nl \\noutperforms 172 if the agent begins in'some state sl, but n2 outperforms nl if it \\nbegins in some other state \\ns2. \\nPut another way, \\nVnl \\n(sl) > VR2(s1), but Vn2(s2) > \\nVRl (s2) Explain why there will always exist a single policy that maximizes Vn(s) \\nfor every initial state s (i.e., an optimal policy n*). In other words, explain why an \\nMDP \\nalways allows a policy n* such that (Vn, s) vn*(s) 2 Vn(s). \\nREFERENCES \\nBaird, L. (1995). Residual algorithms: Reinforcement learning with function approximation. Proceed- \\nings of the \\nTwelfrh \\nInternational Conference on Machine Learning @p. 30-37). San Francisco: \\nMorgan Kaufmann. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 400}, page_content='CHaPrER 13 REINFORCEMENT LEARNING 389 \\nBarto, A. (1992). Reinforcement learning and adaptive critic methods. In D. White & S. Sofge (Eds.), \\nHandbook of intelligent control: Neural, fuzzy, and adaptive approaches (pp. 469-491). New \\nYork: Van \\nNostrand \\nReinhold. \\nBarto, \\nA., Bradtke, S., & Singh, S. (1995). Learning to act using real-time dynamic programming. \\nArtiJicial Intelligence, Special volume: Computational research on interaction and agency, \\n72(1), \\n81-138. \\nBarto, \\nA., Sutton, R., & Anderson, C. (1983). Neuronlike adaptive elements that can solve difficult \\nlearning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13(5), 834- \\n846. \\nBellman, \\nR. E. (1957). Dynamic Programming. Princeton, NJ: Princeton University Press. \\nBellrnan, \\nR. (1958). On a routing problem. Quarterly of Applied Mathematics, 16(1), 87-90. \\nBellman, \\nR. (1961). Adaptive control processes. Princeton, NJ: Princeton University Press. \\nBerenji, \\nR. (1992). Learning and tuning fuzzy controllers through reinforcements. IEEE Transactions \\non Neural Networks, 3(5), 724-740. \\nBertsekas, D. (1987). Dynamicprogramming: Deterministic and stochastic models. Englewood Cliffs, \\nNJ: Prentice Hall. \\nBlackwell, D. (1965). Discounted dynamic programming. Annals of Mathematical Statistics, 36,226- \\n235. \\nBoyan, \\nJ., & Moore, A. (1995). Generalization in reinforcement learning: Safely approximating the \\nvalue function. In G. Tesauro, D. Touretzky, & T. Leen (Eds.), Advances in Neural Information \\nProcessing Systems 7. Cambridge, MA: MIT Press. \\nCrites, R., & Barto, A. (1996). Improving elevator performance using reinforcement learning. In \\nD. S. Touretzky, M. C. Mozer, & M. C. Hasselmo (Eds.), Advances in Neural Information \\nProcessing Systems, 8. \\nDayan, P. (1992). The convergence of TD(A) for general A. Machine Learning, 8, 341-362. \\nDean, T., \\nBasye, \\nK., & Shewchuk, J. (1993). Reinforcement learning for planning and control. In S. \\nMinton \\n(Ed.), Machine Learning Methods for Planning @p. 67-92). San Francisco: Morgan \\nKaufmann. \\nDietterich, T. G., & Flann, N. S. (1995). Explanation-based learning and reinforcement learning: \\nA unified view. Proceedings of the 12th International Conference on Machine Learning @p. \\n176-184). San Francisco: Morgan Kaufmann. \\nFord, L., & Fulkerson, D. (1962). Flows in networks. Princeton, NJ: Princeton University Press. \\nGordon, G. (1995). Stable function approximation in dynamic programming. Proceedings of the \\nTwelfth International Conference on Machine Learning (pp. 261-268). San Francisco: Morgan \\nKaufmann. \\nKaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: A survey. Journal \\nof AI Research, 4, 237-285. Online journal at http://www.cs.washington.edu/research/jair/- \\nhome.htm1. \\nHolland, J. H. (1986). Escaping brittleness: The possibilities of general-purpose learning algorithms \\napplied to parallel rule-based systems. In Michalski, Carbonell, & Mitchell (Eds.), Machine \\nlearning: An \\nartijicial \\nintelligence approach (Vol. 2, pp. 593423). San Francisco: Morgan \\nKaufmann. \\nLaird, J. E., & Rosenbloom, P. S. (1990). Integrating execution, planning, and learning in SOAR for \\nexternal environments. Proceedings of the Eighth National Conference on Artificial Intelligence \\n(pp. 1022-1029). Menlo Park, CA: AAAI Press. \\nLin, L. J. (1992). Self-improving reactive agents based on reinforcement learning, \\nplanning, \\nand \\nteaching. Machine Learning, 8, 293-321. \\nLin, L. J. (1993). Hierarchical learning of robot skills by reinforcement. Proceedings of the Interna- \\ntional Conference on Neural Networks. \\nLittman, M. (1996). Algorithms for sequential decision making (Ph.D. dissertation and Technical \\nReport \\nCS-96-09). \\nBrown University, Department of Computer Science, Providence, RI. \\nMaclin, R., & Shavlik, J. W. (1996). Creating advice-taking reinforcement learners. Machine Learn- \\ning, 22, 251-281. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 401}, page_content=\"Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and empir- \\nical results. Machine Learning, 22(1), 159-195. \\nMahadevan, S., & Connell, J. (1991). Automatic programming of behavior-based robots using rein- \\nforcement learning. In Proceedings of the Ninth National Conference on ArtGcial Intelligence. \\nSan Francisco: Morgan Kaufmann. \\nMcCallum, \\nA. (1995). Reinforcement learning with selective perception and hidden state (Ph.D. dis- \\nsertation). Department of Computer Science, University of Rochester, Rochester, NY. \\nMitchell, T. M., & Thrun, S. B. (1993). Explanation-based neural network learning for robot control. \\nIn C. Giles, S. Hanson, & J. Cowan (Eds.), Advances in Neural Information Processing System \\n5 (pp. 287-294). San Francisco: Morgan-Kaufmann. \\nMoore, A., & Atkeson C. (1993). Prioritized sweeping: Reinforcement learning with less data and \\nless real time. Machine Learning, 13, 103. \\nPeng, J., & Williams, R. (1994). Incremental multi-step Q-learning. Proceedings of the Eleventh \\ninternational Conference on Machine Learning (pp. 226-232). San Francisco: Morgan Kauf- \\nmann. \\nRing, M. (1994). Continual learning in reinforcement environments (Ph.D. dissertation). Computer \\nScience Department, University of Texas at Austin, Austin, TX. \\nSamuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal \\nof Research and Development, 3, 21 1-229. \\nSingh, S. (1992). Reinforcement learning with a hierarchy of abstract models. Proceedings of the \\nTenth National Conference on \\nArt@cial \\nIntelligence (pp. 202-207). San Jose, CA: AAAI \\nPress. \\nSingh, S. (1993). Learning to solve markovian decision processes (Ph.D. dissertation). Also CMPSCI \\nTechnical Report 93-77, Department of Computer Science, University of Massachusetts at \\nAmherst. \\nSingh, S., & Sutton, R. (1996). Reinforcement learning with replacing eligibility traces. Machine \\nLearning, 22, 123. \\nSutton, R. (1988). Learning to predict by the methods of temporal differences. Machine learning, 3, \\n9-44 \\nSutton R. (1991). Planning by incremental dynamic programming. Proceedings of the Eighth Znter- \\nnational Conference on Machine Learning (pp. 353-357). San Francisco: Morgan Kaufmann. \\nTesauro, G. (1995). Temporal difference learning and TD-GAMMON. Communications of the ACM, \\n38(3), 58-68. \\nThrun, S. (1992). The role of exploration in learning control. In D. White & D. Sofge (Eds.), \\nHandbook of intelligent control: Neural, fizzy, and adaptive approaches (pp. 527-559). New \\nYork: Van \\nNostrand \\nReinhold. \\nThrun, S. (1996). Explanation-based neural network learning: A lifelong learning approach. Boston: \\nKluwer Academic Publishers. \\nTsitsiklis, J. (1994). Asynchronous stochastic approximation and Q-learning. Machine Learning, \\n16(3), 185-202. \\nWatkins, C. (1989). Learning from delayed rewards (Ph.D. dissertation). King's College, Cambridge, \\nEngland. \\nWatkins, C., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279-292. \\nZhang, \\nW., & Dietterich, T. G. (1996). High-performance job-shop scheduling with a time-delay \\nTD(A) network. In D. S. Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.), Advances in \\nneural information processing systems, 8, 1024-1030. \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 402}, page_content='APPENDIX \\nNOTATION \\nBelow is a summary of notation used in this book. \\n(a, b]: Brackets of the form [, 1, (, and ) are used to represent intervals, \\nwhere square brackets represent intervals including the boundary \\nand round parentheses represent intervals excluding the boundary. \\nFor example, (1, 31 represents the interval 1 < x 5 3. \\nCxi: The sumx~ +x2+...+xn. \\ni=l \\nn \\nHxi: The product xl .x2..-xn. \\ni=l \\nF: The symbol for logical entailment. For example, A F B denotes \\nthat B follows deductively from A. \\n>,: The symbol for the more general than relation. For example, hi >, \\nhj denotes that hypothesis hi is more general than hi. \\nargmax f (x): The value of x that maximizes f (x). For example, \\nxex \\nargmax \\nx2 = -3 \\nx~{1,2,-3) \\nf(x): A function that approximates the function f (x). \\n6: In PAC-learning, a bound on the probability of failure. In artificial \\nneural network learning, the error term associated with a single unit \\noutput. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 403}, page_content='E : \\nr]: \\nP: \\nn: \\nVE(G): \\nC: \\nD : \\nD: \\nE [x] : \\nE(G): \\nError: \\nH: \\nh (x) : \\nP(x): \\nPr(x) : \\np(x>: \\nQ<s, a): \\n3: \\nVC(H): \\nVSH,D: \\nA bound on the error of a hypothesis (in PAC-learning). \\nThe learning rate in neural network and related learning methods. \\nThe mean of a probability distribution. \\nThe standard deviation of a probability distribution. \\nThe gradient of E with respect to the vector G. \\nClass of possible target functions. \\nThe training data. \\nA probability distribution over the instance space. \\nThe expected value of x. \\nThe sum of squared errors of an artifial neural network whose \\nweights are given by the vector G. \\nThe error in a discrete-valued hypothesis or prediction. \\nHypothesis space. \\nThe prediction produced by hypothesis h for instance x. \\nThe probability (mass) of x. \\nThe probability (mass) of the event x. \\nThe probability density of x. \\nThe Q function from reinforcement learning. \\nThe set of real numbers. \\nThe Vapnik-Chervonenkis dimension of the hypothesis space H. \\nThe Version Space; that is, the set of hypotheses from H that are \\nconsistent with D. \\nIn artificial neural networks, the weight from node i to node j. \\nInstance space. '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 404}, page_content='INDEXES '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 405}, page_content='400 SUBJECT INDEX \\nSUBJECT INDEX \\nPage numbers in italics refer to tables; numbers in bold to figures. An \"n\" fol- \\nlowing a page number refers to a footnote on that page. \\nAbsorbing state, 371 \\nABSTRIPS, 329 \\nAcyclic neural networks. See Multilayer \\nfeedforward networks \\nAdaline rule. See Delta rule \\nAdditive Chernoff bounds, 210-21 1 \\nAdelines, 123 \\nAgents, in reinforcement learning, 368 \\nAgnostic learning, 210-21 1,225 \\nALVINN system, 82-83, 84 \\nAnalytical-inductive learning. See \\nInductive-analytical learning \\nAnalytical learning, 307-330 \\ninductive learning, comparison with, \\n310, 328-329, 334-336, 362 \\nANN learning. See Neural network \\nlearning \\nANNs. See Neural networks, artificial \\nAntecedents of Horn clause, 285 \\nAQ algorithm, 279-280 \\nAQ14 algorithm, comparison with GABIL, \\n256,258 \\nArbitrary functions, representation by \\nfeedforward networks, 105-106 \\nArtificial intelligence, influence on \\nmachine learning, 4 \\nArtificial neural networks. See Neural \\nnetworks, artificial \\nASSISTANT, 77 \\nAstronomical structures, machine learning \\nclassification of, 3 \\nAttributes: \\nchoice of, in sequential vs. simultaneous \\ncovering algorithms, 280-281 \\ncontinuous-valued, 72-73 \\ncost-sensitive measures, 75-76 \\ndiscrete-valued, 72 \\nmeasures for selection of, 73-74, 77 \\nmissing values, strategies for, 75 \\nAutonomous vehicles, 3, 4, 82-83, 84 \\nAverage reward, 371 \\nBackgammon learning program. See \\nTD-GAMMON \\nBACKPROPAGATION \\nalgorithm, \\n83,97, \\n124 \\napplications of, 81, 84, 85, 96, 113 \\nconvergence and local minima, 104-105 \\ndefinition of, 98 \\ndiscovery of hidden layer representations, \\n106-109, 123 \\nfeedforward networks as hypothesis \\nspace, 105-106 \\ngradient descent search, 89, 115-1 16, \\n123 \\ninductive bias of, 106 \\nKBANN algorithm: \\ncomparison with, 344-345 \\nuse in, 339 \\nmomentum, addition of, 100, 104 \\noverfitting in, 108, 110-1 11 \\nin Q learning, 384 \\nsearch of hypothesis space, 97, 106, \\n122-123 \\nin decision tree learning, comparison \\nwith, 106 \\nby genetic algorithms, comparison \\nwith, 259 \\nby KBANN and \\nTANGENTPROP \\nalgorithms, comparison with, \\n350-351 \\nstochastic gradient descent version, \\n98-100, 104-105, 107-108 \\nTANGENTPROP \\nalgorithm, comparison \\nwith, 349 \\nweight update rule: \\nalternative error functions, 117-1 18 \\nderivation of, 101-102 \\nfor hidden unit weights, 103 '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 406}, page_content=\"in KBANN algorithm, 343-344 \\noptimization methods, 119 , \\nfor output unit weights, 102-103, 171 \\nBacktracking, ID3 algorithm and, 62 \\nBackward chaining search for explanation \\ngeneration, 3 14 \\nBaldwin effect, 250, 267 \\ncomputational models for, 267-268 \\nBayes classifier, naive. See Naive Bayes \\nclassifier \\nBayes optimal classifier, 174-176, 197, \\n222 \\nlearning Boolean concepts using version \\nspaces, 176 \\nBayes optimal learner. See Bayes optimal \\nclassifier \\nBayes rule. See Bayes theorem \\nBayes theorem, 4, 156-159 \\nin BRUTE-FORCE MAP LEARNING \\nalgorithm, 160-162 \\nconcept learning and, 158-163 \\nin inductive-analytical learning, 338 \\nBayesian belief networks, 184-191 \\nchoice among alternative networks, 190 \\nconditional independence in, 185 \\nconstraint-based approaches in, 191 \\ngradient ascent search in, 188-190 \\ninference methods, 187-188 \\njoint probability distribution \\nrepresentation, 185-1 87 \\nlearning from training data, 188-191 \\nnaive Bayes classifier, comparison with, \\n186 \\nrepresentation of causal knowledge, 187 \\nBayesian classifiers, 198. See also Bayes \\noptimal classifier; Naive Bayes \\nclassifier \\nBayesian learning, 154-198 \\ndecision tree learning, comparison with, \\n198 \\nBayesian methods, influence on machine \\nlearning, 4 \\nBeam search: \\ngeneral-to-specific. See General-to- \\nspecific beam search \\ngenerate-and-test. See Generate-and-test \\nbeam search \\nBellman-Ford shortest path algorithm, 386, \\n1117 \\nBellman residual errors, 385 \\nBellman's equation, 385-386 \\nBFS-ID3 algorithm, 63 \\nBinomial distribution, 133-137, 143, 151 \\nBiological evolution, 249, 250, 266-267 \\nBiological neural networks, comparison \\nwith artificial neural networks, 82 \\nBit strings, 252-253, 258-259, 269 \\nBlocks, stacking of. See Stacking problems \\nBody of Horn clause, 285 \\nBoolean conjunctions, PAC learning of, \\n211-212 \\nBoolean functions: \\nrepresentation by feedforward networks, \\n105-106 \\nrepresentation by perceptrons, 87-88 \\nBoundary set representation for version \\nspaces, 3 1-36 \\ndefinition of, 3 1 \\nBounds: \\none-sided, 141, 144 \\ntwo-sided, 141 \\nBrain, neural activity in, 82 \\nBreadth first search in ID3 algorithm, 63 \\nBRUTE-FORCE MAP LEARNING algorithm, \\n159-162 \\nBayes theorem in, 160-162 \\nC4.5 algorithm, 55, 77 \\nGABIL, comparison with, 256,258 \\nmissing attribute values, method for \\nhandling, 75 \\nrule post-pruning in, 71-72 \\nCADET system, 241-244 \\nCANDIDATE-ELIMINATION algorithm, \\n29-37,4547 \\napplications of, 29, 302 \\nBayesian interpretation of, 163 \\ncomputation of version spaces, 32-36 \\ndefinition of, 33 \\nID3 algorithm, comparison with, 61-64 \\ninductive bias of, 43-46, 63-64 \\nlimitations of, 29, 37, 41, 42, 46 \\nsearch of hypothesis space, 64 \\nCandidate specializations: \\ngenerated by FOCL algorithm, 357-361 \\ngenerated by FOIL algorithm, 287-288, \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 407}, page_content=\"CART system, 77 \\nCASCADE-CORRELATION algorithm, \\n121-123 \\nCase-based reasoning, 23 1, 240-244, 246, \\n247 \\nadvantages of, 243-244 \\napplications of, 240 \\nother instance-based learning methods, \\ncomparison with, 240 \\nCausal knowledge, representation by \\nBayesian belief networks, 187 \\nCentral Limit Theorem, 133, 142-143, 167 \\nCheckers learning program, \\n2-3,5-14, \\n387 \\nalgorithms for, 14 \\ndesign, 13 \\nas sequential control process, 369 \\nChemical mass spectroscopy, \\nCANDIDATE-ELIMINATION algorithm \\nin, 29 \\nChess learning program, 308-310 \\nexplanation-based learning in, 325 \\nChunking, \\n327, 330 \\nCIGOL, 302 \\nCircuit design, genetic programming in, \\n265-266 \\n' Circuit layout, genetic algorithms in, \\n256 \\nClassification problems, 54 \\nCLA~~IFYJAIVEBAYES-TEXT, 182-183 \\nCLAUDIEN, 302 \\nClauses, 284, 285 \\nCLS. See Concept Learning System \\nClustering, 191 \\nCN2 algorithm, 278, 301 \\nchoice of attribute-pairs in, \\n280-281 \\nComplexity, sample. See Sample \\ncomplexity \\nComputational complexity, 202 \\nComputational complexity theory, \\ninfluence on machine learning, 4 \\nComputational learning theory, \\n201-227 \\nConcept learning, 20-47 \\nalgorithms for, 47 \\nBayes theorem and, 158-163 \\ndefinition of, 21 \\ngenetic algorithms in, 256 \\nID3 algorithm specialized for, 56 \\nnotation for, 22-23 \\nsearch of hypothesis space, 23-25, \\n4-7 \\ntask design in, 21-22 \\nConcept Learning System, 77 \\nConcepts, partially learned, 38-39 \\nConditional independence, 185 \\nin Bayesian belief networks, \\n186-187 \\nConfidence intervals, 133, 138-141, 150, \\n151 \\nfor discrete-valued hypotheses, 13 \\n1-132, \\n140-141 \\nderivation of, 142-143 \\none-sided, 144, 145 \\nConjugate gradient method, 119 \\nConjunction of boolean literals, PAC \\nlearning of, 21 1-212 \\nConsequent of Horn clause, 285 \\nConsistent learners, 162-163 \\nbound on sample complexity, 207-210, \\n225 \\nequation for, 209 \\nConstants, in logic, 284, 285 \\nConstraint-based approaches in Bayesian \\nbelief networks, 191 \\nConstructive induction, 292 \\nContinuous functions, representation \\nby feedforward networks, \\n105-106 \\nContinuous-valued hypotheses, training \\nerror of, 89-90 \\nContinuous-valued target function, 197 \\nmaximum likelihood (ML) hypothesis \\nfor, 164-167 \\nControl theory, influence on machine \\nlearning, 4 \\nConvergence of Q learning algorithm: \\nin deterministic environments, 377-380, \\n386 \\nin nondeterministic environments, \\n382-383, 386 \\nCredit assignment, 5 \\nCritic, 12, 13 \\nCross entropy, 170 \\nminimization of, 1 18 \\nCross-validation, 11 1-1 12 \\nfor comparison of learning algorithms, \\n145-151 \\nk-fold. See k-fold cross-validation \\nin k-NEAREST NEIGHBOR algorithm, 235 \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 408}, page_content='leave-one-out, 235 \\nin neural network learning, 11 1-1 12 \\nCrossover mask, 254 \\nCrossover operators, 252-254, 261, \\n262 \\nsingle-point, 254, 261 \\ntwo-point, 254, 257-258 \\nuniform, 255 \\nCrowding, 259, \\nCumulative reward, 371 \\nCurse of dimensionality, 235 \\nData mining, 17 \\nDecision tree learning, 52-77 \\nalgorithms for, 55, 77. See also C4.5 \\nalgorithm, ID3 algorithm \\napplications of, 54 \\nBayesian learning, comparison with, 198 \\nimpact of pruning on accuracy, 128-129 \\ninductive bias in, 63-66 \\nk-NEAREST NEIGHBOR algorithm, \\ncomparison with, 235 \\nMinimum Description Length principle \\nin, 173-174 \\nneural network learning, comparison \\nwith, 85 \\noverfitting \\nin, \\n6749, \\n76-77, 11 1 \\npost-pruning in, 68-69, 77 \\nreduced-error pruning in, 69-7 1 \\nrule post-pruning in, 71-72, 281 \\nsearch of hypothesis space, \\n60-62 \\nby \\nBACKPROPAGATION \\nalgorithm, \\ncomparison with, 106 \\nDeductive learning, 321-322 \\nDegrees of freedom, 147 \\nDelayed learning methods, comparison \\nwith eager learning, \\n244-245 \\nDelayed reward, in reinforcement learning, \\n369 \\nDelta rule, 11, 88-90, 94, 99, 123 \\nDemes, 268 \\nDeterminations, 325 \\nDeterministic environments, Q learning \\nalgorithm for, 375 \\nDirected acyclic neural networks. See \\nMultilayer feedforward networks \\nDiscounted cumulative reward. 371 \\nDiscrete-valued hypotheses: \\nconfidence intervals for, 131-132, \\n140-141 \\nderivation of, 142-143 \\ntraining error of, 205 \\nDiscrete-valued target functions, \\napproximation by decision tree \\nlearning, 52 \\nDisjunctive sets of rules, learning by \\nsequential covering algorithms, \\n275-276 \\nDistance-weighted k-NEAREST NEIGHBOR \\nalgorithm, 233-234 \\nDomain-independent learning algorithms, \\n336 \\nDomain theory, 310, 329. See also \\nimperfect domain theory; Perfect \\ndomain theory; Prior knowledge \\nin analytical learning, \\n31 \\n1-312 \\nas KBANN neural network, 342-343 \\nin PROLOG-EBG, 322 \\nweighting of components in EBNN, \\n35 1-352 \\nDYNA, 380 \\nDynamic programming: \\napplications to reinforcement learning, \\n380 \\nreinforcement learning and, 385-387 \\nEager learning methods, comparison with \\nlazy learning, \\n244245 \\nEBG algorithm, 3 \\n13 \\nEBNN algorithm, 351-356, 362, 387 \\nother explanation-based learning \\nmethods, comparison with, 356 \\nprior knowledge and gradient descent in, \\n339 \\nTANGENTPROP \\nalgorithm in, 353 \\nweighting of inductive-analytical \\ncomponents in, 355,362 \\nEGGS algorithm, 3 \\n13 \\nEM algorithm, 190-196, 197 \\napplications of, \\n191, \\n194 \\nderivation of algorithm for k-means, \\n195-196 \\nsearch for maximum likelihood \\n(ML) \\nhypothesis, \\n194-195 '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 409}, page_content=\"Entailment, 321n \\nrelationship with 8-subsumption and \\nmore-general-than partial ordering, \\n299-300 \\nEntropy, 55-57, 282 \\nof optimal code, 172n \\nEnvironment, in reinforcement learning, \\n368 \\nEquivalent sample size, 179-1 80 \\nError bars for discrete-valued hypotheses. \\nSee Confidence intervals, for \\ndiscrete-valued hypotheses \\nError of hypotheses: \\nsample. See Sample error \\ntraining. See Training error \\ntrue. See True error \\nEstimation bias, 133, 137-138, 151 \\nEstimator, 133, 137-138, 143, 150-151 \\nEvolution of populations: \\nargument for Occam's razor, 66 \\nin genetic algorithms, 260-262 \\nEvolutionary computation, 250, 262 \\napplications of, 269 \\nExample-driven search, comparison with \\ngenerate-and-test beam search, 281 \\nExpected value, 133, 136 \\nExperiment generator, 12-13 \\nExplanation-based learning, 3 12-330 \\napplications of, 325-328 \\nderivation of new features, 320-321 \\ninductive bias in, 322-323 \\ninductive learning and, 330 \\nlazy methods in, 328 \\nlimitations of, 308, 329 \\nprior knowledge in, 308-309 \\nreinforcement learning and, 330 \\nutility analysis in, 327-328 \\nExplanations generated by backward \\nchaining search, 314 \\nExplicit prior knowledge, 329 \\nExploration in reinforcement learning, 369 \\nFace recognition, 17 \\nBACKPROPAGATION algorithm in, 8 1, \\n112-1 \\n17 \\nFeedforward networks. See Multilayer \\nFIND-S algorithm, 26-28, 46 \\nBayesian interpretation of, 162-163 \\ndefinition of, 26 \\ninductive bias of, 45 \\nlimitations of, 28-29 \\nmistake-bound learning in, 220-221 \\nPAC learning of boolean conjunctions \\nwith, 212 \\nsearch of hypothesis space, 27-28 \\nFinite horizon reward, 37 1 \\nFirst-order Horn clauses, 283-284, \\n3 18-3 19. See also First-order rules \\nin analytical learning, 3 1 1 \\nin PROLOG-EBG, 313, 314 \\nFirst-order logic, basic definitions, 285 \\nFirst-order representations, applications of, \\n275 \\nFirst-order resolution rule, 296-297 \\nFirst-order rules, 274-275, 283, 301, 302. \\nSee also First-order Horn clauses \\nin FOIL algorithm, 285-291 \\npropositional rules, comparison with, \\n283 \\nFitness function, 250-252, 255-256, 258 \\nFitness proportionate selection, 255 \\nFitness sharing, 259 \\nFOCL algorithm, 302 \\nextensions to FOIL, 357 \\nsearch step alteration with prior \\nknowledge, 339-340 \\nFOIL algorithm, 286,290-291, 302 \\nextensions in FOCL, 357 \\ninformation gain measure in, 289 \\nLEARN-ONE-RULE and sequential \\ncovering algorithms, comparison \\nwith, 287 \\nlearning first-order rules in, 285-291 \\npost-pruning in, 291 \\nrecursive rule learning in, 290 \\nFunction approximation, 8 \\nFunction approximation algorithms: \\nchoice of, 9-1 1 \\nas lookup table substitute, 384 \\nFunctions, in logic, 284, 285 \\nGABIL, 256-259, 269 \\nC4.5 and AQ14 algorithms, comm.rison \\nfeedforward \\nnetworks with, 25-6, 258 \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 410}, page_content='extensions to, 258-259 \\nID5R \\nalgorithm, comparison with, 258 \\nGain ratio, 73-74 \\nGAS. See Genetic algorithms \\nGaussian distribution. See Normal \\ndistribution \\nGaussian kernel function, 238-240 \\nGeneral-to-specific beam search, 277-279, \\n302 \\nadvantages of, 281 \\nin CN2 algorithm, 278 \\nin FOCL algorithm, 357-361 \\nin FOIL algorithm, 287,357-358 \\nGeneral-to-specific ordering of \\nhypotheses, 24-25, \\n4546. See also \\nMore-general-than partial ordering \\nGeneralization accuracy in neural \\nnetworks, 1 10-1 1 1 \\nGeneralizer, 12, 13 \\nGenerate-and-test beam search, 250 \\nexample-driven search, comparison with, \\n28 \\n1 \\ninverse entailment operators, comparison \\nwith, 299 \\ninverse resolution, comparison with, \\n298-299 \\nGenetic algorithms, 249-270 \\nadvantages of, 250 \\napplications of, 256, 269 \\nfitness function in, 255-256 \\nlimitations of, 259 \\nparallelization \\nof, 268 \\nrepresentation of hypotheses, 252-253 \\nsearch of hypothesis space, 259, \\n268-269 \\nGenetic operators, 252-255, 257, 261-262 \\nGenetic programming, 250, 262-266, 269 \\napplications of, 265, 269 \\nperformance of, 266 \\nrepresentation in, 262-263 \\nGibbs algorithm, 176 \\nGlobal method, 234 \\nGOLEM, 28 1 \\nGP. See Genetic programming \\nGradient ascent search, 170-171 \\nin Bayesian belief networks, 188-190 \\nGradient descent search, 89-91, 93, 97, \\n115-116, 123 \\nin EBNN algorithm, 339 \\nleast-squared error hypothesis in, 167 \\nlimitations of, 92 \\nweight update rule, 91-92, 237 \\nstochastic approximation to, 92-94, \\n98-100, 104-105, 107-108 \\nGradient of error, 91 \\nGreedy search: \\nin sequential covering algorithms, \\n276-278 \\nin PROLOG-EBG, 323 \\nGRENDEL program, 303 \\nGround literal, 285 \\nHALVING algorithm, 223 \\nmistake-bound learning in, 221-222 \\nHandwriting recognition, \\n34 \\nBACKPROPAGATION \\nalgorithm in, 8 1 \\nTANGENTPROP \\nalgorithm in, 348-349 \\nHead of Horn clause, 285 \\nHidden layer representations, discovery \\nby \\nBACKPROPAGATION \\nalgorithm, \\n106-109, 123 \\nHidden units: \\nBACKPROPAGATION \\nweight tuning rule \\nfor, 103 \\nCASCADE-CORRELATION algorithm, \\naddition by, 121-123 \\nchoice in radial basis function networks, \\n239-240 \\nin face recognition task, 1 15-1 17 \\nHill-climbing search: \\nin FOIL algorithm, 286,287 \\nin genetic algorithms, 268 \\nin ID3 algorithm, \\n60-61 \\nHoeffding bounds, 210-21 1 \\nHorn clauses, 284, 285 \\nHorn clauses, first-order. See First-order \\nHorn clauses \\nHuman learning: \\nexplanations in, 309 \\nprior knowledge in, 330 \\nHypotheses. See also Discrete-valued \\nhypotheses; General-to-specific \\nordering of hypotheses; Hypothesis \\nspace \\nerror differences between two, 143-144 \\nestimation of accuracy, 129-130 '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 411}, page_content=\"Hypotheses, estimation of accuracy \\n(continued) \\nbias and variance in estimate, 129, \\n151, 152 \\nerrors in, 129-131, 151 \\nevaluation of, 128-129 \\njustification of, in inductive vs. analytical \\nlearning, 334-336 \\nrepresentations of, 23 \\ntesting of, \\n144-145 \\nHypothesis space, 14-15 \\nbias in, \\n40-42, \\n46, 129 \\nfinite, sample complexity for, 207-214, \\n225 \\ninfinite, sample complexity for, 214-220 \\nVC dimension of, \\n214-217 \\nHypothesis space search \\nby \\nBACKPROPAGATION \\nalgorithm, 97, \\n106, 122-123 \\ncomparison with decision tree \\nlearning, 106 \\ncomparison with KBANN and \\nTANGENTPROP \\nalgorithms, 350-35 1 \\nby CANDIDATE-ELIMINATION algorithm, \\n64 \\nin concept learning, 23-25, 46-47 \\nconstraints on, 302-303 \\nby FIND-S algorithm, 27-28 \\nby FOIL algorithm, \\n286-287, \\n357-361 \\nby genetic algorithms, 250, 259 \\nby gradient descent, 90-91 \\nby ID3 algorithm, \\n60-62,64, \\n76 \\nby KBANN algorithm, 346 \\nby learning algorithms, 24 \\nby LEARN-ONE-RULE, 277 \\nin machine learning, 14-15, 18 \\nuse of prior knowledge, 339-340, 362 \\nID3 algorithm, \\n55-64,77 \\nbacktracking and, 62 \\nCANDIDATE-ELIMINATION algorithm, \\ncomparison with, 61-62 \\nchoice of attributes in, 280-281 \\nchoice of decision tree, 63 \\ncost-sensitive measures, 75-76 \\nextensions to, 77. See also C4.5 \\nalgorithm \\ninductive bias of, 63-64, 76 \\nLEARN-ONE-RULE, search comparison \\nwith, 277 \\nlimitations of, 61-62 \\noverfitting \\nin, 67-68 \\nsearch of hypothesis space, 60-62, 64, \\n76 \\nsequential covering algorithms, \\ncomparison with, 280-281 \\nspecialized for concept learning, 56 \\nuse of information gain in, 58-60 \\nID5R algorithm, comparison with GABIL, \\n258 \\nILP. See Inductive logic programming \\nImage encoding in face recognition, 114 \\nImperfect domain theory: \\nin EBNN algorithm, 356 \\nin explanation-based learning, 330 \\nin FOCL algorithm, 360 \\nin KBANN algorithm, 344-345 \\nIncremental explanation methods, 328 \\nIncremental gradient descent. See \\nStochastic gradient descent \\nINCREMENTAL VERSION SPACE MERGING \\nalgorithm, 47 \\nInductive-analytical learning, \\n334-363 \\nadvantages of, 362 \\nexplanation-based learning and, 330 \\nlearning problem, 337-338 \\nprior knowledge methods to alter search, \\n339-340,362 \\nproperties of ideal systems, 337 \\nweighting of components in EBNN \\nalgorithm, 351-352,355 \\nweighting prior knowledge in, 338 \\nInductive bias, 39-45, 137-138. See also \\nOccam's razor; Preference bias; \\nRestriction bias \\nof \\nBACKPROPAGATION \\nalgorithm, 106 \\nbias-free learning, \\n40-42 \\nof CANDIDATE-ELIMINATION algorithm, \\n43-46, 63-64 \\nin decision tree learning, 63-66 \\ndefinition of, 43 \\nin explanation-based learning, 322-323 \\nof FIND-S algorithm, 45 \\nof ID3 algorithm, \\n63-64,76 \\nof inductive learning algorithms, 42-46 \\nof k-NEAREST NEIGHBOR algorithm, 234 \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 412}, page_content='of LMS algorithm, 64 \\nof ROTE-LEARNER algorithm, 44-45 \\nInductive inference. See Inductive learning \\nInductive learning, 42, 307-308. See \\nalso Decision tree learning; \\nGenetic algorithms; Inductive logic \\nprogramming; Neural network \\nlearning \\nanalytical learning, comparison with, \\n310, 328-329, 334-336, 362 \\ninductive bias in, 4246 \\nInductive learning hypothesis, 23 \\nInductive logic programming, 275,29 1 \\nPROLOG-EBG, comparison with, 322 \\nInformation gain, 73 \\ndefinition of, 57-58 \\nin FOIL algorithm, 289 \\nin ID3 algorithm, 55,5840 \\nInformation theory: \\ninfluence on machine learning, 4 \\nMinimum Description Length principle \\nand, 172 \\nInitialize-thehypothesis approach, \\n339-346 \\nBayesian belief networks in, 346 \\nInstance-based learning, 230-247. See also \\nCase-based reasoning; k-NEAREST \\nNEIGHBOR \\nalgorithm; \\nLocally \\nweighted regression \\nadvantages, 245-246 \\ncase-based reasoning, comparison with \\nother methods, 240 \\nlimitations of, 23 1 \\nInverse entailment, 292, 302 \\nfirst-order, 297 \\ngenerate-and-test beam search, \\ncomparison with, 299 \\nin PROGOL, 300-302 \\nInverse resolution, 294-296, 302 \\nfirst-order, 297-298 \\ngenerate-and-test beam search, \\ncomparison with, 298-299 \\nlimitations of, 300 \\nInverted deduction, 291-293 \\nJ \\nJacobian, 354 \\nJob-shop scheduling, genetic algorithms in, \\nJoint probability distribution, in Bayesian \\nbelief networks, 185-187 \\nk-fold cross-validation, 112, 147, 150 \\nk-means problem, 19 1-193 \\nderivation of EM algorithm for, 195-196 \\nk-NEAREST NEIGHBOR algorithm, 23 1-233, \\n246 \\napplications of, 234 \\ncross-validation in, 235 \\ndecision tree and rule learning, \\ncomparison with, 235 \\ndistance-weighted, 233-234 \\ninductive bias of, 234 \\nmemory indexing in, 236 \\nk-term CNF expressions, 2 13-214 \\nk-term DNF expressions, 213-214 \\nK2 algorithm, 190-191 \\nKBANN algorithm, 340-347, 362, 387 \\nadvantages of, 344 \\nBACKPROPAGATION algorithm, \\ncomparison with, 344-345 \\nBACKPROPAGATION weight update rule \\nin, 343-344 \\nhypothesis space search by \\nBACKPROPAGATION \\nand \\nTANGENTPROP, \\ncomparison with, \\n350-35 1 \\nlimitations of, 345 \\nprior knowledge in, 339 \\nkd-tree, 236 \\nKernel function, 236, 238, 246 \\nKernel function, Gaussian. See Gaussian \\nkernel function \\nKnowledge-Based \\nArtificial \\nNeural \\nNetwork (KBANN) algorithm. See \\nKBANN algorithm \\nKnowledge compilation, 320 \\nKnowledge level learning, 323-325 \\nKnowledge reformulation, 320 \\nLamarckian evolution, 266 \\nLanguage bias. See Restriction bias \\nLazy explanation methods, 328 \\nLazy learning methods, comparison with \\neager learning, 244-245 '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 413}, page_content=\"LEARN-ONE-RULE algorithm: \\nFOIL algorithm, comparison with, 287 \\nID3 algorithm, search comparison with, \\n277 \\nrule performance in, 282 \\nrule post-pruning in, 28 1 \\nvariations of, 279-280,286 \\nLearning: \\nhuman. See Human learning \\nmachine. See Machine learning \\nLearning algorithms \\nconsistent learners, 162-163 \\ndesign of, 9-11, 17 \\ndomain-independent, 336 \\nerror differences between two, 145-15 1 \\nsearch of hypothesis space, 24 \\nLearning problems, 2-5, 17 \\ncomputational theory of, 201-202 \\nin inductive-analytical learning, 337-338 \\nLearning rate, 88, 91 \\nLearning systems: \\ndesign of, 5-14, 17 \\nprogram modules, 11-1' : \\nLeast mean squares algori ,m. See LMS \\nalgorithm \\nLeast-squared error hypothesis: \\nclassifiers for, 198 \\ngradient descent in, 167 \\nmaximum likelihood (ML) hypothesis \\nand, 164-167 \\nLeave-one-out cross-validation, 235 \\nLegal case reasoning, case-based reasoning \\nin, 240 \\nLEMMA-ENUMERATOR algorithm, 324 \\nLifelong learning, 370 \\nLine search, 119 \\nLinear programming, as weight update \\nalgorithm, 95 \\nLinearly separable sets, 86, 89, 95 \\nLIST-THEN-ELLMINATE algorithm, 30 \\nLiteral, 284, 285 \\nLMS algorithm, 11, 15 \\ninductive bias of, 64 \\nLMS weight update rule. See Delta rule \\nLocal method, 234 \\nLocally weighted regression, 23 1, \\n236-238, 246 \\nlimitations of, 238 \\nLogical constants, 284, 285 \\nLogical terms, 284, 285 \\nLogistic function, 96, 104 \\nLookup table: \\nfunction approximation algorithms as \\nsubstitute, 384 \\nneural network as substitute, 384 \\nLower bound on sample complexity, \\n217-218 \\nm-estimate of probability, 179-180, 198, \\n282 \\nMachine learning, 15. See also entries \\nbeginning with Learning \\napplications, 3, 17 \\ndefinition of, 2 \\ninfluence of other disciplines on, 4, 17 \\nsearch of hypothesis space, 14-15, 18 \\nManufacturing process control, 17 \\nMAP hypothesis. See Maximum \\na posteriori hypothesis \\nMAP LEARNING algorithm, BRUTE-FORCE. \\nSee BRUTE-FORCE MAP LEARNING \\nalgorithm \\nMarkov decision processes (MDP), 370, \\n387 \\napplications of, 386 \\nMARKUS, 302 \\nMARVIN, 302 \\nMaximally general hypotheses, \\ncomputation by \\nCANDIDATE- \\nELIMINATION algorithm, 3 1, \\n46 \\nMaximally specific hypotheses: \\ncomputation by \\nCANDIDATE- \\nELIMINATION algorithm, 3 1, \\n46 \\ncomputation by FIND-S algorithm, \\n26-28, 62-63 \\nMaximum a posteriori (MAP) hypothesis, \\n157, 197. See also BRUTE-FORCE \\nMAP LEARNING algorithm \\nnaive Bayes classifier and, 178 \\noutput of consistent learners, 162-163 \\nMaximum likelihood (ML) hypothesis, 157 \\nEM algorithm search for, 194-195 \\nleast-squared error hypothesis and, - - \\nweight update rules in, 237-238 164-167 \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 414}, page_content='prediction of probabilities with, \\n167-170 \\nMDP. See Markov decision processes \\nMean error, 143 \\nMean value, 133, 136 \\nMeans-ends planner, 326 \\nMechanical design, case-based reasoning \\nin, \\n240-244 \\nMedical diagnosis: \\nattribute selection measure, 76 \\nBayes theorem in, 157-158 \\nMETA-DENDRAL, \\n302 \\nMFOIL, \\n302 \\nMinimum Description Length principle, \\n66,69, \\n171-173, 197, 198 \\nin decision tree learning, 173-174 \\nin inductive logic programming, \\n292-293 \\nMIS, 302 \\nMistake-bound learning, 202, 220, 226 \\nin CANDIDATE-ELIMINATION algorithm, \\n221-222 \\nin FIND-S algorithm, 220-221 \\nin HALVING algorithm, 221-222 \\nin LIST-THEN-ELIMINATE algorithm, \\n221-222 \\nin WEIGHTED-MAJORITY algorithm, \\n224-225 \\nMistake bounds, optimal. See Optimal \\nmistake bounds \\nML hypothesis. See Maximum likelihood \\nhypothesis \\nMomentum, addition to \\nBACKPROPAGATION \\nalgorithm, 100, 104 \\nMore-general-than partial ordering, 24-28, \\n46 \\nin CANDIDATE-ELIMINATION algorithm, \\n29 \\nin FIND-S algorithm, 26-28 \\nO-subsumption, \\nentailment, and, 299-300 \\nin version spaces, 31 \\nMultilayer feedforward networks \\nBACKPROPAGATION \\nalgorithm in, 95-101 \\nfunction representation in, 105-106, 115 \\nrepresentation of decision surfaces, 96 \\ntraining of multiple networks, 105 \\nVC dimension of, 218-220 \\nNaive Bayes classifier, \\n154-155, \\n177-179, \\n197 \\nBayesian belief network, comparison \\nwith, 186 \\nmaximum a posteriori (MAP) hypothesis \\nand, 178 \\nuse in text classification, 180-184 \\nNaive Bayes learner. See Naive Bayes \\nclassifier \\nNegation-as-failure strategy, 279, \\n319, \\n321n \\nNegative literal, 284, 285 \\nNeural network learning, 81-124. \\nSee also BACKPROPAGATION \\nalgorithm; CASCADE-CORRELATION \\nalgorithm, EBNN \\nalgorithm, \\nKBANN \\nalgorithm, \\nTANGENTPROP \\nalgorithm \\napplications of, 83, 85 \\nin face recognition, 113 \\ncross-validation in, 11 1-1 12 \\ndecision tree learning, comparison with, \\n85 \\ndiscovery of hidden layer representations \\nin, 107 \\noverfitting in, 123 \\nin Q learning, 384 \\nrepresentation in, 82-83, 84, 105-106 \\nNeural networks, artificial, 81-124. \\nSee also Multilayer feedforward \\nnetworks; Radial basis function \\nnetworks; Recurrent networks \\nbiological neural networks, comparison \\nwith, 82 \\ncreation by \\nKBANN \\nalgorithm, 342-343 \\nVC dimension of, 218-220 \\nNeural networks, biological, 82 \\nNeurobiology, \\ninfluence on machine \\nlearning, 4, 82 \\nNew features: \\nderivation in \\nBACKPROPAGATION \\nalgorithm, 106-109, 123 \\nderivation in explanation-based learning, \\n320-321 \\nNEWSWEEDER \\nsystem, 183-184 \\nNondeterministic environments, Q learning \\nMutation operator, 252, 253, 255, 257, 262 in, 381-383 '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 415}, page_content=\"410 SUBJECT INDEX \\nNormal distribution, 133, 139-140, 143, \\n151, 165 \\nfor noise, 167 \\nin paired tests, 149 \\nOccam's razor, 4, 65-66, 171 \\nOffline learning systems, 385 \\nOne-sided bounds, 141, 144 \\nOnline learning systems, 385 \\nOptimal brain damage approach, 122 \\nOptimal code, 172 \\nOptimal mistake bounds, 222-223 \\nOptimal policy for selecting actions, \\n371-372 \\nOptimization problems: \\nexplanation-based learning in, 325 \\ngenetic algorithms in, 256, 269 \\nreinforcement learning in, 256 \\nOutput encoding in face recognition, \\n114-1 \\n15 \\nOutput units, \\nBACKPROPAGATION \\nweight \\nupdate rule for, 102-103 \\nOverfitting, 123 \\nin \\nBACKPROPAGATION \\nalgorithm, 108, \\n11&111 \\nin decision tree learning, \\n66-69, \\n76-77, \\n111 \\ndefinition of, 67 \\nMinimum Description Length principle \\nand, 174 \\nin neural network learning, 123 \\nPAC learning, 203-207, 225, 226 \\nof boolean conjunctions, 21 1-212 \\ndefinition of, \\n206-207 \\ntraining error in, 205 \\ntrue error in, \\n204-205 \\nPaired tests, 147-150, 152 \\nParallelization in genetic algorithms, 268 \\nPartially learned concepts, 38-39 \\nPartially observable states in reinforcement \\nlearning, 369-370 \\nPerceptron training rule, 88-89, \\n94,95 \\nPerceptrons, 86, 95, 96, 123 \\nrepresentation of boolean functions, \\nVC dimension of, 219 \\nweight update rule, 88-89, 94, 95 \\nPerfect domain theory, 3 12-3 13 \\nPerformance measure, 6 \\nPerformance system, 11-12, \\n13 \\nPhilosophy, influence on machine \\nlearning, 4 \\nPlanning problems: \\nPRODIGY in, 327 \\ncase-based reasoning in, \\n240-241 \\nPolicy for selecting actions, 370-372 \\nPopulation evolution, in genetic algorithms, \\n260-262 \\nPositive literal, 284, 285 \\nPost-pruning: \\nin decision tree learning, 68-69, 77, \\n28 \\n1 \\nin FOIL algorithm, 291 \\nin LEARN-ONE-RULE, \\n28 \\n1 \\nPosterior probability, 155-156, 162 \\nPower law of practice, 4 \\nPower set, 40-42 \\nPredicates, 284, 285 \\nPreference bias, 64, 76, 77 \\nPrior knowledge, 155-156, 336. See also \\nDomain theory \\nto augment search operators, 357-361 \\nin Bayesian learning, 155 \\nderivatives of target function, \\n346-356, \\n362 \\nin explanation-based learning, \\n308-309 \\nexplicit, use in learning, 329 \\nin human learning, 330 \\ninitialize-the-hypothesis approach, \\n339-346, 362 \\nin PROLOG-EBG, 313 \\nsearch alteration in inductive-analytical \\nlearning, 339-340, 362 \\nweighting in inductive-analytical \\nlearning, 338, 362 \\nPrioritized sweeping, 380 \\nProbabilistic reasoning, 163 \\nProbabilities: \\nestimation of, 179-1 80 \\nformulas, 159 \\nmaximum likelihood (ML) hypothesis \\nfor prediction of, 167-170 \\n87-88 \\nprobability \\ndensity, 165 \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 416}, page_content=\"Probability distribution, 133. See also \\nBinomial distribution: Normal \\ndistribution \\napproximately \\ncorrect (PAC) \\nlearning. See PAC learning \\nhxess \\ncontrol \\nin manufacturing, 17 \\nPRODIGY, 326-327, 330 \\nProduct rule, 159 \\n~WL, 300-302 \\n~oL@% 275,302, 330 \\nPROLOG-EBG, 313-321, 328-329 \\napplications of, 325 \\ndeductive learning in, 321-322 \\ndefinition of, 314 \\nderivation of new features in, 320-321 \\ndomain theory in, 322 \\nEBNN algorithm, comparison with, 356 \\nexplanation of training examples, \\n314-318 \\nweakest preimage in, 329 \\ninductive bias in, 322-323 \\ninductive logic programming, \\ncomp'arison \\nwith, 322 \\nlimitations of, 329 \\nperfect domain theory in, 313 \\nprior knowledge in, 313 \\nproperties of, 3 19 \\nregression process in, 3 16-3 18 \\nPropositional rules: \\nlearning by sequential covering \\nalgorithms, 275 \\nlearning first-order rules, comparison \\nwith, 283 \\npsychology, influence on machine \\nlearning, 4 \\nQ function: \\nin deterministic environments, 374 \\nconvergence of Q learning towards, \\n377-380 \\nin nondeterministic environments, 381 \\nconvergence of Q learning towards, \\n382 \\nQ learning algorithm, 372-376. See also \\nReinforcement learning \\nadvantages of, 386 \\nin deterministic environments, 375 \\nconvergence, 377-380 \\ntraining rule, 375-376 \\nstrategies in, 379 \\nlookup table, neural network substitution \\nfor, 384 \\nin nondeterministic environments, \\n381-383 \\nconvergence, 382-383 \\ntraining rule, 382 \\nupdating sequence, 379 \\nQuery strategies, 37-38 \\nRadial basis function networks, 23 1, \\n238-240, 245, 246, 247 \\nadvantages of, 240 \\nRandom variable, 133, 134, 137, 151 \\nRandomized method, 150 \\nRank selection, 256 \\nRBF networks. See Radial basis function \\nnetworks \\nRDT program, 303 \\nReal-valued target function. See \\nContinuous-valued target function \\nRecurrent networks, 119-121. See also \\nNeural networks, artificial \\nRecursive rules, 284 \\nlearning by FOIL algorithm, 290 \\nReduced-error pruning, in decision tree \\nlearning, 69-71 \\nREGRESS algorithm, 3 17-3 18 \\nRegression, 236 \\nin PROLOG-EBG, 316-381 \\nReinforcement learning, 367-387. See also \\nQ learning algorithm \\napplications of, 387 \\ndifferences from other methods, 369-370 \\ndynamic programming and, 380, \\n385-387 \\nexplanation-based learning and, 330 \\nfunction approximation algorithms in, : \\n384-385 \\n1 \\nRelational descriptions, learning of, 302 \\nRelative frequency, 282 \\nRelative mistake bound for \\nWEIGHTED-MAJORITY algorithm, \\n224-225 \\nResidual, 236 \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 417}, page_content=\"Resolution rule, 293-294 \\nfirst-order, \\n296-297 \\ninverse entailment operator and, \\n294-296 \\npropositional, 294 \\nRestriction bias, 64 \\nReward function, in reinforcement \\nlearning, 368 \\nRobot control: \\nby \\nBACKPROPAGATION \\nand EBNN \\nalgorithms, comparison of, 356 \\ngenetic programming in, 269 \\nRobot driving. See Autonomous vehicles \\nRobot perception, attribute cost measures \\nin, 76 \\nRobot planning problems, \\nexplanation- \\nbased learning in, 327 \\nROTE-LEARNER algorithm, inductive bias \\nof, 44-45 \\nRoulette wheel selection, 255 \\nRule for estimating training values, 10, 383 \\nRule learning, \\n274-303 \\nin decision trees, 71-72 \\nin explanation-based learning, 3 1 1-3 19 \\nby FOCL algorithm, 357-360 \\nby genetic algorithms, \\n256-259, \\n269-270, 274 \\nRule post-pruning, in decision tree \\nlearning, 7 1-72 \\nRules: \\ndisjunctive sets of, learning by sequential \\ncovering algorithms, 275-276 \\nfirst-order. See First-order rules \\npropositional. See Propositional rules \\nSafeToStack, \\n310-312 \\nSample complexity, 202. See also Training \\nexamples \\nbound for consistent learners, 207-210, \\n225 \\nequation for, 209 \\nfor finite hypothesis spaces, 207-214 \\nfor infinite hypothesis spaces, 214-220 \\nof k-term CNF and DNF expressions, \\n213-214 \\nof unbiased concepts, \\n212-213 \\nSample error, 130-131, 133-134, 143 \\ntraining error and, 205 \\nSampling theory, 132-141 \\nScheduling problems: \\ncase-based reasoning in, 241 \\nexplanation-based learning in, 325 \\nPRODIGY in, 327 \\nreinforcement learning in, 368 \\nSchema theorem, 260-262 \\ngenetic operators in, 261-262 \\nSearch bias. See Preference bias \\nSearch control problems: \\nexplanation-based learning in, 325-328, \\n329, 330 \\nlimitations of, 327-328 \\nas sequential control processes, 369 \\nSearch of hypothesis space. See Hypothesis \\nspace search \\nSequential control processes, 368-369 \\nlearning task in, 370-373 \\nsearch control problems in, 369 \\nSequential covering algorithms, 274, \\n275-279, 301, 313, 363 \\nchoice of attribute-pairs in, 280-282 \\ndefinition of, 276 \\nFOIL algorithm, comparison with, 287, \\n301-302 \\nID3 algorithm, comparison with, \\n280-28 1 \\nsimultaneous covering algorithms, \\ncomparison with, 280-282 \\nvariations of, 279-280, 286 \\nShattering, \\n214-215 \\nShepard's method, 234 \\nSigmoid function, 97, 104 \\nSigmoid units, 95-96, 115 \\nSimultaneous covering algorithms: \\nchoice of attributes in, 280-281 \\nsequential covering algorithms, \\ncomparison with, 280-282 \\nSingle-point crossover operator, 254, 261 \\nSOAR, 327, 330 \\nSpecific-to-general search, 281 \\nin FOIL algorithm, 287 \\nSpeech recognition, 3 \\nBACKPROPAGATION algorithm in, 8 1 \\nrepresentation by multilayer network, \\n95, 96 \\nVC dimension bound, 2 17-2 18 weight sharing in, 1 18 \"),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 418}, page_content='Split infomation, 73-74 \\nSquashing \\nfunction, \\n96 \\nStacking \\nproblems. See also SafeToStack \\nanalytical \\nlearning in, 3 \\n10 \\nexplanation-based learning in, 3 10 \\ngenetic \\nprogramming in, 263-265 \\nPRODIGY \\nin, 327 \\nStandard \\ndeviation, 133, I 36-1 37 \\nState-transition \\nfunction, 380 \\nStatistics: \\nbasic definitions, 133 \\ninfluence on machine learning, 4 \\nStochastic gradient descent, 93-94, \\n98-100, \\n104-105 \\nStudent t tests, 147-150, 152 \\nSubstitution, 285, 296 \\nSum rule, 159 \\nt tests, 147-150, 152 \\nTANGENTPROP \\nalgorithm, 347-350, 362 \\nBACKPROPAGATION \\nalgorithm, \\ncomparison with, 349 \\nin EBNN algorithm, 352 \\nsearch of hypothesis space \\nby KBANN and \\nBACKPROPAGATION \\nalgorithms, comparison with, \\n350-35 \\n1 \\ntanh \\nfunction, 97 \\nTarget concept, 22-23,4041 \\nPAC learning of, 21 1-213 \\nTarget function, 7-8, 17 \\ncontinuous-valued. See Continuous- \\nvalued target function \\nrepresentation of, 8-9, 14, 17 \\nTD-GAMMON, 3, 14, 369, 383 \\nTD(Q \\nand \\nBACKPROPAGATION \\nalgorithm \\nin, 384 \\nTD(h), 383-384, 387 \\nTemporal credit assignment, in \\nreinforcement learning, 369 \\nTemporal difference learning, 383-384, \\n386-387 \\nTerms, in logic, 284, 285 \\nText classification, naive Bayes classifier \\nin, 180-184 \\nTheorem of total probability, 159 \\n0-subsumption, 302 \\nwith entailment and \\nmore-general-than partial ordering, \\n299-300 \\nTournament \\nselection, \\n256 \\nTraining and validation set approach, 69. \\nSee also Validation set \\nTraining derivatives, 117-1 18 \\nTraining error: \\nof continuous-valued hypotheses, 89-90 \\nof discrete-valued hypotheses, 205 \\nin multilayer networks, 98 \\nalternative error functions, 1 17-1 18 \\nTraining examples, 5-6, 17, 23. See also \\nSample complexity \\nexplanation in PROLOG-EBG, 3 14-3 18 \\nin PAC learning, 205-207 \\nbounds on, 226 \\nVoronoi diagram of, 233 \\nTraining experience, 5-6, 17 \\nTraining values, rule for estimation of, \\n10 \\nTrue error, 130-131, 133, 137, 150, \\n204-205 \\nof two hypotheses, differences in, \\n143-144 \\nin version spaces, 208-209 \\nTwo-point crossover operator, 255, \\n257-258 \\nTwo-sided bounds, 141 \\nUnbiased estimator, 133, 137 \\nUnbiased learners, \\n4042 \\nsample complexity of, 2 12-2 1 3 \\nUniform crossover operator, 255 \\nUnifying substitution, 285, 296 \\nUnsupe~ised \\nlearning, 191 \\nUtility analysis, in explanation-based \\nlearning, 327-328 \\nValidation set. See also Training and I \\nvalidation set approach i \\ncross-validation and, 1 1 1-1 12 \\nerror over, 1 10 \\nVapnik-Chervonenkis \\n(VC) \\ndimension. See \\nVC dimension '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 419}, page_content='Variables, in logic, 284, 285 \\nVariance, 133, 136-137, 138, 143 \\nVC dimension, 214-217, 226 \\nbound on sample complexity, 217-218 \\ndefinition of, 215 \\nof neural networks, 218-220 \\nVersion space representation theorem, 32 \\nVersion spaces, 29-39, 46, 47, 207-208 \\nBayes optimal classifier and, 176 \\ndefinition of, 30 \\nexhaustion of, 208-210, 226 \\nrepresentations of, 30-32 \\nVoronoi diagram, 233 \\nWeakest preimage, 316, 329 \\nWeight decay, 11 1, 117 \\nWeight sharing, 1 18 \\nWeight update rules, 10-1 1 \\nBACKPROPAGATION weight update rule, \\n101-103 \\nalternative error functions, 117-1 18 \\nin KBANN algorithm, 343-344 \\noptimization methods, 119 \\noutput units, 171 \\ndelta rule, 1 1, 88-90, 94 \\ngradient ascent, 170-17 1 \\ngradient descent, 91-92, 95 \\nlinear programming, 95 \\nperceptron training rule, 88-89 \\nstochastic gradient descent, 93-94 \\nWEIGHTED-MAJORITY algorithm, 222-226 \\nmistake-bound learning in, 224-225 \\nWeighted voting, 222, 223, 226 \\nWidrow-Hoff rule. See Delta rule '),\n",
       " Document(metadata={'source': 'D:\\\\interview_qa\\\\Machine - Learning - Tom Mitchell.pdf', 'page': 420}, page_content='')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "for page in data:\n",
    "    question_gen += page.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1094558"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100000000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_text(question_gen[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(page_content=split) for split in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter1 = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_answer_gen = text_splitter1.split_documents(\n",
    "    documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_answer_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_ollama\n",
      "  Using cached langchain_ollama-0.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langchain_ollama) (0.3.21)\n",
      "Collecting ollama<1,>=0.3.0 (from langchain_ollama)\n",
      "  Downloading ollama-0.4.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (0.1.145)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (2.10.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.0->langchain_ollama) (4.12.2)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from ollama<1,>=0.3.0->langchain_ollama) (0.27.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.3.0->langchain_ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (3.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\himan\\anaconda3\\envs\\interview\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.0->langchain_ollama) (2.2.3)\n",
      "Using cached langchain_ollama-0.2.0-py3-none-any.whl (14 kB)\n",
      "Downloading ollama-0.4.1-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: ollama, langchain_ollama\n",
      "Successfully installed langchain_ollama-0.2.0 ollama-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.2:1b\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an expert at creating questions based on coding materials and documentation.\n",
    "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
    "You do this by asking questions about the text below:\n",
    "\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Create questions that will prepare the coders or programmers for their tests.\n",
    "Make sure not to lose any important information.\n",
    "There should be only 10 questions in output without answer and nothing else not even your output.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_template = (\"\"\"\n",
    "You are an expert at creating practice questions based on coding material and documentation.\n",
    "Your goal is to help a coder or programmer prepare for a coding test.\n",
    "We have received some practice questions to a certain extent: {existing_answer}.\n",
    "We have the option to refine the existing questions or add new ones.\n",
    "(only if necessary) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original questions in English.\n",
    "If the context is not helpful, please provide the original questions.\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_gen_chain = load_summarize_chain(llm = llm, \n",
    "                                          chain_type = \"refine\", \n",
    "                                          verbose = True, \n",
    "                                          question_prompt=PROMPT_QUESTIONS, \n",
    "                                          refine_prompt=REFINE_PROMPT_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "Machine Learning \n",
      " \n",
      "Tom M. Mitchell \n",
      " \n",
      " \n",
      "Product Details  \n",
      "• Hardcover: 432 pages ; Dimensions (in inches): 0.75 x 10.00 x 6.50  \n",
      "• Publisher: McGraw-Hill Science/Engineering/Math; (March 1, 1997)  \n",
      "• ISBN: 0070428077  \n",
      "• Average Customer Review: \n",
      " Based on 16 reviews. \n",
      "• Amazon.com Sales Rank: 42,816  \n",
      "• Popular in: Redmond, WA (#17) , Ithaca, NY (#9)  \n",
      " \n",
      " \n",
      "Editorial Reviews \n",
      "From Book News, Inc. An introductory text on primary approaches to machine learning and \n",
      "the study of computer algorithms that improve  automatically through experience. Introduce \n",
      "basics concepts from statistics, artificial intellig ence, information theory, and other disciplines as \n",
      "need arises, with balanced coverage of theory and practice, and presents major algorithms with \n",
      "illustrations of their use. Includes chapter exerci ses. Online data sets and implementations of \n",
      "several algorithms are available on a Web site. No  prior background in artificial intelligence or \n",
      "statistics is assumed. For advanced undergraduat es and graduate students in computer science, \n",
      "engineering, statistics, and social scien ces, as well as software professionals. Book News, Inc.®, \n",
      "Portland, OR  \n",
      "Book Info: Presents the key algorithms and theory that form the core of machine learning. \n",
      "Discusses such theoretical issues as How does learning performance vary with the number of \n",
      "training examples presented? and Which learning algorithms are most a ppropriate for various \n",
      "types of learning tasks? DLC: Computer algorithms.  \n",
      "Book Description: This book covers the field of machin e learning, which is the study of \n",
      "algorithms that allow computer programs to automatically improve through experience. The \n",
      "book is intended to support upper level undergraduate and introductory level graduate courses in \n",
      "machine learning PREFACE \n",
      "The field of machine learning is concerned with the question of how to construct \n",
      "computer programs that automatically improve with experience. In recent years \n",
      "many successful machine learning applications have been developed, ranging from \n",
      "data-mining programs that learn to detect fraudulent credit card transactions, to \n",
      "information-filtering systems that learn users' reading preferences, to autonomous \n",
      "vehicles that learn to drive on public highways. At the same time, there have been \n",
      "important advances in the theory and algorithms that form the foundations of this \n",
      "field. \n",
      "The goal of this textbook is to present the key algorithms and theory that \n",
      "form the core of machine learning. Machine learning draws on concepts and \n",
      "results from many fields, including statistics, artificial intelligence, philosophy, \n",
      "information theory, biology, cognitive science, computational complexity, and \n",
      "control theory. \n",
      "My \n",
      "belief is that the best way to learn about machine learning is \n",
      "to view it from all of these perspectives and to understand the problem settings, \n",
      "algorithms, and assumptions that underlie each. In the past, this has been difficult \n",
      "due to the absence of a broad-based single source introduction \n",
      "to \n",
      "the field. The \n",
      "primary goal of this book is to provide such an introduction. \n",
      "Because of the interdisciplinary nature of the material, this book makes \n",
      "few assumptions about the background of the reader. Instead, it introduces basic \n",
      "concepts from statistics, artificial intelligence, information theory, and other disci- \n",
      "plines as the need arises, focusing on just those concepts most relevant to machine \n",
      "learning. The book is intended for both undergraduate and graduate students in \n",
      "fields such as computer science, engineering, statistics, and the social sciences, \n",
      "and as a reference for software professionals and practitioners. Two principles \n",
      "that guided the writing of the book were that it should be accessible to undergrad- \n",
      "uate students and that it should contain the material I would want my own Ph.D. \n",
      "students to learn before beginning their doctoral research in machine learning. xvi PREFACE \n",
      "A third principle that guided the writing of this book was that it should \n",
      "present a balance of theory and practice. Machine learning theory attempts to an- \n",
      "swer questions such as \"How does learning performance vary with the number of \n",
      "training examples presented?\" and \"Which learning algorithms are most appropri- \n",
      "ate for various types of learning tasks?\" This book includes discussions of these \n",
      "and other theoretical issues, drawing on theoretical constructs from statistics, com- \n",
      "putational complexity, and Bayesian analysis. The practice of machine learning \n",
      "is covered by presenting the major algorithms in the field, along with illustrative \n",
      "traces of their operation. Online data sets and implementations of several algo- \n",
      "rithms are available via the World Wide Web at http://www.cs.cmu.edu/-tom1 \n",
      "mlbook.html. These include neural network code and data for face recognition, \n",
      "decision tree learning, code and data for financial loan analysis, and Bayes clas- \n",
      "sifier code and data for analyzing text documents. I am grateful to a number of \n",
      "colleagues who have helped to create these online resources, including Jason \n",
      "Ren- \n",
      "nie, Paul Hsiung, Jeff Shufelt, Matt Glickman, Scott Davies, Joseph \n",
      "O'Sullivan, \n",
      "Ken Lang, Andrew \n",
      "McCallum, \n",
      "and Thorsten Joachims. \n",
      "ACKNOWLEDGMENTS \n",
      "In writing this book, I have been fortunate to be assisted by technical experts \n",
      "in many of the subdisciplines that make up the field of machine learning. This \n",
      "book could not have been written without their help. I am deeply indebted to \n",
      "the following scientists who took the time to review chapter drafts and, in many \n",
      "cases, to tutor me and help organize chapters in their individual areas of expertise. \n",
      "Avrim \n",
      "Blum, Jaime Carbonell, William Cohen, Greg Cooper, Mark Craven, \n",
      "Ken DeJong, \n",
      "Jerry \n",
      "DeJong, Tom Dietterich, Susan Epstein, \n",
      "Oren \n",
      "Etzioni, \n",
      "Scott Fahlman, Stephanie Forrest, David Haussler, Haym \n",
      "Hirsh, \n",
      "Rob Holte, \n",
      "Leslie Pack Kaelbling, Dennis \n",
      "Kibler, \n",
      "Moshe Koppel, John Koza, Miroslav \n",
      "Kubat, John Lafferty, Ramon Lopez de Mantaras, Sridhar Mahadevan, Stan \n",
      "Matwin, \n",
      "Andrew \n",
      "McCallum, \n",
      "Raymond Mooney, Andrew Moore, Katharina \n",
      "Morik, \n",
      "Steve Muggleton, Michael Pazzani, David Poole, Armand Prieditis, \n",
      "Jim Reggia, Stuart Russell, Lorenza \n",
      "Saitta, \n",
      "Claude Sammut, Jeff Schneider, \n",
      "Jude \n",
      "Shavlik, \n",
      "Devika Subramanian, Michael Swain, Gheorgh Tecuci, \n",
      "Se- \n",
      "bastian \n",
      "Thrun, \n",
      "Peter Turney, Paul Utgoff, Manuela Veloso, Alex Waibel, \n",
      "Stefan Wrobel, and Yiming Yang. \n",
      "I am also grateful to the many instructors and students at various universi- \n",
      "ties who have field tested various drafts of this book and who have contributed \n",
      "their suggestions. Although there is no space to thank the hundreds of students, \n",
      "instructors, and others who tested earlier drafts of this book, I would like to thank \n",
      "the following for particularly helpful comments and discussions: \n",
      "Shumeet \n",
      "Baluja, \n",
      "Andrew \n",
      "Banas, \n",
      "Andy \n",
      "Barto, \n",
      "Jim Blackson, Justin \n",
      "Boyan, \n",
      "Rich \n",
      "Caruana, \n",
      "Philip Chan, Jonathan Cheyer, Lonnie Chrisman, Dayne \n",
      "Frei- \n",
      "tag, Geoff Gordon, Warren Greiff, Alexander Harm, Tom Ioerger, Thorsten PREFACE xvii \n",
      "Joachim, Atsushi Kawamura, Martina Klose, Sven Koenig, Jay Modi, An- \n",
      "drew Ng, Joseph O'Sullivan, Patrawadee Prasangsit, Doina Precup, Bob \n",
      "Price, Choon Quek, Sean Slattery, Belinda Thom, Astro Teller, Will Tracz \n",
      "I would like to thank Joan Mitchell for creating the index for the book. I \n",
      "also would like to thank Jean Harpley for help in editing many of the figures. \n",
      "Jane \n",
      "Loftus \n",
      "from ETP Harrison improved the presentation significantly through \n",
      "her copyediting of the manuscript and generally helped usher the manuscript \n",
      "through the intricacies of final production. Eric Munson, my editor at \n",
      "McGraw \n",
      "Hill, provided encouragement and expertise in all phases of this project. \n",
      "As always, the greatest debt one owes is to one's colleagues, friends, and \n",
      "family. In my case, this debt is especially large. I can hardly imagine a more \n",
      "intellectually stimulating environment and supportive set of friends than those I \n",
      "have at Carnegie Mellon. Among the many here who helped, I would especially \n",
      "like to thank Sebastian \n",
      "Thrun, \n",
      "who throughout this project was a constant source \n",
      "of encouragement, technical expertise, and support of all kinds. My parents, as \n",
      "always, encouraged and asked \"Is it done yet?\" at just the right times. Finally, I \n",
      "must thank my family: Meghan, Shannon, and Joan. They are responsible for this \n",
      "book in more ways than even they know. This book is dedicated to them. \n",
      "Tom M. Mitchell CHAPTER \n",
      "INTRODUCTION \n",
      "Ever since computers were invented, we have wondered whether they might be \n",
      "made to learn. If we could understand how to program them to learn-to improve \n",
      "automatically with experience-the impact would be dramatic. Imagine comput- \n",
      "ers learning from medical records which treatments are most effective for new \n",
      "diseases, houses learning from experience to optimize energy costs based on the \n",
      "particular usage patterns of their occupants, or personal software assistants learn- \n",
      "ing the evolving interests of their users in order to highlight especially relevant \n",
      "stories from the online morning newspaper. A successful understanding of how to \n",
      "make computers learn would open up many new uses of computers and new levels \n",
      "of competence and customization. And a detailed understanding of \n",
      "information- \n",
      "processing algorithms for machine learning might lead to a better understanding \n",
      "of human learning abilities (and disabilities) as well. \n",
      "We do not yet know how to make computers learn nearly as well as people \n",
      "learn. However, algorithms have been invented that are effective for certain types \n",
      "of learning tasks, and a theoretical understanding of learning is beginning to \n",
      "emerge. Many practical computer programs have been developed to exhibit use- \n",
      "ful types of learning, and significant commercial applications have begun to ap- \n",
      "pear. For problems such as speech recognition, algorithms based on machine \n",
      "learning outperform all other approaches that have been attempted to date. In \n",
      "the field known as\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "There should be only 10 questions in output without answer and nothing else not even your output.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input_documents': [Document(metadata={}, page_content='Machine Learning \\n \\nTom M. Mitchell \\n \\n \\nProduct Details  \\n• Hardcover: 432 pages ; Dimensions (in inches): 0.75 x 10.00 x 6.50  \\n• Publisher: McGraw-Hill Science/Engineering/Math; (March 1, 1997)  \\n• ISBN: 0070428077  \\n• Average Customer Review: \\n Based on 16 reviews. \\n• Amazon.com Sales Rank: 42,816  \\n• Popular in: Redmond, WA (#17) , Ithaca, NY (#9)  \\n \\n \\nEditorial Reviews \\nFrom Book News, Inc. An introductory text on primary approaches to machine learning and \\nthe study of computer algorithms that improve  automatically through experience. Introduce \\nbasics concepts from statistics, artificial intellig ence, information theory, and other disciplines as \\nneed arises, with balanced coverage of theory and practice, and presents major algorithms with \\nillustrations of their use. Includes chapter exerci ses. Online data sets and implementations of \\nseveral algorithms are available on a Web site. No  prior background in artificial intelligence or \\nstatistics is assumed. For advanced undergraduat es and graduate students in computer science, \\nengineering, statistics, and social scien ces, as well as software professionals. Book News, Inc.®, \\nPortland, OR  \\nBook Info: Presents the key algorithms and theory that form the core of machine learning. \\nDiscusses such theoretical issues as How does learning performance vary with the number of \\ntraining examples presented? and Which learning algorithms are most a ppropriate for various \\ntypes of learning tasks? DLC: Computer algorithms.  \\nBook Description: This book covers the field of machin e learning, which is the study of \\nalgorithms that allow computer programs to automatically improve through experience. The \\nbook is intended to support upper level undergraduate and introductory level graduate courses in \\nmachine learning PREFACE \\nThe field of machine learning is concerned with the question of how to construct \\ncomputer programs that automatically improve with experience. In recent years \\nmany successful machine learning applications have been developed, ranging from \\ndata-mining programs that learn to detect fraudulent credit card transactions, to \\ninformation-filtering systems that learn users\\' reading preferences, to autonomous \\nvehicles that learn to drive on public highways. At the same time, there have been \\nimportant advances in the theory and algorithms that form the foundations of this \\nfield. \\nThe goal of this textbook is to present the key algorithms and theory that \\nform the core of machine learning. Machine learning draws on concepts and \\nresults from many fields, including statistics, artificial intelligence, philosophy, \\ninformation theory, biology, cognitive science, computational complexity, and \\ncontrol theory. \\nMy \\nbelief is that the best way to learn about machine learning is \\nto view it from all of these perspectives and to understand the problem settings, \\nalgorithms, and assumptions that underlie each. In the past, this has been difficult \\ndue to the absence of a broad-based single source introduction \\nto \\nthe field. The \\nprimary goal of this book is to provide such an introduction. \\nBecause of the interdisciplinary nature of the material, this book makes \\nfew assumptions about the background of the reader. Instead, it introduces basic \\nconcepts from statistics, artificial intelligence, information theory, and other disci- \\nplines as the need arises, focusing on just those concepts most relevant to machine \\nlearning. The book is intended for both undergraduate and graduate students in \\nfields such as computer science, engineering, statistics, and the social sciences, \\nand as a reference for software professionals and practitioners. Two principles \\nthat guided the writing of the book were that it should be accessible to undergrad- \\nuate students and that it should contain the material I would want my own Ph.D. \\nstudents to learn before beginning their doctoral research in machine learning. xvi PREFACE \\nA third principle that guided the writing of this book was that it should \\npresent a balance of theory and practice. Machine learning theory attempts to an- \\nswer questions such as \"How does learning performance vary with the number of \\ntraining examples presented?\" and \"Which learning algorithms are most appropri- \\nate for various types of learning tasks?\" This book includes discussions of these \\nand other theoretical issues, drawing on theoretical constructs from statistics, com- \\nputational complexity, and Bayesian analysis. The practice of machine learning \\nis covered by presenting the major algorithms in the field, along with illustrative \\ntraces of their operation. Online data sets and implementations of several algo- \\nrithms are available via the World Wide Web at http://www.cs.cmu.edu/-tom1 \\nmlbook.html. These include neural network code and data for face recognition, \\ndecision tree learning, code and data for financial loan analysis, and Bayes clas- \\nsifier code and data for analyzing text documents. I am grateful to a number of \\ncolleagues who have helped to create these online resources, including Jason \\nRen- \\nnie, Paul Hsiung, Jeff Shufelt, Matt Glickman, Scott Davies, Joseph \\nO\\'Sullivan, \\nKen Lang, Andrew \\nMcCallum, \\nand Thorsten Joachims. \\nACKNOWLEDGMENTS \\nIn writing this book, I have been fortunate to be assisted by technical experts \\nin many of the subdisciplines that make up the field of machine learning. This \\nbook could not have been written without their help. I am deeply indebted to \\nthe following scientists who took the time to review chapter drafts and, in many \\ncases, to tutor me and help organize chapters in their individual areas of expertise. \\nAvrim \\nBlum, Jaime Carbonell, William Cohen, Greg Cooper, Mark Craven, \\nKen DeJong, \\nJerry \\nDeJong, Tom Dietterich, Susan Epstein, \\nOren \\nEtzioni, \\nScott Fahlman, Stephanie Forrest, David Haussler, Haym \\nHirsh, \\nRob Holte, \\nLeslie Pack Kaelbling, Dennis \\nKibler, \\nMoshe Koppel, John Koza, Miroslav \\nKubat, John Lafferty, Ramon Lopez de Mantaras, Sridhar Mahadevan, Stan \\nMatwin, \\nAndrew \\nMcCallum, \\nRaymond Mooney, Andrew Moore, Katharina \\nMorik, \\nSteve Muggleton, Michael Pazzani, David Poole, Armand Prieditis, \\nJim Reggia, Stuart Russell, Lorenza \\nSaitta, \\nClaude Sammut, Jeff Schneider, \\nJude \\nShavlik, \\nDevika Subramanian, Michael Swain, Gheorgh Tecuci, \\nSe- \\nbastian \\nThrun, \\nPeter Turney, Paul Utgoff, Manuela Veloso, Alex Waibel, \\nStefan Wrobel, and Yiming Yang. \\nI am also grateful to the many instructors and students at various universi- \\nties who have field tested various drafts of this book and who have contributed \\ntheir suggestions. Although there is no space to thank the hundreds of students, \\ninstructors, and others who tested earlier drafts of this book, I would like to thank \\nthe following for particularly helpful comments and discussions: \\nShumeet \\nBaluja, \\nAndrew \\nBanas, \\nAndy \\nBarto, \\nJim Blackson, Justin \\nBoyan, \\nRich \\nCaruana, \\nPhilip Chan, Jonathan Cheyer, Lonnie Chrisman, Dayne \\nFrei- \\ntag, Geoff Gordon, Warren Greiff, Alexander Harm, Tom Ioerger, Thorsten PREFACE xvii \\nJoachim, Atsushi Kawamura, Martina Klose, Sven Koenig, Jay Modi, An- \\ndrew Ng, Joseph O\\'Sullivan, Patrawadee Prasangsit, Doina Precup, Bob \\nPrice, Choon Quek, Sean Slattery, Belinda Thom, Astro Teller, Will Tracz \\nI would like to thank Joan Mitchell for creating the index for the book. I \\nalso would like to thank Jean Harpley for help in editing many of the figures. \\nJane \\nLoftus \\nfrom ETP Harrison improved the presentation significantly through \\nher copyediting of the manuscript and generally helped usher the manuscript \\nthrough the intricacies of final production. Eric Munson, my editor at \\nMcGraw \\nHill, provided encouragement and expertise in all phases of this project. \\nAs always, the greatest debt one owes is to one\\'s colleagues, friends, and \\nfamily. In my case, this debt is especially large. I can hardly imagine a more \\nintellectually stimulating environment and supportive set of friends than those I \\nhave at Carnegie Mellon. Among the many here who helped, I would especially \\nlike to thank Sebastian \\nThrun, \\nwho throughout this project was a constant source \\nof encouragement, technical expertise, and support of all kinds. My parents, as \\nalways, encouraged and asked \"Is it done yet?\" at just the right times. Finally, I \\nmust thank my family: Meghan, Shannon, and Joan. They are responsible for this \\nbook in more ways than even they know. This book is dedicated to them. \\nTom M. Mitchell CHAPTER \\nINTRODUCTION \\nEver since computers were invented, we have wondered whether they might be \\nmade to learn. If we could understand how to program them to learn-to improve \\nautomatically with experience-the impact would be dramatic. Imagine comput- \\ners learning from medical records which treatments are most effective for new \\ndiseases, houses learning from experience to optimize energy costs based on the \\nparticular usage patterns of their occupants, or personal software assistants learn- \\ning the evolving interests of their users in order to highlight especially relevant \\nstories from the online morning newspaper. A successful understanding of how to \\nmake computers learn would open up many new uses of computers and new levels \\nof competence and customization. And a detailed understanding of \\ninformation- \\nprocessing algorithms for machine learning might lead to a better understanding \\nof human learning abilities (and disabilities) as well. \\nWe do not yet know how to make computers learn nearly as well as people \\nlearn. However, algorithms have been invented that are effective for certain types \\nof learning tasks, and a theoretical understanding of learning is beginning to \\nemerge. Many practical computer programs have been developed to exhibit use- \\nful types of learning, and significant commercial applications have begun to ap- \\npear. For problems such as speech recognition, algorithms based on machine \\nlearning outperform all other approaches that have been attempted to date. In \\nthe field known as')], 'output_text': 'Here are 10 questions based on the text:\\n\\n1. What is the field of study that this book belongs to?\\n2. Who has helped write chapter drafts for this book?\\n3. What have computers been able to learn about people in recent years?\\n4. What type of learning tasks have algorithms been effective at solving?\\n5. Why are machine learning outperforming other approaches in speech recognition?\\n6. What field is known as the \"field where humans do not yet know how to make computers learn nearly as well as people learn\"?\\n7. What types of problems have been successfully solved using computer programs that exhibit useful forms of learning?\\n8. What was a major challenge when developing algorithms for machine learning?\\n9. How has technology advanced in recent years in terms of making computers learn better than humans?\\n10. What is the current state of understanding regarding human learning abilities and disabilities as compared to computer learning capabilities?'}\n"
     ]
    }
   ],
   "source": [
    "ques = ques_gen_chain.invoke(documents)\n",
    "\n",
    "print(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ques['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 10 questions based on the text:\n",
      "\n",
      "1. What is the field of study that this book belongs to?\n",
      "2. Who has helped write chapter drafts for this book?\n",
      "3. What have computers been able to learn about people in recent years?\n",
      "4. What type of learning tasks have algorithms been effective at solving?\n",
      "5. Why are machine learning outperforming other approaches in speech recognition?\n",
      "6. What field is known as the \"field where humans do not yet know how to make computers learn nearly as well as people learn\"?\n",
      "7. What types of problems have been successfully solved using computer programs that exhibit useful forms of learning?\n",
      "8. What was a major challenge when developing algorithms for machine learning?\n",
      "9. How has technology advanced in recent years in terms of making computers learn better than humans?\n",
      "10. What is the current state of understanding regarding human learning abilities and disabilities as compared to computer learning capabilities?\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embed = OllamaEmbeddings(\n",
    "    model=\"llama3.2:1b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(document_answer_gen, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer_gen =ChatOllama(\n",
    "    model = \"llama3.2:1b\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = result.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result_list[2:12]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is being discussed in this passage about the potential of computers to learn?',\n",
       " '2. Who has contributed to creating online resources, including Jason Rennie and Paul Hsiung.',\n",
       " '3. What type of learning tasks have algorithms been invented to be effective at?',\n",
       " '4. How do speech recognition programs outperform other approaches for certain problems?',\n",
       " '5. What is the focus of the theoretical understanding of human learning abilities (and disabilities) being discussed in this passage?',\n",
       " '6. What type of computer program has begun to appear as a commercial application due to its effectiveness in certain tasks.',\n",
       " '7. What field is known for algorithms based on machine learning?',\n",
       " '8. What problems have algorithms based on machine learning been effective at solving so far?',\n",
       " '9. Who has helped to create online resources, including those mentioned earlier (Jason Rennie and Paul Hsiung).',\n",
       " '10. How would you describe the potential of computers to learn nearly as well as people?']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen, \n",
    "                                               chain_type=\"stuff\", \n",
    "                                               retriever=vector_store.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  1. What is being discussed in this passage about the potential of computers to learn?\n",
      "Answer:  The passage is discussing the potential of computers to learn and perform tasks autonomously, similar to humans. It mentions that there are several researchers, including Drew Ng, Joseph O'Sullivan, Patrawadee Prasangsit, Doina Precup, Bob Price, Choon Quek, Sean Slattery, Belinda Thom, Astro Teller, Will Tracz, Joan Mitchell, and others, who have contributed to the development of this area.\n",
      "\n",
      "\n",
      "Question:  2. Who has contributed to creating online resources, including Jason Rennie and Paul Hsiung.\n",
      "Answer:  According to the text, it appears that the following individuals have contributed to creating online resources related to machine learning:\n",
      "\n",
      "1. Jason Rennie\n",
      "2. Paul Hsiung\n",
      "\n",
      "\n",
      "Question:  3. What type of learning tasks have algorithms been invented to be effective at?\n",
      "Answer:  According to the text, machine learning algorithms have been invented to be effective at a variety of learning tasks, including:\n",
      "\n",
      "* Speech recognition\n",
      "* Recognizing patterns in data (i.e., performing various types of classification and clustering)\n",
      "* Image classification\n",
      "* Image segmentation\n",
      "* Natural Language Processing (NLP)\n",
      "\n",
      "These algorithms can perform tasks such as image classification, object detection, sentiment analysis, text summarization, and more.\n",
      "\n",
      "\n",
      "Question:  4. How do speech recognition programs outperform other approaches for certain problems?\n",
      "Answer:  According to the text, speech recognition programs outperform other approaches for certain problems due to their ability to automatically improve through experience, which is introduced in Chapter 1 of the book.\n",
      "\n",
      "It's not explicitly stated how they achieve this improvement, but based on the context, it can be inferred that machine learning algorithms are used to train or refine the models over time. This allows the programs to adapt and become more accurate in recognizing speech sounds and converting them into text.\n",
      "\n",
      "The text mentions that algorithms based on machine learning outperform all other approaches that have been attempted to date for problems such as speech recognition, suggesting that these types of algorithms provide a significant advantage in certain situations.\n",
      "\n",
      "\n",
      "Question:  5. What is the focus of the theoretical understanding of human learning abilities (and disabilities) being discussed in this passage?\n",
      "Answer:  The passage does not specifically mention the \"theoretical understanding of human learning abilities (and disabilities)\". However, it does mention that the book aims to provide an introduction to machine learning and its application to various fields, including those related to human learning.\n",
      "\n",
      "It does mention that the book is intended for both undergraduate and graduate students in fields such as computer science, engineering, statistics, and the social sciences, which suggests a focus on understanding and analyzing complex systems, including those involved in human learning.\n",
      "\n",
      "\n",
      "Question:  6. What type of computer program has begun to appear as a commercial application due to its effectiveness in certain tasks.\n",
      "Answer:  The context mentions \"algorithms based on machine learning\" and the field of Machine Learning. Machine Learning is a subfield of artificial intelligence that involves training algorithms to learn from data, rather than being programmed with specific instructions.\n",
      "\n",
      "Based on this context, it appears that computer programs have begun to appear as commercial applications due to their effectiveness in certain tasks such as:\n",
      "\n",
      "* Speech recognition\n",
      "* Image and object detection\n",
      "* Natural language processing\n",
      "\n",
      "These types of programs are often implemented using machine learning algorithms, which allow them to learn from data and improve over time.\n",
      "\n",
      "\n",
      "Question:  7. What field is known for algorithms based on machine learning?\n",
      "Answer:  According to the text, the field known for algorithms based on machine learning is Machine Learning.\n",
      "\n",
      "\n",
      "Question:  8. What problems have algorithms based on machine learning been effective at solving so far?\n",
      "Answer:  The text doesn't specifically mention what problems have been effectively solved using algorithms based on machine learning. It does mention that these algorithms outperform other approaches for certain tasks, such as speech recognition. However, it would be a good idea to look up the specific examples mentioned in the text to get an accurate answer to your question.\n",
      "\n",
      "\n",
      "Question:  9. Who has helped to create online resources, including those mentioned earlier (Jason Rennie and Paul Hsiung).\n",
      "Answer:  The text mentions that several individuals have contributed to creating online resources for this book, but it does not specifically mention Jason Rennie and Paul Hsiung. However, it does mention that the authors were assisted in improving the presentation through copyediting by Jane Loftus from ETP Harrison, and online data sets and implementations of several algorithms are available on a Web site.\n",
      "\n",
      "\n",
      "Question:  10. How would you describe the potential of computers to learn nearly as well as people?\n",
      "Answer:  The text does not mention or describe how a computer can \"learn\" nearly as well as people. In fact, it states that there are significant commercial applications of machine learning, including algorithms based on machine learning that outperform other approaches for certain problems such as speech recognition. The text implies that computers have made significant progress in improving their performance on these types of tasks, but does not suggest that they can learn nearly as well as people.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer each question and save to a file\n",
    "for question in result:\n",
    "    print(\"Question: \", question)\n",
    "    answer = answer_generation_chain.run(question)\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"\\n\")\n",
    "    # Save answer to file\n",
    "    with open(\"answers1.txt\", \"a\") as f:\n",
    "        f.write(\"Question: \" + question + \"\\n\")\n",
    "        f.write(\"Answer: \" + answer + \"\\n\"+ \"\\n\"+ \"\\n\")\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
